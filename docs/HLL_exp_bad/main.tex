\documentclass{IEEEtran}
\usepackage{graphicx}
\usepackage{array}
\usepackage{url}
\usepackage{hyperref}


%% TODO NOTES

\usepackage{xcolor}
\definecolor{oxygenorange}{HTML}{FFDD00}
\usepackage[color=oxygenorange]{todonotes}
\newcommand{\mathilde}[2][inline]{\todo[#1]{\textbf{Mathilde:} #2}\xspace}
\newcommand{\kenny}[2][inline]{\todo[#1]{\textbf{Kenny:} #2}\xspace}

\title{HyperLogLog: Exponentially Bad in Adversarial Settings}
\author{Mathilde Raynal, Kenneth G. Paterson}

\begin{document}

\maketitle

\IEEEtitleabstractindextext{%
\begin{abstract}
Computing the count of distinct elements in massive data sets is a common task but naive approaches are memory-expensive. The HyperLogLog (HLL) algorithm (Durand and Flajolet, 2003) estimates a data stream's cardinality while using significantly less memory than a naive approach at the cost of some accuracy. This trade-off makes the HLL algorithm very attractive for a wide range of applications such as database management and network monitoring, where an exact count may not be needed. Recently, the HLL algorithm has started to be proposed for use in scenarios where the inputs may be adversarially generated, for example detection of network scanning attacks. This prompts an examination of the performance of the HLL algorithm in the face of adversarial inputs. We show that in such a setting, the HLL algorithm's estimate of cardinality can be exponentially bad: when an adversary has access to the internals of the HLL algorithm, an adversary who has some flexibility in choosing what inputs will be recorded can manipulate the cardinality estimate to be exponentially smaller than the true cardinality. 
\end{abstract}
}

\IEEEdisplaynontitleabstractindextext

\section{Introduction}
Naive approaches to computing the cardinality of a big data set, such as sorting the elements or simply maintaining the set of unique elements seen, are impractical at scale. The HyperLogLog algorithm (HLL henceforth)~\cite{loglog,hll2} is today the most widely used cardinality estimator. It is increasingly used in settings where adversaries may have  incentives to manipulate the estimate made by the algorithm. For example,~\cite{portscanhll} proposes its use for detecting network scanning attacks, while Facebook engineers have reported~\cite{fbhll} that they use HLL for estimating the number of active users. These applications demand a careful evaluation of the performance of HLL under adversarial input. 

Very recently, Reviriego and Ting~\cite{hllvuln} initiated work in this direction, showing that in specific attack settings, the cardinality estimate made by HLL could be modified through input selection. In this paper, we present a complete analysis of HLL in adversarial setting, considering more realistic attack scenarios and giving more powerful attacks than~\cite{hllvuln}. In particular, we show that with only modest knowledge of the HLL internals and a moderate amount of computation, an adversary can make HLL underestimate true cardinality by a factor this is exponential in the number of buckets used by the algorithm.

The rest of the paper is organised as follows. Section~\ref{sec:overview} gives an overview of the HyperLogLog algorithm. Section~\ref{sec:attacks} presents our different adversarial models. There, we also give attacks and evaluate their impacts under the different adversarial models. Section~\ref{sec:conclusions} provides our conclusions and some ideas for future work.


 
\section{Overview of HLL}\label{sec:overview}
\subsection{The Algorithm}
HLL~\cite{loglog,hll2} is a streaming algorithm based on the key observation that, for a stream of randomly distributed values represented as bit-strings of some fixed length, if we observe a value with the maximum of $k$ leading zero-bits, then the cardinality of the stream (i.e.\ the number of distinct values it contains) is likely to be on the order of $2^{k+1}$. In practice, to ensure a uniform distribution, the stream values are first processed by a hash function $h$ before the leading bits are inspected. So to estimate the cardinality of a stream, we need only store a representation of $k$, the length of the largest observed string of leading zero-bits for numbers in the stream. This provides a very compact mechanism for estimating the stream cardinality -- if the true cardinality is $N$, then just $\log\log(N)$ bits are needed to store the estimate. 

This simple approach suffers from large variance in the estimation. In order to improve the estimate, we can use many estimators in parallel instead of one, and average the results. Each estimate is stored in its own bucket. To map a value $x$ to one of $2^n$ buckets, \cite{loglog} suggests using the first $n$ bits of $h(x)$ as a bucket index, and then computing the longest sequence of leading zero bits on the remaining $\ell$ bits of $h(x)$ (so that the output length of $h(\cdot)$ is $n+\ell$). Estimators from all buckets are averaged using the harmonic mean and scaled by a constant $\alpha$, where $\alpha$ empirically computed to correct a systematic multiplicative bias. As result, HLL is able to estimate cardinalities greater than $10^9$ with a typical standard error of 2\%, using only 1.5 kB of memory~\cite{add-ref-for-this-claim}.

An interesting property of HLL is that it supports merging in a lossless way. To combine two buckets at index $i$ in two different HLL instances, take the maximum of the two bucket entries and assign that to the matching bucket $i$ in the merged HLL. Such a simple set union operation allows easy parallelisation operations among multiple machines independently, provided they use the same hash function and the same number of buckets.

\subsection{Applications}
For completeness and motivation, we provide examples of concrete uses of HLL:
\begin{itemize}
    \item Cardinality estimation provides a significantly faster way for Facebook to compute statistics on their users. They use HLL to find out how many distinct people visited their website in the past week~\cite{fbhll}.
    \item HLL has been proposed as a solution to detecting Denial-of-Service attacks~\cite{portscanhll} (this paper actually uses a variation on HLL called sliding HLL~\cite{slidinghll} which estimates the cardinality over a moving time window). Specifically, HLL is used to count how many different ports are used as destination port over all packets received, as an attempt to identify when a port scan attack is underway.
        \item HLL has been implemented in network switches to approximate the number of distinct packets in traffic flows and overcome the switch resource limits~\cite{flexswitch}. The number of unique flows traversing a switch can then be used to provide a better congestion control protocol.
        \end{itemize}

Regardless of the sensitivity of the above examples, none of the relevant papers are clear about their threat model, especially for the HLL component.  They are all potentially vulnerable to the attacks we present below.
%By not assessing the risks involved with the presence of an adversary targeting the HLL sketch, they may allow an attack like the one we present, breaking down the whole solution.

\subsection{Related Work}

The work of~\cite{cardestprivacy} breaks the privacy of the HLL sketch and concludes that having the ability to test the insertion \kenny{what does this mean?} of a targeted item gives information about whether the item was originally present in the input data set. It is argued in~\cite{cardestprivacy} that HLL can be used by organizations to store and process location data, which is inherently sensitive, so losing privacy would leak users' locations and potentially cause great harm. 

In~\cite{hllvuln}, Reviriego and Ting exploit a vulnerability of HLL to manipulate the cardinality estimate, leading to a five-fold reduction in the estimate compared to the true cardinality.

Both~\cite{cardestprivacy,hllvuln} propose mitigations using a salted sketch, either as replacement (but at the cost of loosing mergability) or in addition to an unsalted sketch (inducing a memory overhead). 
%This might not be a convenient solution for some applications, they thus encourage further research. 

\section{Vulnerabilities}\label{sec:attacks}
This section presents different adversarial settings and the corresponding attacks to manipulate the HLL estimate. The attacker's goal in our work is to bring the estimate down, as this would cause the most damage when looking at the applications of HLL. One can think about the attacker trying the hide from a detection system. Conveniently, it is easy to adopt the same strategy as the one we present for an attacker who is looking to artificially inflate the HLL estimate given a limited data set input. The impact over the cardinality estimate and the computational effort to be done by the attacker is analyzed afterwards. 

\subsection{Adversarial setting}
We model 3 adversarial scenarios based on the attacker's knowledge and capabilities. For all of them, we assume that the sketch already contains some existing data from honest users to simulate a real life setup. Among the scenarios, S3 has the strongest assumptions as it combines the knowledge of attackers from S1 and S2.

S1: the attacker is able to test insertion of an item on a black-box snapshot of the sketch in use and check the effect on the cardinality estimate.

S2: the attacker has access to the details of the implementation, i.e., the number of buckets and the hash function $h$.

S3: the attacker has direct access to the sketch in use and all internals, i.e., the number of buckets, the hash function $h$, and also the sketch's buckets content. Those buckets hold the intermediate estimators updated by the honest user's data, that are averaged in a later step to compute the global cardinality estimator.\\

S1 and S2 are presented as M2 and M1 respectively in \cite{hllvuln}. We do not put our focus on S1 as this previous work fully exploit this scenario. The attack works as follow: ``after each insertion the cardinality is checked and the items which do not increment the HLL estimate are retained``. We start by presenting our attack in S2, providing much stronger results then the corresponding attack of \cite{hllvuln}, and then use the additional information the attacker has access to in S3 to make the attack even more efficient. 

\subsection{Manipulating the estimate}
Under S2: The attacker has access to $h$ so he can adjust some field of the item $x$ he is trying to insert and keep only the one such that the $l$ rightmost bits of $h(x)$ start with a 1. Indeed, the number of leading 0 bits computed by the estimator of the destination bucket is always zero, so the estimator of this bucket is unlikely to be updated and the averaged approximated cardinality is left unchanged. As the attacker knows how many buckets are used, he can recover $n$ and $l$, and aim for the $n$th bit (first bit not used for bucket allocation) to be 1. For example, in the scenario of \cite{portscanhll}, the attacker could change the source port of his IP packet $x$ until he is satisfied with $h(x)$. \textbf{(what about repeated sampling?)}\\

Under S3: The attacker can adopt the same strategy but make the attack more efficient and reduce the sampling requirements with the additional information. Now that the attacker has the sketch, not only he knows the overall estimated cardinality but also the intermediate estimators computed per bucket. So, he can "allow" some leading 0 bits in the $l$ rightmost bits of $h(x)$ as long as it does not increment the bucket's estimate. In other words, if $h(x)$ is mapped to the $i$th bucket with estimated cardinality $2^{c_i + 1}$ (it observed a value with $c_i$ leading 0s), then, instead of the strict restriction of a 1 bit at the $n$th index, the attacker is satisfied when having at least one 1 bit in the $n$ to $n+c_i + 1$ bit slice of $h(x)$. In case the attacker hits an empty bucket (the estimator is 0), he must skip that bucket completely. Under those conditions, the estimator of the buckets are not updated, and there is no effect on the cardinality estimation of the HLL sketch.

\subsection{Impact}
Now that we have a way to exploit the vulnerability, we need to evaluate to which extent it can affect the estimate. We assume that $h$ is a "good" hash function, so any bit of $h(x)$ has a probability of $\frac{1}{2}$ to be 1 and $\frac{1}{2}$ to be 0.\\

Under S2, the attacker need to craft 2 $x$s on average to obtain a 1 bit at the $n$th position of $h(x)$ and successfully perform the attack. As we do not care about the remaining bits, the attacker can theoretically "hide" up to 2$^{len(h(x))-1}$ items into the sketch.\\

Under S3, let's compute the average work for the attacker to find an $x$ such that $h(x)$ meets the attack's requirements. We assume $h(x)$ is mapped to bucket $i$. The probability that $h(x)$ does not increment the estimator is the probability that $h(x)$ has less than $c_i$ leading 0s. So:\\
$P(h(x)$ does NOT update bucket $i| h(x)$ mapped to bucket $i)=P(h(x)$ has LESS than $c_i$ leading 0s$)=1-P(h(x)$ has strictly MORE than $c_i$ leading 0s$)=1-\frac{1}{2^{c_i+1}}$\\
Across all buckets, the attacker does not update any estimate with probability $P=
\sum_{i=1}^{2^n}P(h(x)$ mapped to bucket $i$ AND $h(x)$ does NOT update bucket $i) = \sum_{i=1}^{2^n}P(h(x)$ mapped to bucket $i)P(h(x)$ does NOT update bucket $i| h(x)$ mapped to bucket $i) = \sum_{i=1}^{2^n}\frac{1}{2^n} \times (1-\frac{1}{2^{c_i+1}}) = 1-\frac{1}{2}\times(\frac{1}{2^n}\times \sum_{i=1}^{2^n} {(2^{c_i})}^{-1}) = 1-\frac{1}{2}\times\frac{1}{H_c} = 1-(2H_c)^{-1}$, with $H_c$ the harmonic mean of the numbers $2^{c_1}, 2^{c_2}, ..., 2^{c_{2^n}}$.
So the attacker needs to craft on average $(1-(2H_c)^{-1})^{-1} x$s, and can theoretically "hide" up to 2$^{len(h(x))}\times (1-(2H_c)^{-1})$ items into the sketch.

\textit{NOTE: does it take into account the special case of empty bucket ? counters are initialized to $c_i=-oo$ (or -1 in the code???) thus $P(h(x)$ has LESS than $-oo$ leading $0s) = 0$ but it is hard to add it to the probabilities while staying clear.}\\
%2$^{len(h(x))}\times \frac{2H_c-1}{H_c}$ items into the sketch.\\

We assume here that there already are values coming from other honest users' data in the sketch, we now remove this assumption. Unfortunately, the attacker in S2 has no way to detect that he is attacking an empty sketch and cannot adapt his strategy, so all buckets hit will update once their estimator to $2^{0+1}=2$ items, leading to the sketch final approximation of the number of distinct elements to $\alpha2^{n+1}$ regardless of the amount of items the attacker will send. However, the attacker in S3 is able to notice that all buckets are empty and adapt his attack. As mentioned above, the attacker in S3 avoids empty buckets, but when dealing with an empty sketch, sacrifices must be made if the attacker wants to insert at least one item. To limit the detection, the attacker can target and fill one bucket only. This would fix $n+1$ bits of $h(x)$, allowing the attacker to add $2^{len(h(x))-n-1}$ elements while keeping the cardinality to 1.

\subsection{Experimental results}
There exist many implementations of the HyperLogLog algorithm available online. Among the repositories collecting more than 350 stars on github, we can cite \cite{clahll} and \cite{datasketch}. It is also featured in many frameworks, including Redis \cite{redis}, Google's BigQuery \cite{bigquery} and several products of Apache such as Spark \cite{spahll} \cite{spahll2} and Druid \cite{druhll}. It is interesting to point out that they all use a constant number of buckets and a fixed hash function, \textit{which is public or should be public according to Kerckhoffs's principle,} thus are potentially vulnerable to cardinality estimate manipulation when put in an adversarial setting.\\

As proof-of-concept, we implement our attack against the HLL algorithm from \cite{clahll}. The code is available at \href{https://github.com/PizzaWhisperer/HLLVuln}{\textit{\url{https://github.com/PizzaWhisperer/HLLVuln}}}. Our items are 4 character long ASCII strings \textit{(7'311'616 distinct items)} and we choose $h$ to be the 32 bits Murmur3 hash function from \cite{murmur3code}. We create an HLL sketch using 256 buckets and initialize it with 1'000 random items representing an honest user's data.

We challenge the attacker to pick the largest possible number of items to add to the sketch while trying to keep the cardinality estimate as low as possible, either in the S2 or the S3 scenario. In a second time, in order to get results that can be compared to the work of \cite{hllvuln}, we mount our attack in their more restrictive setting: instead of picking from the universe of possible strings, the attacker has to choose from a set of 250'000 random items that are given to him. Finally , for completeness, we also run the attacks against empty sketches. All attacks are ran 30 times and the outcomes are reported in Tab. \ref{table:tab1} and Tab. \ref{table:tab2}, which detail the estimated cardinality at the beginning of the attack, the number of items added by the attacker and the approximation of the cardinality after adding the attacker's items.

Those empirical results confirm our impact analysis on several points. First, when attacking a sketch already containing some items, we can see that the attacker is able to hide approximately half of the items' set under S2, and around 83\%%( $205$'$000 \approx 250'000 \times 0.82 \approx 250$'$000\times(1-\frac{1}{2\times H_c})$, with $H_c\approx\frac{1'000}{256}=3.9$).
$\approx (1-\frac{1}{2\times H_c})$ with $H_c\approx\frac{1'000}{256}=3.9$, under S3.
Secondly, when attacking an empty sketch, the final cardinality estimate is always exactly 367 $=\alpha2^{n+1}$ with $\alpha = 0.72$ as specified by the Eq. (29) of \cite{hll2} or 1 in respectively S2 and S3.

To assess the efficiency of our attacks, we can confront our results from Tab. \ref{table:tab2} with the attack presented in the Section 5.1 of \cite{hllvuln}, where they target the Redis \cite{redishll} open-source implementation of HLL and perform the attack against an empty sketch. Given a set of 250'000 items, the attacker could choose ``74'390 distinct items and obtain an HLL estimate of only 15'780`` achieving a five-fold reduction from the true cardinality. In comparison, the attacker following our strategy is able to add 125'000 distinct items while keeping the estimate at 367, demonstrating a reduction by a factor of 340. 

\begin{table}[h]
\caption{Result of the attacks over 30 iterations}
\begin{tabular}{| m{8.5em} | m{4em} | m{4em} | m{4em} | m{4em} |}
    \hline
    \textbf{Scenario} & \multicolumn{2}{c|}{S2} & \multicolumn{2}{c|}{S3} \\ \hline
    \textbf{Initial data?} & yes & no & yes & no \\ \hline
    \textbf{Original Est. Card.} & 1'000 & 0 & 1'005 & 0 \\ \hline
    \textbf{\# Items added} & 3'655'744 & 3'655'740 & 6'090'003 & 14'220 \\ \hline
    \textbf{Final Est. Card.} & 1'011 & 367 & 1'005 & 1 \\ \hline
\end{tabular}
\label{table:tab1}
\end{table}

\begin{table}[h]
\caption{Result of the attacks in the setting of \cite{hllvuln} over 30 iterations}
\begin{tabular}{| m{8.5em} | m{4em} | m{4em} | m{4em} | m{4em} |}
    \hline
    \textbf{Scenario} & \multicolumn{2}{c|}{S2} & \multicolumn{2}{c|}{S3} \\ \hline
    \textbf{Initial data?} & yes & no & yes & no \\ \hline
    \textbf{Original Est. Card.} & 1'004 & 0 & 998 & 0 \\ \hline
    \textbf{\# Items added} & 124'940 & 125'085 & 204'669 & 501 \\ \hline
    \textbf{Final Est. Card.} & 1'058 & 367 & 998 & 1 \\ \hline
\end{tabular}
\label{table:tab2}
\end{table}

\section{Conclusion}\label{sec:conclusions}
The HyperLogLog algorithm is an elegant and efficient solution to the distinct count problem. Its simple structure makes it easy to code and use, as shows the growing number of available open-source implementations. Nonetheless, disclosing such low level details can be exploited by malicious users to manipulate the estimate of the sketch and break the system that is built on top. Our attack is trivial but powerful and should raise awareness on the security of HLL, such that product designers understand the risks they may encounter when exposed to an adversarial setting. Future work could consist in finding mitigations that do not rely on salt. For example, would the imposed ID setting (like Facebook) solve the issue but still be convenient for the various HLL applications? Also left for future work is to formalize the extension of our attack to the HLL variants, i.e., to show that it also breaks the HLL++ \cite{hllpratice} and sliding HLL \cite{slidinghll} algorithms.

\bibliographystyle{ieeetr}
\bibliography{ref.bib}

\end{document}
