\documentclass{IEEEtran}
\usepackage{graphicx}
\usepackage{array}
\usepackage{url}
\usepackage{hyperref}


%% TODO NOTES

\usepackage{xcolor}
\definecolor{oxygenorange}{HTML}{FFDD00}
\usepackage[color=oxygenorange]{todonotes}
\newcommand{\mathilde}[2][inline]{\todo[#1]{\textbf{Mathilde:} #2}\xspace}
\newcommand{\kenny}[2][inline]{\todo[#1]{\textbf{Kenny:} #2}\xspace}

\title{HyperLogLog: Exponentially Bad in Adversarial Settings}
\author{Mathilde Raynal, Kenneth G. Paterson}

\begin{document}

\maketitle

\IEEEtitleabstractindextext{%
\begin{abstract}
Computing the count of distinct elements in massive data sets is a common task but naive approaches are memory-expensive. The HyperLogLog (HLL) algorithm (Flajolet \emph{et al.}, 2007) estimates a data stream's cardinality while using significantly less memory than a naive approach at the cost of some accuracy. This trade-off makes the HLL algorithm very attractive for a wide range of applications such as database management and network monitoring, where an exact count may not be needed. Recently, the HLL algorithm has started to be proposed for use in scenarios where the inputs may be adversarially generated, for example detection of network scanning attacks. This prompts an examination of the performance of the HLL algorithm in the face of adversarial inputs. In this cautionary note, we show that in such a setting, the HLL algorithm's estimate of cardinality can be exponentially bad: when an adversary has access to the internals of the HLL algorithm and has some flexibility in choosing what inputs will be recorded, it can manipulate the cardinality estimate to be exponentially smaller than the true cardinality.
\end{abstract}
}

\IEEEdisplaynontitleabstractindextext

\section{Introduction}
Naive approaches to computing the cardinality of a big data set, such as sorting the elements or simply maintaining the set of unique elements seen, are impractical at scale. The HyperLogLog algorithm (HLL henceforth)~\cite{hll} is today the most widely used cardinality estimator. It is increasingly used in settings where adversaries may have  incentives to manipulate the estimate made by the algorithm. For example,~\cite{portscanhll} proposes its use for detecting network scanning attacks, while Facebook engineers have reported~\cite{fbhll} that they use HLL for estimating the number of active users. These applications demand a careful evaluation of the performance of HLL under adversarial input.

Very recently, Reviriego and Ting~\cite{hllvuln} initiated work in this direction, showing that in specific attack settings, the cardinality estimate made by HLL could be modified through input selection. In this paper, we present a complete analysis of HLL in adversarial setting, considering more realistic attack scenarios and giving more powerful attacks than~\cite{hllvuln}. In particular, we show that with only modest knowledge of the HLL internals and a moderate amount of computation, an adversary can make HLL underestimate true cardinality by a factor that is exponential in the number of buckets used by the algorithm.

The rest of the paper is organised as follows. Section~\ref{sec:overview} gives an overview of the HyperLogLog algorithm. Section~\ref{sec:attacks} presents our different adversarial models. There, we also give attacks and evaluate their impacts under the different adversarial models. Section~\ref{sec:conclusions} provides our conclusions and some ideas for future work.



\section{Overview of HLL}\label{sec:overview}
\subsection{The Algorithm}
HLL~\cite{hll} is a streaming algorithm based on the key observation that, for a stream of randomly distributed values represented as bit-strings of some fixed length, if we observe a value with the maximum of $k$ leading zero-bits, then the cardinality of the stream (i.e.\ the number of distinct values it contains) is likely to be on the order of $2^{k+1}$. In practice, to ensure a uniform distribution, the stream values are first processed by a hash function $h$ before the leading bits are inspected. So to estimate the cardinality of a stream, we need only store a representation of $k$, the length of the largest observed string of leading zero-bits for numbers in the stream. This provides a very compact mechanism for estimating the stream cardinality -- if the true cardinality is $N$, then just $\log\log(N)$ bits are needed to store the estimate.

This simple approach suffers from large variance in the estimation. In order to improve the estimate, we can use many estimators in parallel instead of one, and average the results. Each estimate is stored in its own bucket. The collection of buckets is referred to as the HLL \emph{sketch}. To map a value $x$ to one of $2^n$ buckets, \cite{loglog} suggests using the first $n$ bits of $h(x)$ as a bucket index, and then computing the longest sequence of leading zero bits on the remaining $\ell$ bits of $h(x)$ (so that the output length of $h(\cdot)$ is $n+\ell$). Estimators from all buckets are averaged using the harmonic mean and scaled by a constant $\alpha$, where $\alpha$ is empirically computed to correct a systematic multiplicative bias. As result, HLL is able to estimate cardinalities greater than $10^9$ with a typical standard error of 2\%, using only 1.5 kB of memory~\cite{hll}.

An interesting property of HLL is that it supports merging in a lossless way. To combine two buckets at index $i$ in two different HLL instances, take the maximum of the two bucket entries and assign that to the matching bucket $i$ in the merged HLL sketch. Such a simple set union operation allows easy parallelisation of operations among multiple machines independently, provided they use the same hash function and the same number of buckets.

\subsection{Applications}
For completeness and motivation, we provide examples of concrete uses of HLL:
\begin{itemize}
    \item Cardinality estimation provides a significantly faster way for Facebook to compute statistics on their users. They use HLL to find out how many distinct people visited their website in the past week~\cite{fbhll}.
    \item HLL has been proposed as a solution to detecting Denial-of-Service attacks~\cite{portscanhll} (this paper actually uses a variation on HLL called \emph{sliding HLL}~\cite{slidinghll} which estimates the cardinality over a moving time window). Specifically, HLL is used to count how many different ports are used as destination port over all packets received, as an attempt to identify when a port scan attack is underway.
    \item HLL has been implemented in network soft-switches to approximate the number of distinct packets in traffic flows and overcome the switch resource limits~\cite{flexswitch}. The number of unique flows traversing a switch can then be used to provide a better congestion control protocol.
\end{itemize}

Regardless of the sensitivity of the above examples, none of the relevant papers are clear about their threat model, especially for the HLL component.  They are all potentially vulnerable to the attacks we present below.
%By not assessing the risks involved with the presence of an adversary targeting the HLL sketch, they may allow an attack like the one we present, breaking down the whole solution.

\subsection{Related Work}

The work of~\cite{cardestprivacy} studies the privacy properties of HLL in two different attack scenarios, referred to as \emph{insider} and \emph{external}. The attack target of~\cite{cardestprivacy} is different from ours, being concerned with loss of privacy, whereas we care about cardinality estimation. It is argued in~\cite{cardestprivacy} that HLL can be used by organisations to store and process location data, which is inherently sensitive, so losing privacy would leak users' locations and potentially cause great harm. In Section~\ref{sec:attacks} we will discuss the two attack scenarios from~\cite{cardestprivacy} in more detail. In particular, we will show that an attacker who can insert items into the HLL and who can access the HLL cardinality estimate (via typical APIs in HLL implementations) as permitted in the external scenario can actually realise the same attacks as the insider attacker can, at least insofar as cardinality estimate manipulation is concerned.

In~\cite{hllvuln}, Reviriego and Ting exploit a vulnerability of HLL to manipulate the cardinality estimate, leading to a five-fold reduction in the estimate compared to the true cardinality. They use an attack model that we consider to be quite artificial, since it gives the attacker the ability to test whether inserting an item \emph{would} increase the cardinality estimate, without actually having to insert the item. See further discussion in Section~\ref{sec:attacks}.

Both~\cite{cardestprivacy,hllvuln} propose mitigations using a salted sketch, either as replacement (but at the cost of losing mergeability) or in addition to an unsalted sketch (inducing a memory overhead).
%This might not be a convenient solution for some applications, they thus encourage further research.

\section{HLL in Adversarial Settings}\label{sec:attacks}

This section presents different adversarial settings and corresponding attacks that manipulate the HLL cardinality estimate. The adversary's goal is to reduce the estimate as much as possible, since this would cause more damage when  considering the above-mentioned applications of HLL. It is easy to modify our attack strategies for an adversary who instead wishes to artificially inflate the HLL estimate given a limited input capability. We also analyze the impact on the HLL cardinality estimate and the computational effort required.

Throughout, we assume the adversary has the ability to vary the contents of free fields in the input strings which will be inserted into the stream of values. For example it can manipulate IPID and other header fields in the context of IP packets. This enables the adversary to control the output of hash function $h$ and flexibly choose which items will be inserted into the HLL sketch, and in which buckets.

\subsection{Adversarial Setting}
We model four distinct adversarial scenarios based on the adversary's knowledge and capabilities. For all of them, we assume that the sketch already contains some existing data from honest users to simulate a real-life setting. Among the scenarios, the \textit{insider} setup of \cite{cardestprivacy} that we present as S4 makes the strongest assumptions about the adversarial capabilities since we assume that the attacker owns the data structure and thus has an exhaustive view on the sketch. %combines the knowledge of adversaries from S1 and S2.
\kenny{Not really! Because S1 is actually quite unrealistic, as we will argue below.}

\begin{itemize}
\item[S1:] The adversary has access to a \textit{test oracle} provided by the sketch owner, which allows him to submit an item and learn the hypothetical effect of inserting it into the sketch. He is then able to determine if inserting this item \textit{would} increase the cardinality estimation.

%The adversary is able to test the hypothetical effect of the insertion of an item on the cardinality estimation of the sketch. This can achieved via a \textit{test oracle} provided by the sketch owner, which allows an user to test the effect of an insertion on the cardinality estimate but without actually inserting the item, or if the attacker would gain access to a black-box replica of the sketch in use, i.e., buying the same switch and testing in the lab. %black-box access to the HLL in use -- that is, the adversary can ask for the HLL's cardinality estimate at any point in time, and so is able to check the effect on the cardinality estimate when inserting an item.
\kenny{TODO: clarify more what this is terms of a ''test oracle''; its stronger than described because the adverary can test the effect of an insertion without making the insertion!s}
\item[S2:] The adversary has access to the details of the HLL implementation, i.e., the adversary knows the number of buckets $2^n$ and the hash function $h$ in use, but is otherwise ``blind''. In particular, an S2 adversary does not have access to the values of the HLL cardinality estimate at any point in its attack. \mathilde{Would it make sense to add a sentence "This scenario would apply to~\cite{portscanhll}."?}
\item[S3:] The adversary has access to the details of the HLL implementation, i.e., the adversary knows the number of buckets $2^n$ and the hash function $h$ in use. Additionally, he can access and interact with the sketch via an API provided by the sketch owner, allowing him to insert an item or ask for the HLL's cardinality estimate at any point in time.
\kenny{TODO: The external attacker from~\cite{cardestprivacy}  described in terms of oracles.}
\item[S4:] The adversary has direct access to the HLL in use and all its internals, i.e., the number of buckets, the hash function $h$, and also the contents of each bucket in the HLL sketch. Recall that these buckets hold the intermediate estimators that are averaged in a later step to compute the final cardinality estimate. Again, an S4 adversary does not have access to the values of the HLL cardinality estimate at any point in its attack, but it can compute these values for itself (assuming its attack proceeds without input from other users being added to the HLL sketch in parallel with the attack). \mathilde{I am not sure about the last sentence. In the \textit{insider} model of \cite{cardestprivacy} the attacker owns the sketch so has access to both statistics (cardinality estimation) and counters in real time. So if he receives data coming from another user / notices an update in the counters, he can check reiterate over the discarded items and check whether he can retain some more? I know the goal of \cite{cardestprivacy} is different and there it makes sense to consider the scenario where the owner is malicious but not necessarily for us.}
\end{itemize}

Scenarios S1 and S2 are presented as M2 and M1, respectively, in~\cite{hllvuln} and S3 and S4 as the \textit{external} and \textit{internal} attack scenarios in \cite{cardestprivacy}.

We start by highlighting the fact that S1 seems unrealistic in practice, as there is no reason why a sketch owner would provide such \textit{test oracle}. The authors of \cite{hllvuln} argue that the assumptions of S1 could also be fulfilled if the attacker gains access to a replica of the machine the HLL is implemented on. The adversary could run tests on it, sequentially inserting, asking for the cardinality estimation and resetting the device, retaining only the items which do not increment the HLL estimate. \mathilde{Todo: Add sentence This second approach does not look much more convincing/practical.}
%This is unclear what this means in practice, since after insertion into the HLL sketch, an item cannot usually be removed by the adversary.
\kenny{Mathilde: can you think about this sentence that I added? Do we need to rewrite S1? The scenario does not really make sense at all in practice. I think we might use this to beat up the other paper a bit more.}

Furthermore, we show that although \cite{cardestprivacy} presents S3 and S4 as detached scenarios, the attacker with the capabilities of S3 can obtain the knowledge of S4. By analyzing which inputs increase the cardinality estimation, an attacker can easily recover the contents of each bucket in the sketch. To guess the estimator of a targeted bucket, the attacker gradually inserts items with an incrementing number of leading zeros in the $\ell$ rightmost bits of $h(x)$, until the cardinality estimation of the sketch increases. Once a change occurs, the attacker knows that the maximum number of leading zeros observed in that bucket just got updated to a value that can be recovered by looking at the last inserted item. The attacker can determine all estimators with $O(2^{n + \ell})$ pre-computations and $O(\ell \cdot 2^{n})$ insert and cardinality estimation queries. The fuller the sketch the most work the attacker has to provide.

\kenny{TODO: Show that S3 implies S4 $O(2^{n + \ell})$ precomputation and $O(\ell \cdot 2^{n})$ insert and read queries. So we. focus from now on S2 and S4 -- because S1 is unrealistic and S3 implies S4.}

As the assumptions of S1 appear unrealistic and an attacker can escalate from the knowledge of S3 to the one of S4, we focus on the scenarios S2 and S4. We first present our attack in scenario S2, providing much stronger results than the corresponding attack of \cite{hllvuln}, and then use the additional information the adversary has access to in scenario S4 to make the attack even more powerful.

\subsection{Manipulating the HLL Cardinality Estimate}

\noindent\paragraph{Scenario S2} The adversary knows $h$ so it can adjust fields in the item $x$ it is trying to insert and keep only those for which the $\ell$ rightmost bits of $h(x)$ start with a one bit. In this case, the number of leading zero bits computed by the estimator of the destination bucket is always zero, so the estimator of this bucket is unlikely to be updated and the averaged approximate cardinality is likely to be left unchanged. For example, in the scenario of \cite{portscanhll}, the adversary could change the source port of its IP packet until it is satisfied with its hash value. Given sufficient flexibility in the input $x$, the adversary can insert many items into the HLL without increasing the cardinality markedly, by repeatedly sampling from the input domain.

\noindent\paragraph{Scenario S4} The adversary can adopt the same strategy as for S2, but make the attack more efficient and reduce the sampling requirements with the additional information available in this scenario (namely, the count values per bucket in the HLL sketch). So, it can allow some leading zero bits in the $\ell$ rightmost bits of $h(x)$ so long as this does not increase the current bucket estimate. In other words, if $h(x)$ is mapped to the $i$-th bucket with estimated cardinality $2^{c_i + 1}$ (because a value with $c_i$ leading zeroes was previously observed for this bucket), then, instead of the strict restriction of having a one bit in the first position in the $\ell$ rightmost bits of $h(x)$, the adversary is satisfied when there is a one bit in any of the first $c_i+1$ positions in this substring. In case the adversary hits an empty bucket (for which the estimator is 0), it must skip that bucket completely. Under these conditions, the estimators in the buckets are never updated, and there is no effect on the cardinality estimate made by HLL.  A special case arises when the HLL is completely empty at the beginning of the attack; here the adversary is forced to increase the estimate in at least one bucket, but with sampling this can obviously be done in such a way as to increase the HLL estimate to $2$ in a single bucket, leaving all the other buckets untouched.

\subsection{Impact}
We now evaluate the impact of the above attacks on the HLL cardinality estimate. We assume that $h$ is a ``good" hash function, so that each of the bits of $h(x)$ can be regarded as being independent and uniformly random.

In scenario S2, the adversary needs to craft two distinct candidates for $x$ on average to obtain a one bit at the first position in the $\ell$ rightmost bits of $h(x)$, thus successfully performing the attack for a single insertion. Assuming the adversary has $t$ bits of flexibility in its choice of inputs, the adversary can expect to ``hide''  2$^{t-1}$ distinct items without increasing the HLL cardinality estimate at all, under the assumption that all buckets in the HLL sketch already hold a value $c_i ge 1$. On the other hand, if some buckets are empty (but the adversary does not know which ones in the blind setting of S2), then we may expect a moderate increase in the HLL cardinality estimator. In the worst case, when all buckets are initially empty, the cardinality estimate will increase from 0 to at most $\alpha2^{n+1}$ (regardless of the number of items the adversary inserts).

In scenario S4, we compute the average work required of the adversary to find an input $x$ such that $h(x)$ meets the attack's requirements. Let us assume for the moment that $c_i \ge 1$ for all buckets $i$. Assume $h(x)$ is mapped to bucket $i$. The probability that $h(x)$ does not increase the estimator is the probability that the  rightmost $\ell$ bits of $h(x)$ have $c_i$ or less leading zeroes; this probability is equal to $1-\frac{1}{2^{c_i+1}}$ assuming $h$ behaves like a random function. Averaging over all $2^n$ buckets (and using the fact that the bucket choice is uniformly random since the bits of $h$ are independent and uniformly random), the probability that input $x$ does not increase the estimator is given by:
\begin{eqnarray*}
\sum_{i=1}^{2^n}\frac{1}{2^n} \cdot (1-\frac{1}{2^{c_i+1}}) & = & 1-\frac{1}{2}\cdot (\frac{1}{2^n}\cdot \sum_{i=1}^{2^n} {(2^{c_i})}^{-1}) \\
& = &1-(2H_c)^{-1} \\
\end{eqnarray*}
where $H_c$ is the harmonic mean of the counts $2^{c_1}, 2^{c_2}, \ldots, 2^{c_{2^n}}$.

%$P(h(x)$ does NOT update bucket $i| h(x)$ mapped to bucket $i)=P(h(x)$ has LESS than $c_i$ leading 0s$)=1-P(h(x)$ has strictly MORE than $c_i$ leading 0s$)=1-\frac{1}{2^{c_i+1}}$\\
%Across all buckets, the adversary does not update any estimate with probability $P=
%\sum_{i=1}^{2^n}P(h(x)$ mapped to bucket $i$ AND $h(x)$ does NOT update bucket $i) = \sum_{i=1}^{2^n}P(h(x)$ mapped to bucket $i)P(h(x)$ does NOT update bucket $i| h(x)$ mapped to bucket $i) = \sum_{i=1}^{2^n}\frac{1}{2^n} \times (1-\frac{1}{2^{c_i+1}}) = 1-\frac{1}{2}\times(\frac{1}{2^n}\times \sum_{i=1}^{2^n} {(2^{c_i})}^{-1}) = 1-\frac{1}{2}\times\frac{1}{H_c} = 1-(2H_c)^{-1}$, with $H_c$ the harmonic mean of the numbers $2^{c_1}, 2^{c_2}, ..., 2^{c_{2^n}}$.
So the adversary needs to craft on average $(1-(2H_c)^{-1})^{-1}$ distinct candidates for $x$ in order to insert a single candidate, and, given $t$ bits of flexibility in its choice of inputs, can expect to ``hide" up to $(1-(2H_c)^{-1}) \cdot 2^t$ distinct items without increasing the HLL cardinality estimate at all.

%\textit{NOTE: does it take into account the special case of empty bucket ? counters are initialized to $c_i=-oo$ (or -1 in the code???) thus $P(h(x)$ has LESS than $-oo$ leading $0s) = 0$ but it is hard to add it to the probabilities while staying clear.}\\
%2$^{len(h(x))}\times \frac{2H_c-1}{H_c}$ items into the sketch.\\

Note that the above analysis for scenario S4 made the assumption that every bucket count $c_i$ was already at least 1. When some bucket counts are 0, the above analysis needs to be modified in order to avoid touching those buckets altogether. The modified analysis is quite straightforward. Suppose $r \le 2^n-1$ out of $2^n$ buckets have a count of zero. Then the cost of crafting an input $x$ becomes $(1-(2H_c)^{-1})^{-1} \cdot (1- \frac{r}{2^n})^{-1}$ trials, while the total number of items the adversary can expect to insert without increasing the HLL cardinality estimate becomes $(1-(2H_c)^{-1}) \cdot (1- \frac{r}{2^n})\cdot 2^t$. Of course, the adversary can relax its attack and allow some of the empty buckets to be hit. This increases the insertion rate at the cost of an increased final cardinality estimate.

We assumed so far that there are already values coming from other honest users' data in the HLL. We now remove this assumption. The blind adversary in scenario S2 has no way to detect that it is attacking an empty HLL and cannot adapt its strategy, so all buckets that are hit will update their estimator once to $2^{0+1}=2$ items, leading to a final cardinality approximation of $\alpha2^{n+1}$ regardless of the number of items the adversary inserts. However, the adversary in S4 is able to detect that all buckets are empty and adapt its attack accordingly. The adversary can now target and fill one bucket only. This requires fixing $n+1$ bits of $h(x)$ for each input $x$, allowing the adversary to ``hide'' $2^{t-n-1}$ elements while keeping the HLL cardinality estimate to 1. Other work/cardinality-estimate trade-offs are of course possible.

In both scenarios S2 and S4, we see that an adversary who has $t$ bits of flexibility in its inputs can expect to insert exponentially (in $t$) many inputs values into the HLL sketch without increasing the cardinality estimate (or whilst increasing the cardinality estimate slightly in the cases where the HLL sketch has empty buckets in S2 or where the HLL sketch is empty to begin with in S2 and S4). The result is largely independent of the HLL parameters, but does depend on the extent to which the HLL is already filled, via a factor $1-(2H_c)^{-1}$ and the number of empty buckets $r$, via a factor $1- \frac{r}{2^n}$.


\subsection{Experimental Results}
Many different implementations of HLL are available online. Among the repositories collecting more than 350 stars on GitHub, we can cite \cite{clahll} and \cite{datasketch}. HLL is also featured in many frameworks, including Redis \cite{redis}, Google's BigQuery \cite{bigquery} and several products of Apache such as Spark \cite{spahll,spahll2} and Druid \cite{druhll}. It is interesting to point out that all these implementations use a constant number of buckets and a fixed hash function. Thus they are vulnerable to our attacks in scenarios S2 and S4.

As a proof-of-concept, we implemented our attacks against the HLL implementation from \cite{clahll}. The attack code is available at \href{https://github.com/PizzaWhisperer/HLLVuln}{\textit{\url{https://github.com/PizzaWhisperer/HLLVuln}}}. For simplicity, our items are 4-character long ASCII strings, giving us up to 7,311,616 distinct items that can be inserted. We choose $h$ to be the 32-bit Murmur3 hash function from \cite{murmur3code}. \kenny{Why do we get to choose $h$? Isn't it fixed by the specific implementation we are attacking?} \mathilde{The library has a place holder for a 32 bit hash function but lets the user choose it. This was one of my concerns and I was wondering if it would make sense to attack \cite{datasketch} instead (hardcoded $h$, but when we discussed it, you said that due to Kerckhoff's principle, this hash function should be public so it does not matter that much.} We create an HLL sketch using $2^n = 256$ buckets and initialize it with 1,000 random items representing honest users' data.

We then challenge the adversary to pick the largest possible number of items to add to the sketch from our set of. while trying to keep the cardinality estimate as low as possible, either in the S2 or the S4 scenario. In a second time, in order to get results that can be compared to the work of \cite{hllvuln}, we mount our attack in their more restrictive setting: instead of picking from the universe of possible strings, the adversary has to choose from a set of 250'000 random items that are given to him. Finally , for completeness, we also run the attacks against empty sketches. All attacks are ran 30 times and the outcomes are reported in Tab. \ref{table:tab1} and Tab. \ref{table:tab2}, which detail the estimated cardinality at the beginning of the attack, the number of items added by the adversary and the approximation of the cardinality after adding the adversary's items.

Those empirical results confirm our impact analysis on several points. First, when attacking a sketch already containing some items, we can see that the adversary is able to hide approximately half of the items' set under S2, and around 83\%%( $205$'$000 \approx 250'000 \times 0.82 \approx 250$'$000\times(1-\frac{1}{2\times H_c})$, with $H_c\approx\frac{1'000}{256}=3.9$).
$\approx (1-\frac{1}{2\times H_c})$ with $H_c\approx\frac{1'000}{256}=3.9$, under S4. Secondly, when attacking an empty sketch, the final cardinality estimate is always exactly 367 $=\alpha2^{n+1}$ with $\alpha = 0.72$ as specified by the Eq. (29) of \cite{hll} or 1 in respectively S2 and S4.

To assess the efficiency of our attacks, we can confront our results from Tab. \ref{table:tab2} with the attack presented in the Section 5.1 of \cite{hllvuln}, where they target the Redis \cite{redishll} open-source implementation of HLL and perform the attack against an empty sketch. Given a set of 250'000 items, the adversary could choose ``74'390 distinct items and obtain an HLL estimate of only 15'780``~\cite{hllvuln} achieving a five-fold reduction from the true cardinality. In comparison, the adversary following our strategy is able to add 125'000 distinct items while keeping the estimate at 367, demonstrating a reduction by a factor of 340.

\begin{table}[h]
\caption{Result of the attacks over 30 iterations}
\begin{tabular}{| m{8.5em} | m{4em} | m{4em} | m{4em} | m{4em} |}
    \hline
    \textbf{Adv. Scenario} & \multicolumn{2}{c|}{S2} & \multicolumn{2}{c|}{S4} \\ \hline
    \textbf{Initial data?} & yes & no & yes & no \\ \hline
    \textbf{Original Est. Card.} & 1'000 & 0 & 1'005 & 0 \\ \hline
    \textbf{\# Items added} & 3'655'744 & 3'655'740 & 6'090'003 & 14'220 \\ \hline
    \textbf{Final Est. Card.} & 1'011 & 367 & 1'005 & 1 \\ \hline
\end{tabular}
\label{table:tab1}
\end{table}

\begin{table}[h]
\caption{Result of the attacks in the setting of \cite{hllvuln} over 30 iterations}
\begin{tabular}{| m{8.5em} | m{4em} | m{4em} | m{4em} | m{4em} |}
    \hline
    \textbf{Adv. Scenario} & \multicolumn{2}{c|}{S2} & \multicolumn{2}{c|}{S4} \\ \hline
    \textbf{Initial data?} & yes & no & yes & no \\ \hline
    \textbf{Original Est. Card.} & 1'004 & 0 & 998 & 0 \\ \hline
    \textbf{\# Items added} & 124'940 & 125'085 & 204'669 & 501 \\ \hline
    \textbf{Final Est. Card.} & 1'058 & 367 & 998 & 1 \\ \hline
\end{tabular}
\label{table:tab2}
\end{table}

\section{Conclusion}\label{sec:conclusions}
The HyperLogLog algorithm is an elegant and efficient solution to the distinct count problem. Its simple structure makes it easy to code and use, as shows the growing number of available open-source implementations. Nonetheless, disclosing such low level details can be exploited by malicious users to manipulate the estimate of the sketch and break the system that is built on top. Our attack is trivial but powerful and should raise awareness on the security of HLL, such that product designers understand the risks they may encounter when exposed to an adversarial setting. Future work could consist in finding mitigations that do not rely on salt. For example, would the imposed ID setting (like Facebook) solve the issue but still be convenient for the various HLL applications? Also left for future work is to formalize the extension of our attack to the HLL variants, i.e., to show that it also breaks the HLL++ \cite{hllpratice} and sliding HLL \cite{slidinghll} algorithms.

\bibliographystyle{ieeetr}
\bibliography{ref.bib}

\end{document}
