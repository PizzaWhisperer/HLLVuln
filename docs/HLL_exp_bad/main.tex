\documentclass[11pt]{article}
\usepackage{graphicx, amsmath, amsthm, amssymb, setspace, array, url, hyperref, xspace}
\usepackage{a4wide}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\title{HyperLogLog: Exponentially Bad in Adversarial Settings}
\author{Mathilde Raynal, Kenneth G. Paterson\\
\vspace{3mm}
Department of Computer Science, ETH Zurich\\
{\tt mraynal@student.ethz.ch, kenny.paterson@inf.ethz.ch}}

\begin{document}

\maketitle

%\IEEEtitleabstractindextext{%
\begin{abstract}
Computing the count of distinct elements in massive data sets is a common task but naive approaches are memory-expensive. The HyperLogLog (HLL) algorithm (Flajolet \emph{et al.}, 2007) estimates a data stream's cardinality while using significantly less memory than a naive approach at the cost of some accuracy. This trade-off makes the HLL algorithm very attractive for a wide range of applications such as database management and network monitoring, where an exact count may not be needed. Recently, the HLL algorithm has started to be proposed for use in scenarios where the inputs may be adversarially generated, for example detection of network scanning attacks. This prompts an examination of the performance of the HLL algorithm in the face of adversarial inputs. In this cautionary note, we show that in such a setting, the HLL algorithm's estimate of cardinality can be exponentially bad: when an adversary has access to the internals of the HLL algorithm and has some flexibility in choosing what inputs will be recorded, it can manipulate the cardinality estimate to be exponentially smaller than the true cardinality.
\end{abstract}
%}

%\IEEEdisplaynontitleabstractindextext

\section{Introduction}
Naive approaches to computing the cardinality of a big data set, such as sorting the elements or simply maintaining the set of unique elements seen, are impractical at scale. The HyperLogLog algorithm (HLL henceforth)~\cite{hll} is today the most widely used cardinality estimator. It is increasingly used in settings where adversaries may have  incentives to manipulate the estimate made by the algorithm. For example,~\cite{portscanhll} proposes its use for detecting network scanning attacks, while Facebook engineers have reported~\cite{fbhll} that they use HLL for estimating the number of active users. These applications demand a careful evaluation of the performance of HLL under adversarial input.

Very recently, Reviriego and Ting~\cite{hllvuln} initiated work in this direction, showing that in specific attack settings, the cardinality estimate made by HLL could be modified through input selection. In this paper, we present a complete analysis of HLL in adversarial setting, considering more realistic attack scenarios and giving more powerful attacks than~\cite{hllvuln}. In particular, we show that with only modest knowledge of the HLL internals and a moderate amount of computation, an adversary with sufficient flexibility in the choice of inputs can make the HLL cardinality estimate exponentially smaller than the true cardinality (e.g.\ constant instead of $O(2^t)$ where $t$ is the number of bits of flexibility in the input).

The rest of the paper is organised as follows. Section~\ref{sec:overview} gives an overview of the HyperLogLog algorithm. A brief summary of the related works can be found in Section \ref{sec:related}. Section~\ref{sec:attacks} presents our different adversarial models. There, we also give attacks and evaluate their impacts under the different adversarial models. The experimental results are discussed in Section~\ref{sec:exp} and Section~\ref{sec:conclusions} provides our conclusions and some ideas for future work.


\section{Overview of HLL}\label{sec:overview}
\subsection{The Algorithm}
HLL~\cite{hll} is a streaming algorithm based on the key observation that, for a stream of randomly distributed values represented as bit-strings of some fixed length, if we observe a value with the maximum of $k$ leading zero-bits, then the cardinality of the stream (i.e.\ the number of distinct values it contains) is likely to be on the order of $2^{k+1}$. In practice, to ensure a uniform distribution, the stream values are first processed by a hash function $h$ before the leading bits are inspected. So to estimate the cardinality of a stream, we need only to store a representation of $k$, the length of the largest observed string of leading zero-bits for numbers in the stream. This provides a very compact mechanism for estimating the stream cardinality -- if the true cardinality is $N$, then just $\log\log(N)$ bits are needed to store the estimate.

This simple approach suffers from large variance in the estimation. In order to improve the estimate, we can use many estimators in parallel instead of one, and average the results. Each estimate is stored in its own register, that we call a bucket. The collection of buckets is referred to as the HLL \emph{sketch}. To map a value $x$ to one of $2^n$ buckets, \cite{loglog} suggests using the first $n$ bits of $h(x)$ as a bucket index, and then computing the longest sequence of leading zero bits on the remaining $\ell$ bits of $h(x)$. The output length of $h(\cdot)$ is thus $n+\ell$, fixed to 32 in~\cite{hll}. Estimators from all buckets are averaged using the harmonic mean and scaled by a constant $\alpha$, where $\alpha$ is empirically computed to correct a systematic multiplicative bias.

Relatively small and big cardinalities are handled separately to avoid eventual distortions, as shown in Fig.~\ref{fig:hll}. When the raw cardinality estimate is below $\frac{5}{2}\times 2^n$, the Hit Counting algorithm of Whang et al.~\cite{hitcounting} is used to produce the final cardinality estimator, looking at the number of empty buckets (whose register is 0) instead of the number of leading zeros in the stream. A similar correction is applied for large cardinalities in order to take into account the likelihood of hashing collisions. Some works building on top of~\cite{hll} replace the Hit counting algorithm by other schemes and use a hash function with a larger output domain to reduce the chances of collision and thus get rid of the need for large cardinality range correction. For example, \cite{hllpratice} uses a 64-bit hash function and Linear Counting along with bias correction in the small cardinality range. In this report we target the original paper's implementation reported in Fig.~\ref{fig:hll} and leave for future work the analysis of its variations. 

As result, HLL is able to estimate cardinalities greater than $10^9$ with a typical standard error of 2\%, using only 1.5 kB of memory~\cite{hll}.

An interesting property of HLL is that it supports merging in a lossless way. To combine two buckets at index $i$ in two different HLL instances, one can take the maximum of the two bucket entries and assign that to the matching bucket $i$ in the merged HLL sketch. Such a simple set union operation allows easy parallelisation of operations among multiple machines independently, provided they use the same hash function and the same number of buckets.

\begin{figure}[ht]
\centering
     \makebox[\linewidth]{
\fbox{
\begin{minipage}{40em}
\begin{flushleft}
\textit{Let $h : D \rightarrow \{0, 1\}^{32}$ hash data from $\mathcal{D}$ to binary $32$–bit words.\\
Let $\rho(s)$ be the position of the leftmost $1$-bit of $s$: e.g., $\rho(1...) = 1, \rho(0001...) = 4, \rho(0^K) = K + 1.$\\}
\textbf{define} $m = 2^n$ with $n \in [4..16].$\\
\textbf{define} $\alpha_{16} = 0.673$; $\alpha_{32} = 0.697$; $\alpha_{64} = 0.709$; $\alpha_{m} = 0.7213/(1+ 1.079/m)$ for $m \ge 128$;\\
\vspace{0.5em}
\textbf{Algorithm} HyperLogLog (\textbf{input} $\mathcal{M}$ : multiset of items from domain $\mathcal{D}$).\\
\textbf{initialize} a collection of $m$ registers, $M[1], ..., M[m]$, to $0$;\\
\vspace{0.5em}
\textbf{for} $v \in \mathcal{M}$ \textbf{do}\\
    \hspace{2em} \textbf{set} $x := h(v);$\\
    \hspace{2em} \textbf{set} $j = 1 + <x_1x_2...x_{n}>_2$; \hfill \textit{\{the binary address determined by the first $n$ bits of $x$\}}\\
    \hspace{2em} \textbf{set} $w := x_{n+1}x_{n+2}...$;\\
   \hspace{2em} \textbf{set} $M[j] :=$ max$(M[j], \rho(w))$;\\
\vspace{0.2em}
\textbf{compute} $E := \alpha_mm^2\cdot(\sum_{j=1}^m2^{-M[j]})^{-1}$; \hfill \textit{\{the “raw” HyperLogLog estimate\}}\\
\vspace{0.2em}
\textbf{if} $E \le \frac{5}{2}m$ \textbf{then}\\
    \hspace{2em}  \textbf{let} $V$ be the number of registers equal to $0$;\\
    \hspace{2em}  \textbf{if} $V \neq 0$ \textbf{then set} $E^*:= m $ln$(m/V)$ \textbf{else set} $E^*:= E$; \hfill \textit{\{small range correction\}}\\
\textbf{if} $E \le \frac{1}{30}2^{32}$ \textbf{then}\\
    \hspace{2em} \textbf{set} $E^*:= E$; \hfill \textit{\{intermediate range—no correction\}}\\
\textbf{if} $E > \frac{1}{30}2^{32}$ \textbf{then}\\
    \hspace{2em}  \textbf{set} $E^*:= -2^{32}$ln$(1 - E/2^{32})$; \hfill \textit{\{large range correction\}}\\
\textbf{return} \textit{cardinality estimate $E^*$ with typical relative error} $\pm1.04/\sqrt{m}$.
\end{flushleft}
\end{minipage}
}}
\caption{The HyperLogLog Algorithm from~\cite{hll}}
\label{fig:hll}
\end{figure}

\subsection{Applications}
For completeness and motivation, we provide examples of concrete uses of HLL:
\begin{itemize}
    \item Cardinality estimation provides a significantly faster way for Facebook to compute statistics on their users. They use HLL to find out how many distinct people visited their website in the past week~\cite{fbhll}.
    \item HLL has been proposed as a solution to detecting Denial-of-Service attacks~\cite{portscanhll} (this paper actually uses a variation on HLL called \emph{sliding HLL}~\cite{slidinghll} which estimates the cardinality over a moving time window). Specifically, HLL is used to count how many different ports are used as destination port over all packets received, as an attempt to identify when a port scan attack is underway.
    \item HLL has been implemented in network soft-switches to approximate the number of distinct packets in traffic flows and overcome the switch resource limits~\cite{flexswitch}. The number of unique flows traversing a switch can then be used to provide a better congestion control protocol.
\end{itemize}

Regardless of the sensitivity of the above examples, none of the relevant papers are clear about their threat model, especially for the HLL component.  They are all potentially vulnerable to the attacks we present below.

\section{Related Work}\label{sec:related}

The work of~\cite{cardestprivacy} studies the privacy properties of HLL in two different attack scenarios, referred to as \emph{insider} and \emph{external}. The attack target of~\cite{cardestprivacy} is different from ours, being concerned with loss of privacy, whereas we care about cardinality estimation. It is argued in~\cite{cardestprivacy} that HLL can be used by organisations to store and process location data, which is inherently sensitive, so losing privacy would leak users' locations and potentially cause great harm. In Section~\ref{sec:attacks} we will discuss the two attack scenarios from~\cite{cardestprivacy} in more detail. In particular, we will show that an attacker who can insert items into the HLL and who can access the HLL cardinality estimate (via typical APIs in HLL implementations) as permitted in the external scenario can actually realise the same attacks as the insider attacker can, at least insofar as cardinality estimate manipulation is concerned.

In~\cite{hllvuln}, Reviriego and Ting exploit a vulnerability of HLL to manipulate the cardinality estimate, leading to a five-fold reduction in the estimate compared to the true cardinality. Their attack model can be considered to be quite artificial, since it gives the attacker the ability to test whether inserting an item \emph{would} increase the cardinality estimate, without actually having to insert the item. After some clarifications obtained from personal correspondence with the authors, we finally agree that such setup might be possible under certain circumstances. See further discussion in Section~\ref{sec:attacks}.

Both~\cite{cardestprivacy,hllvuln} propose mitigations using a salted sketch, either as replacement (but at the cost of losing mergeability) or in addition to an unsalted sketch (inducing a memory overhead).
%This might not be a convenient solution for some applications, they thus encourage further research.

\section{HLL in Adversarial Settings}\label{sec:attacks}

This section presents different adversarial settings and corresponding attacks that manipulate the HLL cardinality estimate. The adversary's goal is to reduce the estimate as much as possible, since this would cause more damage when  considering the above-mentioned applications of HLL. It is easy to modify our attack strategies for an adversary who instead wishes to artificially inflate the HLL estimate given a limited input capability. We also analyze the impact on the HLL cardinality estimate and the computational effort required.

Throughout, we assume the adversary has the ability to vary the contents of free fields in the input strings which will be inserted into the stream of values. For example it can manipulate IPID and other header fields in the context of IP packets. This enables the adversary to control the output of hash function $h$ and flexibly choose which items will be inserted into the HLL sketch.

\subsection{Adversarial Setting}\label{sec:set}
We model four distinct adversarial scenarios based on the adversary's knowledge and capabilities. To simulate real-life settings, we consider both cases when the sketch is shared and receives inputs from other honest users but also when inputs are under adversarial control only.
It is safe to assume that the adversary knows the setting being used. 
Those two settings differ mainly in the fact that the adversary is attacking a sketch that already contains other users' data in the former option, but is empty in the latter one.

Among the scenarios, the \textit{insider} setup of~\cite{cardestprivacy} that we present as S4 makes the strongest assumptions about the adversarial capabilities since it assumes that the attacker has a perfect view on the sketch.

\begin{itemize}
\item[S1:] The adversary does not know the details of the target HLL implementation but can access a copy of the HLL instance as a user. He can use the API to insert elements into this shadow sketch, get its cardinality estimate, and reset it to its native empty state. The attacker does not know the state of the targeted HLL sketch, meaning that he does not have the list of elements previously inserted into it by other users.
\item[S2:] The adversary has access to the details of the HLL implementation, i.e., the adversary knows the number of buckets $2^n$ and the hash function $h$ in use, but is otherwise ``blind''. In particular, an S2 adversary does not have access to the values of the HLL cardinality estimate at any point in its attack. This kind of attack scenario is appropriate in the context of the port scanning attack of~\cite{portscanhll}.
\item[S3:] The adversary has access to the details of the HLL implementation, i.e., the adversary knows the number of buckets $2^n$ and the hash function $h$ in use. Additionally, it can access and interact with the sketch via an API provided by the sketch owner, allowing it to insert items into the HLL sketch and ask for the HLL's cardinality estimate at any point in time.
\item[S4:] The adversary has direct access to the HLL in use and all its internals, i.e., the number of buckets, the hash function $h$, and also the contents of each bucket in the HLL sketch \emph{at one specific point in time}, i.e.\ it is given a \emph{snapshot} of the sketch. It then causes chosen inputs to be inserted into the HLL sketch. An S4 adversary can compute a lower bound on the HLL cardinality estimate based on its snapshot and whatever it causes to be inserted into the HLL sketch during its attack (this is irrespective of whether its attack proceeds with input from other users being added to the HLL sketch in parallel with its attack or not; if it knows that there is no other input, then it can compute the \emph{exact} value of the HLL cardinality estimate at every point in its attack instead of a lower bound).
\end{itemize}

Scenarios S1 and S2 are presented as M2 and M1, respectively, in~\cite{hllvuln}. S3 is the \textit{external} attack scenario in~\cite{cardestprivacy}. S4 is a slightly weaker version of the \textit{insider} attack scenario in~\cite{cardestprivacy}, the difference being that an insider adversary in~\cite{cardestprivacy} has continuous access to the HLL sketch internals including all bucket values, while our adversary in S4 only has access to a snapshot of the HLL sketch internals at the start of its attack.

We start by highlighting the fact that we do not consider scenario S1 typical of HLL deployments. In a networking application, the authors of~\cite{hllvuln} argue that the assumptions of S1 could be fulfilled if the attacker learns which machine is used to do the monitoring, buys the same, and uses it as the shadow device. It is possible that the provider uses a different salt or key in the hash per device. This would make any pre-computation on the items using the shadow device irrelevant, and considerably weaken the attack presented in Section 3.2 of~\cite{hllvuln}. Hence, we assume for the rest of this work that shadow and targeted devices have the same internals and security parameters. If the HLL device receives inputs from several users, the attack from Section 3.2 of~\cite{hllvuln} is the easiest to carry out. It works as follow: "after each insertion [in the shadow device], elements that do not increase the cardinality estimate are retained". Iterating through the process several times refines the items' filtering before inserting them into the targeted device. We explore in this work the setup where inputs are coming only from the adversary, and propose an attack that provides better results than the one we just presented. 

Furthermore, we show that although~\cite{cardestprivacy} presents S3 (external adversary) and the stronger version of our S4 (insider adversary) as two separate scenarios, an S3 adversary can easily infer the knowledge given to our S4 adversary in its snapshot. Indeed, an adversary can easily recover the contents of each bucket in the sketch simply by analysing which inputs increase the cardinality estimate. To find the estimate held in a targeted bucket, the attacker gradually inserts items $x$ with an incrementing number of leading zeros in the $\ell$ rightmost bits of $h(x)$ whilst holding the $n$ bits determining the bucket index constant, until the cardinality estimate of the sketch increases. Once an increase occurs, the attacker knows that the maximum number of leading zeros observed in that bucket was just updated to a value that can be recovered by looking at the last inserted item. This assumes the attacker has sufficient flexibility in its choice of inputs $x$ so that the relevant bit conditions on $h(x)$ can be forced. The attacker can determine all bucket contents with, in the worst case, $O(2^{n + \ell})$ pre-computation and $O(\ell \cdot 2^{n})$ insert and cardinality estimate queries, and needs about $n + \ell$ bits of flexibility assuming $h$ behaves like a random function. The pre-computation and query complexity required can be much less if the HLL sketch is only moderately loaded. A version of this attack can be carried out to elevate an external adversary to a full-strength insider adversary as per~\cite{cardestprivacy} (instead of our slightly weaker S4 adversary), since it can simply be done repeatedly each time the external adversary wants to know the complete internals of the HLL sketch. Obvious optimisations can be carried out to reduce the query complexity each time the attack is run (since the bucket contents can only increase over time, some queries are redundant; moreover, the pre-computation is only needed once and is moderate given typical HLL parameters, e.g.\ $n+\ell = 32$).

Since an attacker can escalate from S3 to S4, we focus on scenarios S1, S2 and S4 in what follows. We first present an attack in scenario S1, providing much stronger results than the corresponding attack of~\cite{hllvuln} (in their equivalent scenario M2). We then use the additional information the adversary has access to in scenarios S2 and S4 to make the attack even more powerful.

\subsection{Manipulating the HLL Cardinality Estimate}
\noindent\paragraph{Scenario S1} The goal of the attacker is to infer some information about targeted items' hash values, precisely their bucket mappings, without knowing neither $h$ nor the salt in use (if any). With this knowledge, the adversary is able to retain items that are mapped to a subset of $b$ buckets out of the $2^n$, and minimize the resulting estimated cardinality. To achieve this, the attacker can take advantage of the Hit Counting algorithm initial phase to recover the number of non-empty buckets (whose register is not 0) after inserting few items. Indeed, in the small cardinality range, the cardinality estimator E* is computed as E*$=2^n$ln$(2^n/V)$ with $V$ the number of empty buckets. For typical HLL parameters, e.g.\ $n = 8$, the exact value of $V$, hence $b$, can be recovered from E*. Moreover, when inserting an item in the small cardinality range, the adversary will see a increase in the cardinality estimate if and only if $V$ is modified. This means that the number of empty buckets decreased by 1, therefore that the freshly inserted item is mapped to a bucket that was not hit before. 

Thus, if the adversary blindly picks the first $b$ buckets to be hit as its targeted buckets, he can then discard all items whose insertion causes the cardinality estimator to increase from $b+k$ to $b+k+1$ for $k \in \mathbb{N}$. In other words, when $b$ buckets are already hit and the insertion of an item causes the cardinality estimator to increase, the attacker can discard this item as he knows that a new bucket (other than one of the $b$ buckets to be filled) switched to a non-empty state. The adversary can keep inserting items and discarding some while in the small cardinality range.

As soon as the cardinality estimator exceeds $\frac{5}{2}\times 2^n$, the adversary can assume that it corresponds to the raw HLL cardinality estimate with high probability. In this case, no more useful information can be inferred about the bucket mappings, so the attacker has to reset the sketch to the empty state and start again with the non-discarded items. The adversary must pay attention not to shuffle the non-discarded items in between iterations, as the $b$ buckets to be filled are blindly determined by the first inserted items, and must stay the same throughout the attack. After several iterations, the adversary discards all items mapped to unwanted buckets, such that $b$ buckets are filled and the others left untouched.

The value of $b$ has to be picked smartly to keep the final cardinality estimate low, as the registers must hold values potentially very high. $b$ is going to be chosen such that even in the worst case scenario where all register values $M[j]$ hold the the maximal value of $l+1$, the Hit Counting algorithm is going to be used to produce the final estimation. So $b$ must satisfy:
\begin{align}
E:=\alpha_mm^2\cdot(\sum_{j=1}^m2^{-M[j]})^{-1} & \le \frac{5}{2}m \nonumber\\
\alpha_mm^2\cdot((m-b)\cdot2^{-0}+b\cdot2^{-(l+1)})^{-1} & \le \frac{5}{2}m \nonumber\\
b & \le m\cdot(1-\frac{2\alpha_m}{5})\cdot(1-2^{-(l+1)})^{-1} \label{eqn:1} 
\end{align}
with $m=2^n$. For example with $n = 8$ and $l = 24$, $b$ can take any value up to 184. The initial set of items to be filtered can be built by varying the flexible fields of the items $x$ the adversary is trying to insert.
%so that the adversary can ``hide'' $2^{t-n}$ elements while keeping the HLL cardinality estimate to 1.

\noindent\paragraph{Scenario S2} The adversary knows $h$ so it can adjust fields in the item $x$ it is trying to insert and keep only those for which the $\ell$ rightmost bits of $h(x)$ start with a one bit. In this case, the number of leading zero bits computed by the estimator of the destination bucket is always zero, so the estimator of this bucket is unlikely to be updated and the final averaged approximate cardinality is likely to be left unchanged.
For example, in the scenario of~\cite{portscanhll}, the adversary could change the source port of its IP packet until it is satisfied with its hash value. Given sufficient flexibility in the input $x$, the adversary can insert many items into the HLL sketch without increasing the cardinality markedly, by repeatedly sampling from the input domain.

When attacking an empty sketch, the adversary can keep this strategy and already mount a powerful attack, but can do better by following the approach presented under S1. Filtering items is easier in this scenario as the adversary can infer the bucket mapping of an item $x$ directly from the first $n$ bits of $h(x)$. A trivial approach for the adversary is to retain items mapped to buckets 1 to $b$. We argue why targeting $b$ buckets is a better approach in the impact analysis presented in Section~\ref{sec:impact}. 

\noindent\paragraph{Scenario S4} The adversary can adopt the same strategy as for S2, but make the attack more efficient and reduce the sampling requirements with the additional information available in this scenario (namely, the count values per bucket in the HLL sketch). Therefore, it can allow some leading zero bits in the $\ell$ rightmost bits of $h(x)$ so long as this does not increase the current bucket estimate. In other words, if $h(x)$ is mapped to the $i$-th bucket with estimated cardinality $2^{M[i]}=2^{c_i + 1}$ (because a value with $c_i$ leading zeroes was previously observed for this bucket), then, instead of the strict restriction of having a one bit in the first position in the $\ell$ rightmost bits of $h(x)$, the adversary is satisfied when there is a one bit in any of the first $c_i+1$ positions in this substring. In case the adversary hits an empty bucket (for which the estimator is 0), it must skip that bucket completely. Under these conditions, the estimators in the buckets are never updated, and there is no effect on the cardinality estimate made by HLL.

A special case arises when the HLL is completely empty at the beginning of the attack. Here the attacker is forced to increase the estimate in at least one bucket. Once again, the adversary can choose to fill $b$ buckets following the strategy described in S2, leaving all the other buckets untouched.

\subsection{Impact}\label{sec:impact}
In this section we evaluate the impact of the above attacks on the HLL cardinality estimate. We assume that $h$ is a ``good" hash function, so that each of the bits of $h(x)$ can be regarded as being independent and uniformly random. We assume that the adversary has $t$ bits of flexibility in its choice of inputs.

\subsubsection{Non-Empty Sketch}\label{sec:nonempty}

We start by analysing our results when the attack is ran against a sketch that already received inputs from other users, first under scenario S2 then S4. 

In scenario S2, the adversary needs to craft two distinct candidates for $x$ on average to obtain a one bit at the first position in the $\ell$ rightmost bits of $h(x)$, thus successfully performing the attack for a single insertion. The adversary can expect to ``hide''  $2^{t-1}$ distinct items without increasing the HLL cardinality estimate at all, under the assumption that all buckets in the HLL sketch already hold a value $c_i \ge 1$. On the other hand, if some buckets are empty (but the adversary does not know which ones in the blind setting of S2), then we may expect a moderate increase in the HLL cardinality estimator. Notice that in all cases, provided $t$ is large enough, the adversary can keep the cardinality estimate linear in the total number of buckets ($2^n$) while the expected value of the true cardinality, $2^{t-1}$, can be made as large as the adversary pleases.

In scenario S4, we compute the average work required of the adversary to find an input $x$ such that $h(x)$ meets the attack's requirements. Let us assume for the moment that $c_i \ge 1$ for all buckets $i$. Assume $h(x)$ is mapped to bucket $i$. The probability that $h(x)$ does not increase the estimator is the probability that the  rightmost $\ell$ bits of $h(x)$ have $c_i$ or less leading zeroes; this probability is equal to $1-\frac{1}{2^{c_i+1}}$ assuming $h$ behaves like a random function. Averaging over all $2^n$ buckets (and using the fact that the bucket choice is uniformly random since the bits of $h$ are independent and uniformly random), the probability that input $x$ does not increase the estimator is given by:
\begin{eqnarray*}
\sum_{i=1}^{2^n}\frac{1}{2^n} \cdot (1-\frac{1}{2^{c_i+1}}) & = & 1-\frac{1}{2}\cdot (\frac{1}{2^n}\cdot \sum_{i=1}^{2^n} {(2^{c_i})}^{-1}) \\
& = &1-(2H_c)^{-1} 
\end{eqnarray*}
where $H_c$ is the harmonic mean of the counts $2^{c_1}, 2^{c_2}, \ldots, 2^{c_{2^n}}$.
So the adversary needs to craft on average $(1-(2H_c)^{-1})^{-1}$ distinct candidates for $x$ in order to insert a single candidate, and, given $t$ bits of flexibility in its choice of inputs, can expect to ``hide" up to $(1-(2H_c)^{-1}) \cdot 2^t$ distinct items without increasing the HLL cardinality estimate at all.

Note that the above analysis for scenario S4 made the assumption that every bucket count $c_i$ was already at least 1. When some bucket counts are 0, the above analysis needs to be modified in order to avoid touching those buckets altogether. The modified analysis is quite straightforward. Suppose $V \le 2^n-1$ out of $2^n$ buckets have a count of zero. Then the cost of crafting an input $x$ becomes $(1-(2H_c)^{-1})^{-1} \cdot (1- \frac{V}{2^n})^{-1}$ trials, while the total number of items the adversary can expect to insert without increasing the HLL cardinality estimate becomes $(1-(2H_c)^{-1}) \cdot (1- \frac{V}{2^n})\cdot 2^t$. Of course, the adversary can relax its attack and allow some of the empty buckets to be hit. This would increase the insertion rate at the cost of a small increase in the final cardinality estimate. 

\textbf{Conclusion:} In this setting, we see that an adversary who has $t$ bits of flexibility in its inputs can expect to insert exponentially (in $t$) many input values into the HLL sketch without increasing the cardinality estimate (or whilst increasing the cardinality estimate slightly in the cases where the HLL sketch has empty buckets in S2). The result is largely independent of the HLL parameters, but does depend on the extent to which the HLL sketch is already filled by other users, via a factor $1-(2H_c)^{-1}$ and the number of empty buckets $V$, via a factor $1- \frac{V}{2^n}$. Other trade-offs between work and the cardinality estimate are possible.

\subsubsection{Empty Sketch}

We now consider the case where the HLL sketch is initially empty.
In that case, the output of the attacks is the same for all scenarios.
Targeting $b$ buckets allows the adversary to insert a fraction $\frac{b}{2^n}$ of the items, and thus to retain $\frac{b}{2^n}2^t=2^{t-n+\text{log}_2(b)}$ elements.
After inserting the retained items, the cardinality of the sketch is estimated to $2^n$ln$(2^n/(2^n-b))$, approximating to $b$ for relatively small values of $b$.

When following the initial strategy of S2, we saw in paragraph~\ref{sec:nonempty} that the adversary retains $2^{t-1}$ items. The insertion of those items will hit every bucket of the sketch with high probability, and update every register once to 1. The cardinality of the sketch is then estimated to $\alpha2^{2n}(\sum_{j=1}^{2^n}2^{-1})^{-1}=\alpha2^{n+1}$ and, as $V=0$, no small range cardinality correction is applied. 
On the other hand, when targeting and filling half the buckets ($b=2^{n-1}$), the adversary can insert as many items but lower the estimated cardinality to $2^n$ln$(2^n/(2^n-2^{n-1}))=2^n\text{ln}(2)$ instead of $\alpha{2^{n+1}}$ that we previously had, at no extra computational cost.

\textbf{Conclusion:} When confronted to an empty sketch, the adversary is forced to concede a slight cardinally estimation increase in order to be able to insert some elements.
For a fixed $b$, an adversary who has $t$ bits of flexibility in its inputs is then able to insert exponentially (in $t$) many input values into the HLL sketch while keeping the cardinality estimate constant.
Any value below the upper-bound specified in eq. (4.1) can be chosen for $b$ based on the setup requirements. For example if the reset operation is costly, the attacker can adjust $b$ accordingly. 


\section{Experimental Results}\label{sec:exp}
Many different implementations of HLL are available online. Among the repositories collecting more than 350 stars on GitHub, we can cite~\cite{clahll} and~\cite{datasketch}. HLL is also featured in many frameworks, including Redis~\cite{redis}, Google's BigQuery~\cite{bigquery}, Facebook's Airlift~\cite{airlift} and several products of Apache such as Spark~\cite{spahll} and Druid~\cite{druhll}. It is interesting to point out that all these implementations use a constant number of buckets and a fixed hash function. Thus they are vulnerable to our attacks in scenarios S2 and S4.

As a proof-of-concept, we implemented our attacks against the HLL implementation from~\cite{clahll}. The attack code is available at \href{https://github.com/PizzaWhisperer/HLLVuln}{\textit{\url{https://github.com/PizzaWhisperer/HLLVuln}}}. For simplicity, our items are 4-character long ASCII strings, giving us up to $7,311,616= 2^{22.8}$ distinct items that can be inserted (hence we have $t=22.8$ in our attacks). We choose $h$ to be the 32-bit Murmur3 hash function from~\cite{murmur3code} (note that the library allows the hash function to be specified; we pick Murmur3 for our attack and consider it reasonable that the adversary should know the hash function in use).
We create an HLL sketch using $2^n = 256$ buckets and initialise it with 1,000 random items representing honest users' data.

We then challenge the adversary to pick the largest possible number of items to add to the sketch from our set of  $7,311,616$ possible inputs while trying to keep the cardinality estimate as low as possible, either in the S1, S2 or S4 scenario. As a second experiment, and in order to get results that can be compared to the work of~\cite{hllvuln}, we mount our attack in their more restrictive setting: instead of picking from the universe of possible strings, the adversary has to choose what to insert from a set of 250,000 random items that are given to it. Finally, for completeness, we also run the attacks against HLL sketches filled with adversarial inputs only, thus that are initially empty. We vary the value of $b$ in the attack under the S1 scenario to demonstrate possible trade-offs. For simplicity and to keep the attack as striking as possible, we set $b$ to 1 in the attacks on empty sketch under S2 and S4. All attacks were run 30 times and the outcomes averaged. Our results are reported in Tables~\ref{table:tab1} and~\ref{table:tab2}, which detail the estimated cardinality at the beginning of the attack, the number of items added by the adversary, and the cardinality estimate after adding the adversary's items.

These empirical results confirm our impact analysis on several points.

First, when attacking a sketch already containing some items, we can see that the adversary is on average able to hide about half of the items in our starting set under S2 (as predicted by our theoretical analysis), and about 83\% under S4. %( $205$'$000 \approx 250'000 \times 0.82 \approx 250$'$000\times(1-\frac{1}{2\times H_c})$, with $H_c\approx\frac{1'000}{256}=3.9$)
Here $0.83 \approx (1-\frac{1}{2\cdot H_c})$ with $H_c\approx\frac{1000}{256}=3.9$, so the results are also consistent with our theoretical analysis.

Secondly, when attacking an empty HLL sketch, we can see that in all scenarios, the final cardinality estimate is always exactly $2^n$ln$(2^n/(2^n-b))$ after inserting a fraction $\frac{b}{2^n}$ of the items (this comes from the Hit Counting algorithm result that arises from our attack targeting $b$ buckets).

We can also compare our results from Table~\ref{table:tab2} with results from the attack presented in Section 5.1 of~\cite{hllvuln}. In that paper, the authors target the Redis~\cite{redis} open-source implementation of HLL and perform the attack against an empty sketch filled with adversarial inputs only. Given a set of 250,000 items, the adversary could choose ``74,390 distinct items and obtain an HLL estimate of only 15,780''~\cite{hllvuln}, achieving a five-fold reduction from the true cardinality after 4 iterations (resets). The S1 adversary following our strategy is able to add 977 and 97,433 distinct items while keeping the cardinality estimate respectively at 1 and 126. Compared to~\cite{hllvuln}, we can reduce the cardinality estimate by a factor of almost 1,000 instead of 5 at the cost of more resets.

\begin{table}[h]
\centering
\caption{Attack results, averaged over 30 iterations.}
     \makebox[\linewidth]{
\begin{tabular}{| m{9.8em} | m{4em} | m{4.2em} | m{4.1em} | m{4.1em} | m{4.1em} | m{4em} |}
    \hline
    \textbf{Scenario} &  \multicolumn{2}{c|}{S1} & \multicolumn{2}{c|}{S2} & \multicolumn{2}{c|}{S4} \\ \hline
    \textbf{\# Targeted buckets} & 1 & 100 & - & 1 & - & 1 \\ \hline
    \textbf{Initial data?} & no & no & yes & no & yes & no \\ \hline
    \textbf{Original Card. Est.} & 0 & 0 & 1,000 & 0 & 1,005 & 0 \\ \hline
    \textbf{\# Items added} & 28,566 & 2,856,100 & 3,655,744 & 28,566 & 6,090,003 & 28,566 \\ \hline
    \textbf{Final Card. Est.} & 1 & 126 & 1'011 & 1 & 1,005 & 1 \\ \hline
\end{tabular}}
\label{table:tab1}
\end{table}

\begin{table}[h]
\centering
\caption{Attack results, averaged over 30 iterations, in the setting of \cite{hllvuln}.}
     \makebox[\linewidth]{
\begin{tabular}{| m{9.8em} | m{4em} | m{4.2em} | m{4.1em} | m{4.1em} | m{4.1em} | m{4em} |}
    \hline
    \textbf{Scenario} & \multicolumn{2}{c|}{S1} & \multicolumn{2}{c|}{S2} & \multicolumn{2}{c|}{S4} \\ \hline
    \textbf{\# Targeted buckets} & 1 & 100 & - & 1 & - & 1 \\ \hline
    \textbf{Initial data?} & no & no & yes & no & yes & no \\ \hline
    \textbf{Original Card. Est.} & 0 & 0 & 1,004 & 0 & 998 & 0 \\ \hline
    \textbf{\# Items added} & 977 & 97,433 & 124,940 & 979 & 204,669 & 988 \\ \hline
    \textbf{Final Card. Est.} & 1 & 126 & 1,058 & 1 & 998 & 1 \\ \hline
\end{tabular}}
\label{table:tab2}
\end{table}

\section{Conclusion}\label{sec:conclusions}
The HyperLogLog algorithm is an elegant and efficient solution to the problem of estimating the cardinality of large sets. Its simple structure makes it easy to code and use, as shown by the growing number of available open-source implementations. Nonetheless, malicious users can manipulate the HLL cardinality estimate and thence break the security properties of systems relying on HLL. Our attacks are simple but powerful and should raise awareness of the limitations of HLL. Our analysis may assist software developers in understanding the risks they run when using HLL in adversarial settings.

A formal analysis of using salting as a countermeasure to our attacks remains open.
%Future work could consist in finding mitigations that do not rely on salt. For example, would the imposed ID setting (like Facebook) solve the issue but still be convenient for the various HLL applications?
Also left for future work is the task of extending our attacks to HLL variants such as sliding HLL~\cite{slidinghll}, HLL++~\cite{hllpratice} and Redis alternative using the maximum likelihood approach from~\cite{newhll}.


\bibliographystyle{ieeetr}
\bibliography{ref.bib}

\end{document}
