\documentclass{IEEEtran}
\usepackage{graphicx}
\usepackage{array}
\usepackage{url}
\usepackage{hyperref}


%% TODO NOTES

\usepackage{xcolor}
\definecolor{oxygenorange}{HTML}{FFDD00}
\usepackage[color=oxygenorange]{todonotes}
\newcommand{\mathilde}[2][inline]{\todo[#1]{\textbf{Mathilde:} #2}\xspace}
\newcommand{\kenny}[2][inline]{\todo[#1]{\textbf{Kenny:} #2}\xspace}

\title{HyperLogLog: Exponentially Bad in Adversarial Settings}
\author{Mathilde Raynal, Kenneth G. Paterson}

\begin{document}

\maketitle

\IEEEtitleabstractindextext{%
\begin{abstract}
Computing the count of distinct elements in massive data sets is a common task but naive approaches are memory-expensive. The HyperLogLog (HLL) algorithm (Durand and Flajolet, 2003) estimates a data stream's cardinality while using significantly less memory than a naive approach at the cost of some accuracy. This trade-off makes the HLL algorithm very attractive for a wide range of applications such as database management and network monitoring, where an exact count may not be needed. Recently, the HLL algorithm has started to be proposed for use in scenarios where the inputs may be adversarially generated, for example detection of network scanning attacks. This prompts an examination of the performance of the HLL algorithm in the face of adversarial inputs. In this cautionary note, we show that in such a setting, the HLL algorithm's estimate of cardinality can be exponentially bad: when an adversary has access to the internals of the HLL algorithm, an adversary who has some flexibility in choosing what inputs will be recorded can manipulate the cardinality estimate to be exponentially smaller than the true cardinality. 
\end{abstract}
}

\IEEEdisplaynontitleabstractindextext

\section{Introduction}
Naive approaches to computing the cardinality of a big data set, such as sorting the elements or simply maintaining the set of unique elements seen, are impractical at scale. The HyperLogLog algorithm (HLL henceforth)~\cite{loglog,hll2} is today the most widely used cardinality estimator. It is increasingly used in settings where adversaries may have  incentives to manipulate the estimate made by the algorithm. For example,~\cite{portscanhll} proposes its use for detecting network scanning attacks, while Facebook engineers have reported~\cite{fbhll} that they use HLL for estimating the number of active users. These applications demand a careful evaluation of the performance of HLL under adversarial input. 

Very recently, Reviriego and Ting~\cite{hllvuln} initiated work in this direction, showing that in specific attack settings, the cardinality estimate made by HLL could be modified through input selection. In this paper, we present a complete analysis of HLL in adversarial setting, considering more realistic attack scenarios and giving more powerful attacks than~\cite{hllvuln}. In particular, we show that with only modest knowledge of the HLL internals and a moderate amount of computation, an adversary can make HLL underestimate true cardinality by a factor this is exponential in the number of buckets used by the algorithm.

The rest of the paper is organised as follows. Section~\ref{sec:overview} gives an overview of the HyperLogLog algorithm. Section~\ref{sec:attacks} presents our different adversarial models. There, we also give attacks and evaluate their impacts under the different adversarial models. Section~\ref{sec:conclusions} provides our conclusions and some ideas for future work.


 
\section{Overview of HLL}\label{sec:overview}
\subsection{The Algorithm}
HLL~\cite{loglog,hll2} is a streaming algorithm based on the key observation that, for a stream of randomly distributed values represented as bit-strings of some fixed length, if we observe a value with the maximum of $k$ leading zero-bits, then the cardinality of the stream (i.e.\ the number of distinct values it contains) is likely to be on the order of $2^{k+1}$. In practice, to ensure a uniform distribution, the stream values are first processed by a hash function $h$ before the leading bits are inspected. So to estimate the cardinality of a stream, we need only store a representation of $k$, the length of the largest observed string of leading zero-bits for numbers in the stream. This provides a very compact mechanism for estimating the stream cardinality -- if the true cardinality is $N$, then just $\log\log(N)$ bits are needed to store the estimate. 

This simple approach suffers from large variance in the estimation. In order to improve the estimate, we can use many estimators in parallel instead of one, and average the results. Each estimate is stored in its own bucket. The collection of buckets is referred to as the HLL \emph{sketch}. To map a value $x$ to one of $2^n$ buckets, \cite{loglog} suggests using the first $n$ bits of $h(x)$ as a bucket index, and then computing the longest sequence of leading zero bits on the remaining $\ell$ bits of $h(x)$ (so that the output length of $h(\cdot)$ is $n+\ell$). Estimators from all buckets are averaged using the harmonic mean and scaled by a constant $\alpha$, where $\alpha$ empirically computed to correct a systematic multiplicative bias. As result, HLL is able to estimate cardinalities greater than $10^9$ with a typical standard error of 2\%, using only 1.5 kB of memory~\cite{add-ref-for-this-claim}.

An interesting property of HLL is that it supports merging in a lossless way. To combine two buckets at index $i$ in two different HLL instances, take the maximum of the two bucket entries and assign that to the matching bucket $i$ in the merged HLL sketch. Such a simple set union operation allows easy parallelisation of operations among multiple machines independently, provided they use the same hash function and the same number of buckets.

\subsection{Applications}
For completeness and motivation, we provide examples of concrete uses of HLL:
\begin{itemize}
    \item Cardinality estimation provides a significantly faster way for Facebook to compute statistics on their users. They use HLL to find out how many distinct people visited their website in the past week~\cite{fbhll}.
    \item HLL has been proposed as a solution to detecting Denial-of-Service attacks~\cite{portscanhll} (this paper actually uses a variation on HLL called \emph{sliding HLL}~\cite{slidinghll} which estimates the cardinality over a moving time window). Specifically, HLL is used to count how many different ports are used as destination port over all packets received, as an attempt to identify when a port scan attack is underway.
        \item HLL has been implemented in network switches to approximate the number of distinct packets in traffic flows and overcome the switch resource limits~\cite{flexswitch}. The number of unique flows traversing a switch can then be used to provide a better congestion control protocol.
        \end{itemize}

Regardless of the sensitivity of the above examples, none of the relevant papers are clear about their threat model, especially for the HLL component.  They are all potentially vulnerable to the attacks we present below.
%By not assessing the risks involved with the presence of an adversary targeting the HLL sketch, they may allow an attack like the one we present, breaking down the whole solution.

\subsection{Related Work}

The work of~\cite{cardestprivacy} breaks the privacy of HLL and concludes that having the ability to test the insertion \kenny{what does this mean? see other comment later} of a targeted item gives information about whether the item was originally present in the input data set. It is argued in~\cite{cardestprivacy} that HLL can be used by organisations to store and process location data, which is inherently sensitive, so losing privacy would leak users' locations and potentially cause great harm. 

In~\cite{hllvuln}, Reviriego and Ting exploit a vulnerability of HLL to manipulate the cardinality estimate, leading to a five-fold reduction in the estimate compared to the true cardinality.

Both~\cite{cardestprivacy,hllvuln} propose mitigations using a salted sketch, either as replacement (but at the cost of losing mergability) or in addition to an unsalted sketch (inducing a memory overhead). 
%This might not be a convenient solution for some applications, they thus encourage further research. 

\section{HLL in Adversarial Settings}\label{sec:attacks}

This section presents different adversarial settings and corresponding attacks that manipulate the HLL cardinality estimate. The adversary's goal is to reduce the estimate as much as possible, since this would cause more damage when  considering the above-mentioned applications of HLL. It is easy to modify our attack strategies for an adversary who instead wishes to artificially inflate the HLL estimate given a limited input capability. We also analyze the impact on the HLL cardinality estimate and the computational effort required. 

Throughout, we assume the adversary has the ability to vary the contents of free fields in the input strings which will be inserted into the stream of values. For example it can manipulate IPID and other header fields in the context of IP packets. This enables the adversary to control the output of hash function $h$ and flexibly choose which items will be inserted into the HLL sketch, and in which buckets.

\subsection{Adversarial Setting}
We model three distinct adversarial scenarios based on the adversary's knowledge and capabilities. For all of them, we assume that the sketch already contains some existing data from honest users to simulate a real-life setting. Among the scenarios, S3 makes the strongest assumptions about the adversarial capabilities, since it combines the knowledge of adversarys from S1 and S2.

\begin{itemize}
\item[S1:] the adversary has black-box access to the HLL in use -- that is, the adversary can ask for the HLL's cardinality estimate at any point in time, and so is able to check the effect on the cardinality estimate when inserting an item; 
\item[S2:] the adversary has access to the details of the HLL implementation, i.e., the adversary knows the number of buckets $2^n$ and the hash function $h$ in use.
\item[S3:] the adversary has direct access to the HLL in use and all its internals, i.e., the number of buckets, the hash function $h$, and also the contents of each bucket in the HLL sketch. Recall that these buckets hold the intermediate estimators that are averaged in a later step to compute the final cardinality estimate.
\end{itemize}

Scenarios S1 and S2 are presented as M2 and M1, respectively, in~\cite{hllvuln}. We do not put our focus on S1 as this previous work in~\cite{hllvuln} fully exploits this scenario. Their attack works as follows: ``after each insertion the cardinality is checked and the items which do not increment the HLL estimate are retained''. It is unclear what this means in practice, since after insertion into the HLL, an item cannot usually be removed by the adversary. \kenny{Mathilde: can you think about this sentence that I added? Do we need to rewrite S1? The scenario does not really make sense at all in practice. I think we might use this to beat up the other paper a bit more. Maybe it also applies to the ``privacy'' paper.}

We start by presenting our attack in scenario S2, providing much stronger results than the corresponding attack of \cite{hllvuln}, and then use the additional information the adversary has access to in scenario S3 to make the attack even more powerful. 

\subsection{Manipulating the HLL Cardinality Estimate}

\noindent\paragraph{Scenario S2:} The adversary knows $h$ so it can adjust fields in the item $x$ it is trying to insert and keep only those for which the $\ell$ rightmost bits of $h(x)$ start with a one bit. In this case, the number of leading zero bits computed by the estimator of the destination bucket is always zero, so the estimator of this bucket is unlikely to be updated and the averaged approximate cardinality is likely to be left unchanged. For example, in the scenario of \cite{portscanhll}, the adversary could change the source port of his IP packet until it is satisfied with its hash value. Given sufficient flexibility in the input $x$, the adversary can insert many items into the HLL without increasing the cardinality markedly, by repeatedly sampling from the input domain.

\noindent\paragraph{Scenario S3:} The adversary can adopt the same strategy as for S2, but make the attack more efficient and reduce the sampling requirements with the additional information available in this scenario (namely, the count values per bucket in the HLL sketch). So, it can allow some leading zero bits in the $\ell$ rightmost bits of $h(x)$ so long as this does not increase the current bucket estimate. In other words, if $h(x)$ is mapped to the $i$-th bucket with estimated cardinality $2^{c_i + 1}$ (because a value with $c_i$ leading zeroes was previously observed for this bucket), then, instead of the strict restriction of having a one bit in the first position in the $\ell$ rightmost bits of $h(x)$, the adversary is satisfied when there is a one bit in any of the first $c_i+1$ positions in this substring. In case the adversary hits an empty bucket (for which the estimator is 0), it must skip that bucket completely. Under these conditions, the estimators in the buckets are never updated, and there is no effect on the cardinality estimate made by HLL.  A special case arises when the HLL is completely empty at the beginning of the attack; here the adversary is forced to increase the estimate in at least one bucket, but with sampling this can obviously be done in such a way as to increase the HLL estimate to $2$ in a single bucket, leaving all the other buckets untouched.

\subsection{Impact}
We now evaluate the impact of the above attacks on the HLL cardinality estimate. We assume that $h$ is a ``good" hash function, so that each of the bits of $h(x)$ can be regarded as being independent and uniformly random.

In scenario S2, the adversary needs to craft two distinct candidates for $x$ on average to obtain a one bit at the first position in the $\ell$ rightmost bits of $h(x)$, thus successfully performing the attack for a single insertion. Assuming the adversary has $t$ bits of flexibility in its choice of inputs, the adversary can expect to ``hide''  2$^{t-1}$ distinct items without increasing the HLL cardinality estimate at all.

In scenario S3, we compute the average work required of the adversary to find an input $x$ such that $h(x)$ meets the attack's requirements. Let us assume for the moment that $c_i \ge 1$ for all buckets $i$. Assume $h(x)$ is mapped to bucket $i$. The probability that $h(x)$ does not increase the estimator is the probability that the  rightmost $\ell$ bits of $h(x)$ have $c_i$ or less leading zeroes; this probability is equal to $1-\frac{1}{2^{c_i+1}}$ assuming $h$ behaves like a random function. Averaging over all $2^n$ buckets (and using the fact that the bucket choice is uniformly random since the bits of $h$ are independent and uniformly random), the probability that input $x$ does not increase the estimator is given by:
\begin{eqnarray*}
\sum_{i=1}^{2^n}\frac{1}{2^n} \cdot (1-\frac{1}{2^{c_i+1}}) & = & 1-\frac{1}{2}\cdot (\frac{1}{2^n}\cdot \sum_{i=1}^{2^n} {(2^{c_i})}^{-1}) \\
& = &1-(2H_c)^{-1} \\
\end{eqnarray*}
where $H_c$ is the harmonic mean of the counts $2^{c_1}, 2^{c_2}, \ldots, 2^{c_{2^n}}$.

%$P(h(x)$ does NOT update bucket $i| h(x)$ mapped to bucket $i)=P(h(x)$ has LESS than $c_i$ leading 0s$)=1-P(h(x)$ has strictly MORE than $c_i$ leading 0s$)=1-\frac{1}{2^{c_i+1}}$\\
%Across all buckets, the adversary does not update any estimate with probability $P=
%\sum_{i=1}^{2^n}P(h(x)$ mapped to bucket $i$ AND $h(x)$ does NOT update bucket $i) = \sum_{i=1}^{2^n}P(h(x)$ mapped to bucket $i)P(h(x)$ does NOT update bucket $i| h(x)$ mapped to bucket $i) = \sum_{i=1}^{2^n}\frac{1}{2^n} \times (1-\frac{1}{2^{c_i+1}}) = 1-\frac{1}{2}\times(\frac{1}{2^n}\times \sum_{i=1}^{2^n} {(2^{c_i})}^{-1}) = 1-\frac{1}{2}\times\frac{1}{H_c} = 1-(2H_c)^{-1}$, with $H_c$ the harmonic mean of the numbers $2^{c_1}, 2^{c_2}, ..., 2^{c_{2^n}}$.
So the adversary needs to craft on average $(1-(2H_c)^{-1})^{-1}$ distinct candidates for $x$ in order to insert a single candidate, and, given $t$ bits of flexibility in its choice of inputs, can expect to ``hide" up to $(1-(2H_c)^{-1}) \cdot 2^t$ distinct items without increasing the HLL cardinality estimate at all.

%\textit{NOTE: does it take into account the special case of empty bucket ? counters are initialized to $c_i=-oo$ (or -1 in the code???) thus $P(h(x)$ has LESS than $-oo$ leading $0s) = 0$ but it is hard to add it to the probabilities while staying clear.}\\
%2$^{len(h(x))}\times \frac{2H_c-1}{H_c}$ items into the sketch.\\

Note that the above analysis made the assumption that every bucket count $c_i$ was already at least 1. When some bucket counts are 0, the above analysis needs to be modified in order to avoid touching those buckets altogether. The modified analysis is quite straightforward. Suppose $r \le 2^n-1$ out of $2^n$ buckets have a count of zero. Then the cost of crafting an input $x$ becomes $(1-(2H_c)^{-1})^{-1} \cdot (1- \frac{r}{2^n})^{-1}$ trials, while the total number of items the adversary can expect to insert becomes $(1-(2H_c)^{-1}) \cdot (1- \frac{r}{2^n})\cdot 2^t$.

We assumed so far that there are already values coming from other honest users' data in the HLL. We now remove this assumption. Unfortunately, the adversary in scenario S2 has no way to detect that he is attacking an empty HLL and cannot adapt his strategy, so all buckets hit will update their estimator once to $2^{0+1}=2$ items, leading to a final cardinality approximation of $\alpha2^{n+1}$ regardless of the number of items the adversary inserts. However, the adversary in S3 is able to detect that all buckets are empty and adapt his attack accordingly. The adversary can target and fill one bucket only. This requires fixing $n+1$ bits of $h(x)$ for each input $x$, allowing the adversary to ``hide'' $2^{t-n-1}$ elements while keeping the HLL cardinality estimate to 1. Other work/cardinality estimate trade-offs are of course possible.

\subsection{Experimental Results}
There exist many implementations of the HyperLogLog algorithm available online. Among the repositories collecting more than 350 stars on github, we can cite \cite{clahll} and \cite{datasketch}. It is also featured in many frameworks, including Redis \cite{redis}, Google's BigQuery \cite{bigquery} and several products of Apache such as Spark \cite{spahll} \cite{spahll2} and Druid \cite{druhll}. It is interesting to point out that they all use a constant number of buckets and a fixed hash function, \textit{which is public or should be public according to Kerckhoffs's principle,} thus are potentially vulnerable to cardinality estimate manipulation when put in an adversarial setting.\\

As proof-of-concept, we implement our attack against the HLL algorithm from \cite{clahll}. The code is available at \href{https://github.com/PizzaWhisperer/HLLVuln}{\textit{\url{https://github.com/PizzaWhisperer/HLLVuln}}}. Our items are 4 character long ASCII strings \textit{(7'311'616 distinct items)} and we choose $h$ to be the 32 bits Murmur3 hash function from \cite{murmur3code}. We create an HLL sketch using 256 buckets and initialize it with 1'000 random items representing an honest user's data.

We challenge the adversary to pick the largest possible number of items to add to the sketch while trying to keep the cardinality estimate as low as possible, either in the S2 or the S3 scenario. In a second time, in order to get results that can be compared to the work of \cite{hllvuln}, we mount our attack in their more restrictive setting: instead of picking from the universe of possible strings, the adversary has to choose from a set of 250'000 random items that are given to him. Finally , for completeness, we also run the attacks against empty sketches. All attacks are ran 30 times and the outcomes are reported in Tab. \ref{table:tab1} and Tab. \ref{table:tab2}, which detail the estimated cardinality at the beginning of the attack, the number of items added by the adversary and the approximation of the cardinality after adding the adversary's items.

Those empirical results confirm our impact analysis on several points. First, when attacking a sketch already containing some items, we can see that the adversary is able to hide approximately half of the items' set under S2, and around 83\%%( $205$'$000 \approx 250'000 \times 0.82 \approx 250$'$000\times(1-\frac{1}{2\times H_c})$, with $H_c\approx\frac{1'000}{256}=3.9$).
$\approx (1-\frac{1}{2\times H_c})$ with $H_c\approx\frac{1'000}{256}=3.9$, under S3.
Secondly, when attacking an empty sketch, the final cardinality estimate is always exactly 367 $=\alpha2^{n+1}$ with $\alpha = 0.72$ as specified by the Eq. (29) of \cite{hll2} or 1 in respectively S2 and S3.

To assess the efficiency of our attacks, we can confront our results from Tab. \ref{table:tab2} with the attack presented in the Section 5.1 of \cite{hllvuln}, where they target the Redis \cite{redishll} open-source implementation of HLL and perform the attack against an empty sketch. Given a set of 250'000 items, the adversary could choose ``74'390 distinct items and obtain an HLL estimate of only 15'780`` achieving a five-fold reduction from the true cardinality. In comparison, the adversary following our strategy is able to add 125'000 distinct items while keeping the estimate at 367, demonstrating a reduction by a factor of 340. 

\begin{table}[h]
\caption{Result of the attacks over 30 iterations}
\begin{tabular}{| m{8.5em} | m{4em} | m{4em} | m{4em} | m{4em} |}
    \hline
    \textbf{Scenario} & \multicolumn{2}{c|}{S2} & \multicolumn{2}{c|}{S3} \\ \hline
    \textbf{Initial data?} & yes & no & yes & no \\ \hline
    \textbf{Original Est. Card.} & 1'000 & 0 & 1'005 & 0 \\ \hline
    \textbf{\# Items added} & 3'655'744 & 3'655'740 & 6'090'003 & 14'220 \\ \hline
    \textbf{Final Est. Card.} & 1'011 & 367 & 1'005 & 1 \\ \hline
\end{tabular}
\label{table:tab1}
\end{table}

\begin{table}[h]
\caption{Result of the attacks in the setting of \cite{hllvuln} over 30 iterations}
\begin{tabular}{| m{8.5em} | m{4em} | m{4em} | m{4em} | m{4em} |}
    \hline
    \textbf{Scenario} & \multicolumn{2}{c|}{S2} & \multicolumn{2}{c|}{S3} \\ \hline
    \textbf{Initial data?} & yes & no & yes & no \\ \hline
    \textbf{Original Est. Card.} & 1'004 & 0 & 998 & 0 \\ \hline
    \textbf{\# Items added} & 124'940 & 125'085 & 204'669 & 501 \\ \hline
    \textbf{Final Est. Card.} & 1'058 & 367 & 998 & 1 \\ \hline
\end{tabular}
\label{table:tab2}
\end{table}

\section{Conclusion}\label{sec:conclusions}
The HyperLogLog algorithm is an elegant and efficient solution to the distinct count problem. Its simple structure makes it easy to code and use, as shows the growing number of available open-source implementations. Nonetheless, disclosing such low level details can be exploited by malicious users to manipulate the estimate of the sketch and break the system that is built on top. Our attack is trivial but powerful and should raise awareness on the security of HLL, such that product designers understand the risks they may encounter when exposed to an adversarial setting. Future work could consist in finding mitigations that do not rely on salt. For example, would the imposed ID setting (like Facebook) solve the issue but still be convenient for the various HLL applications? Also left for future work is to formalize the extension of our attack to the HLL variants, i.e., to show that it also breaks the HLL++ \cite{hllpratice} and sliding HLL \cite{slidinghll} algorithms.

\bibliographystyle{ieeetr}
\bibliography{ref.bib}

\end{document}
