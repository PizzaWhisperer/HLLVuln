\documentclass[11pt]{article}
\usepackage{graphicx, amsmath, amsthm, amssymb, setspace, array, url, hyperref, xspace, todonotes, pifont}
\usepackage{a4wide}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\title{HyperLogLog: Exponentially Bad in Adversarial Settings}
\author{Kenneth G. Paterson and Mathilde Raynal\\
\vspace{3mm}
Department of Computer Science, ETH Zurich\\
{\tt kenny.paterson@inf.ethz.ch, mraynal@student.ethz.ch}}

\begin{document}

\maketitle

%\IEEEtitleabstractindextext{%
\begin{abstract}
Computing the count of distinct elements in massive data sets is a common task but naive approaches are memory-expensive. The HyperLogLog (HLL) algorithm (Flajolet \emph{et al.}, 2007) estimates a data set's cardinality while using significantly less memory than a naive approach at the cost of some accuracy. This trade-off makes the HLL algorithm very attractive for a wide range of applications such as database management and network monitoring, where an exact count may not be needed. Recently, the HLL algorithm has started to be proposed for use in scenarios where the inputs may be adversarially generated, for example counting social network users or detection of network scanning attacks. This prompts an examination of the performance of the HLL algorithm in the face of adversarial inputs. We show that in such a setting, the HLL algorithm's estimate of cardinality can be exponentially bad: when an adversary has access to the internals of the HLL algorithm and has some flexibility in choosing what inputs will be recorded, it can manipulate the cardinality estimate to be exponentially smaller than the true cardinality. We study both the original HLL algorithm and a more modern version of it (Ertl, 2017) that is used in Redis. We present experimental results confirming our theoretical analysis. Finally, we consider attack prevention: we show to modify HLL in a simple way that provably prevents cardinality estimate manipulation attacks.
\end{abstract}
%}

%\IEEEdisplaynontitleabstractindextext

\section{Introduction}
Naive approaches to computing the cardinality of large data sets, such as sorting the elements or simply maintaining the set of unique elements seen, are impractical at scale and in streaming settings. The HyperLogLog algorithm (HLL henceforth)~\cite{hll} is today the most widely used cardinality estimator. It is increasingly used in settings where adversaries may have  incentives to manipulate the estimate made by the algorithm. For example:
\begin{itemize}
    \item Cardinality estimation provides a significantly faster way for Facebook to compute statistics on their users. They use HLL to find out how many distinct people visited their website in the past week~\cite{fbhll}.
    \item HLL has been proposed as a solution to detecting Denial-of-Service attacks~\cite{portscanhll} (this paper actually uses a variation on HLL called \emph{sliding HLL}~\cite{slidinghll} which estimates the cardinality over a moving time window). Specifically, HLL is used to count how many different ports are used as destination port over all packets received, as an attempt to identify when a port scan attack is underway.
    \item HLL has been implemented in network soft-switches to approximate the number of distinct packets in traffic flows and overcome the switch resource limits~\cite{flexswitch}. The number of unique flows traversing a switch can then be used to provide a better congestion control protocol.
\end{itemize}

Regardless of the sensitivity of the above examples, none of the above-cited sources are clear about their threat model, especially for the HLL component. They are all potentially vulnerable to the attacks we present in this paper.

Very recently, Reviriego and Ting~\cite{hllvuln} initiated work on the performance of HLL in adversarial settings. They showed that in very specific attack settings, the cardinality estimate made by HLL could be modified through input selection. In this paper, we present a complete analysis of HLL in adversarial settings, considering more realistic attack scenarios and giving more powerful attacks than~\cite{hllvuln}. In particular, we show that with only modest knowledge of the HLL internals and a moderate amount of computation, an adversary with sufficient flexibility in the choice of inputs can make the HLL cardinality estimate exponentially smaller than the true cardinality (e.g.\ constant instead of ${\cal O}(2^t)$ where $t$ is the number of bits of flexibility in the adversary's input). We demonstrate this theoretically and experimentally for the ``classic'' version of HLL. We also study a specific ``modern'' version of HLL which adopts recommendation from~\cite{hllnew} and show how variants of our attacks apply there too. This modern version of HLL is interesting because it is the one used in Redis, a popular key-value database technology.

To complement our attack results, we provide a formal security analysis of HLL. We show that in the ``shadow device'' attack scenario of~\cite{hllvuln}, no security is possible even if the HLL is secretly keyed. On the other hand, we also show that in the most powerful attack setting introduced here, where the attacker has full access to the internals of the HLL (except for a secret key!), it is possible to provably secure HLL against cardinality manipulation by the simple expedient of replacing its internal hash function by a keyed, variable-input-length pseudo-random function (VIL-PRF). We describe a suitable instantiation of the required VIL-PRF using off-the-shelf cryptographic components. 

The rest of the paper is organised as follows. Section~\ref{sec:related} immediately below discusses related work. Section~\ref{sec:overview} gives an overview of HLL. Section~\ref{sec:attacks} presents our different adversarial models. In Section~\ref{sec:manip}, we describe attacks on HLL and evaluate their impacts under the different adversarial models. Section~\ref{sec:redis} considers the security of the more modern version of HLL due to Ertl~\cite{hllnew} that is used in Redis. Our experimental results are discussed in Section~\ref{sec:exp} while Section~\ref{sec:formal} provides our formal security analysis of HLL. Section~\ref{sec:conclusions} provides our conclusions and some ideas for future work.

\subsection{Related Work}\label{sec:related}

Desfontaines \emph{et al.}~\cite{cardestprivacy} studied the privacy properties of HLL in two different attack scenarios, referred to as \emph{insider} and \emph{external}. The attack target of~\cite{cardestprivacy} is different from ours, being concerned with loss of privacy, whereas we care about manipulating cardinality estimation. It is argued in~\cite{cardestprivacy} that HLL can be used by organisations to store and process location data, which is inherently sensitive, so losing privacy would leak users' locations and potentially cause great harm. In Section~\ref{sec:attacks} we will discuss the two attack scenarios from~\cite{cardestprivacy} in more detail. 
%In particular, we will show that an adversary who can insert items into the HLL and who can access the HLL cardinality estimate (via typical APIs in HLL implementations) as permitted in the external scenario of~\cite{cardestprivacy} can actually realise the same attacks as the insider adversary can, at least insofar as cardinality estimate manipulation is concerned.

In~\cite{hllvuln}, Reviriego and Ting exploit a vulnerability of HLL to manipulate the cardinality estimate, leading to a five-fold reduction in the estimate compared to the true cardinality. We consider the attack model used in~\cite{hllvuln} to be quite artificial, since it gives the adversary the ability to test whether inserting an item \emph{would} increase the cardinality estimate, without actually having to insert the item. After some clarifications obtained through personal correspondence with the authors of~\cite{hllvuln}, we understand that such a setup might be possible under certain circumstances, for example where the adversary has access to an identical ``shadow'' device; see further discussion in Section~\ref{sec:attacks}. For completeness, we include analyses of this attack scenario, showing that we can drastically improve on the attacks in~\cite{hllvuln} (e.g.\ we can reduce the HLL cardinality estimate by a factor of almost 1,000 instead of the factor of 5 achieved in~\cite{hllvuln}).

Both~\cite{cardestprivacy,hllvuln} propose mitigations using a salted sketch, either as a replacement for (but at the cost of losing mergeability) or in addition to (inducing a memory overhead) an unsalted sketch. We show in Section~\ref{sec:formal} that when a salt is used appropriately (i.e.\ as the secret key for a pseudo-random function that is used in place of hash function $h$), security against adversarial inputs can be achieved even in our strongest adversarial setting. 
%This might not be a convenient solution for some applications, they thus encourage further research.

Clayton \emph{et al.}~\cite{CCS:ClaPatShrPS19} provide a provable-security treatment of probabilistic data structures in adversarial environments more generally. They use their formalism to analyze Bloom filters, counting (Bloom) filters and count-min sketch data structures, but not HLL. %It is an interesting open problem to use their framework to develop a formal understanding of how HLL can be made secure in the adversarial setting. 

\section{Overview of HLL}\label{sec:overview}
\subsection{The Algorithm}
HLL~\cite{hll} is a streaming algorithm based on the key observation that, for a stream of randomly distributed values represented as bit-strings of some fixed length, if we observe a value with the maximum of $k$ leading zero-bits, then the cardinality of the stream (i.e.\ the number of distinct values it contains) is likely to be on the order of $2^{k+1}$. In practice, to ensure a uniform distribution, the stream values are first processed by a hash function $h$ before the leading bits are inspected. So to estimate the cardinality of a stream, we need only to store a representation of $k$, the length of the largest observed string of leading zero-bits for numbers in the stream. This provides a very compact mechanism for estimating the stream cardinality -- if the true cardinality is $N$, then just $\log\log(N)$ bits are needed to store the estimate.

This simple approach suffers from large variance in the estimation. In order to improve the estimate, we can use many estimators in parallel instead of one, and average the results. Each estimate is stored in its own register, that we call a bucket. We refer to the collection of buckets and their contents as the HLL \emph{sketch}. To map a value $x$ to one of $m=2^n$ buckets, \cite{loglog} suggests using the first $n$ bits of $h(x)$ as a bucket index, and then computing the longest sequence of leading zero bits on the remaining $\ell$ bits of $h(x)$. The output length of $h(\cdot)$ is thus $n+\ell$, fixed to 32 in~\cite{hll}. Estimates from all buckets are averaged using the harmonic mean and scaled by a constant $\alpha_m$, where $\alpha_m$ is empirically computed to correct a systematic multiplicative bias.

Relatively small and big cardinalities are handled separately to correct for systematic distortions, as shown in Fig.~\ref{fig:hll}. When the raw cardinality estimate is below $\frac{5}{2}m$, the Hit Counting algorithm of Whang \emph{et al.}~\cite{hitcounting} is used to produce the final cardinality estimate, looking at the number of empty buckets (whose register value is 0) instead of the number of leading zeros in the stream. A similar correction is applied for large cardinalities in order to take into account the likelihood of hashing collisions. In addition, the cardinality estimate is usually rounded to a whole number before being output. 

HLL as described in Figure~\ref{fig:hll} is able to estimate cardinalities greater than $10^9$ with a typical standard error of 2\%, using only 1.5 kB of memory~\cite{hll}.

Some works building on~\cite{hll} replace the Hit Counting algorithm by other schemes and use a hash function with a larger output domain to reduce the chances of collision and thus get rid of the need for large cardinality range correction. For example, \cite{hllpractice} uses a 64-bit hash function and Linear Counting along with bias correction in the small cardinality range, while~\cite{hllnew} proposes new estimators based on a more precise statistical analysis of the counting problem. In this report we mostly target the HLL formulation in the original paper~\cite{hll} as represented in Fig.~\ref{fig:hll}. We also study~\cite{hllnew} because of its recent deployment in Redis.  We leave for future work the analysis of further HLL variants. 

An interesting property of HLL is that it supports merging in a lossless way. To combine two buckets at index $i$ in two different HLL instances, one can take the maximum of the two bucket entries and assign that to the matching bucket $i$ in the merged HLL sketch. Such a simple set union operation allows easy parallelisation of operations among multiple machines independently, provided they use the same hash function and the same number of buckets. Clearly this procedure cannot be carried out if the different HLL sketches are differently seeded or keyed.

\begin{figure}[tb!]
\centering
     \makebox[\linewidth]{
\fbox{
\begin{minipage}{40em}
\begin{flushleft}
\textit{Let $h : \mathcal{D} \rightarrow \{0, 1\}^{32}$ hash data from some domain $\mathcal{D}$ to binary $32$-bit strings.\\
For $s$ of bit-length $\ell$, let $\rho(s)$ denote the position of the leftmost $1$-bit of $s$: e.g., $\rho(1...) = 1, \rho(0001...) = 4, \rho(0^\ell) = \ell + 1.$\\}
\textbf{define} $m = 2^n$ with $n \in [4..16].$\\
\textbf{define} $\alpha_{16} = 0.673$; $\alpha_{32} = 0.697$; $\alpha_{64} = 0.709$; $\alpha_{m} = 0.7213/(1+ 1.079/m)$ for $m \ge 128$;\\
\vspace{0.5em}
\textbf{Algorithm} HyperLogLog (\textbf{input} ${\cal X}$ : multiset of items from domain $\mathcal{D}$).\\
\textbf{initialize} a collection of $m$ registers, $M[1], ..., M[m]$, to $0$;\\
\vspace{0.5em}
\textbf{for} $x \in \cal X$ \textbf{do}\\
    \hspace{2em} \textbf{set} $y := h(x);$\\
    \hspace{2em} \textbf{set} $j = 1 + <y_1y_2...y_{n}>_2$; \hfill \textit{\{bucket index determined by the first $n$ bits of $y$\}}\\
    \hspace{2em} \textbf{set} $w := y_{n+1}y_{n+2} \ldots y_{n+\ell}$;\hfill \textit{\{bit string determined by the last $\ell$ bits of $y$\}}\\
   \hspace{2em} \textbf{set} $M[j] :=$ max$(M[j], \rho(w))$;\\
\vspace{0.2em}
\textbf{compute} $E := \alpha_mm^2\cdot(\sum_{j=1}^m2^{-M[j]})^{-1}$; \hfill \textit{\{the raw HyperLogLog estimate\}}\\
\vspace{0.2em}
\textbf{if} $E \le \frac{5}{2}m$ \textbf{then}\\
    \hspace{2em}  \textbf{let} $V$ be the number of registers equal to $0$;\\
    \hspace{2em}  \textbf{if} $V \neq 0$ \textbf{then set} $E^*:=  m\cdot \ln(m/V)$ \textbf{else set} $E^*:= E$; \hfill \textit{\{small range correction\}}\\
\textbf{if} $E \le \frac{1}{30}2^{32}$ \textbf{then}\\
    \hspace{2em} \textbf{set} $E^*:= E$; \hfill \textit{\{intermediate range --- no correction\}}\\
\textbf{if} $E > \frac{1}{30}2^{32}$ \textbf{then}\\
    \hspace{2em}  \textbf{set} $E^*:= -2^{32}$ln$(1 - E/2^{32})$; \hfill \textit{\{large range correction\}}\\
\textbf{return} \textit{cardinality estimate $E^*$ with typical relative error} $\pm1.04/\sqrt{m}$.
\end{flushleft}
\end{minipage}
}}
\caption{The HyperLogLog Algorithm from~\cite{hll}}
\label{fig:hll}
\end{figure}



\section{HLL in Adversarial Settings}\label{sec:attacks}

This section presents different adversarial settings and corresponding attacks that manipulate the HLL cardinality estimate. The adversary's goal is to reduce the estimate as much as possible, since this would cause more damage when  considering the above-mentioned applications of HLL. It is easy to modify our attack strategies for an adversary who instead wishes to artificially inflate the HLL cardinality estimate given a limited input capability. We also analyze the impact on the HLL cardinality estimate and the computational effort required.

Throughout, we assume the adversary has the ability to vary the contents of free fields in the input strings which will be inserted into the stream of values. For example it can manipulate IPID and other header fields in the context of IP packets. We model this by assuming the adversary has access to a large set ${\cal X}$ of potential inputs from which it can select items to insert into the HLL. Our adversary's objective, then, is to insert as many items from ${\cal X}$ as possible whilst keeping the HLL cardinality estimate as low as possible.

% This enables the adversary to control the output of hash function $h$ and flexibly choose which items will be inserted into the HLL sketch.

\subsection{Adversarial Scenarios}\label{sec:set}
We model four distinct adversarial scenarios based on the adversary's knowledge and capabilities. To simulate real-life settings, we consider both cases when the sketch is shared and receives inputs from other honest users but also when inputs are under the sole control of the adversary.
It is reasonable to assume that the adversary knows which case it is in. 
These two settings differ mainly in the fact that the adversary is attacking a sketch that may already contain other users' data in the former option, but is empty in the latter.

Among the scenarios, the \textit{insider} setup of~\cite{cardestprivacy} that we present as S4 makes the strongest assumptions about the adversarial capabilities since it assumes that the adversary has a perfect view of the sketch (at some point in time).

\begin{itemize}
\item[S1:] The adversary does not know the details of the target HLL implementation but can access a shadow copy of the HLL instance via its API. It can use the API to insert elements into this shadow sketch, get its cardinality estimate, and reset it to its native empty state. The adversary does not know the state of the targeted HLL sketch, meaning in particular that it does not have the list of elements previously inserted into it by other users. It can insert items into the target HLL sketch, but is otherwise ``blind''. 
\item[S2:] The adversary has access to the details of the HLL implementation, i.e.\ it knows the number of buckets $m=2^n$ and the hash function $h$ in use. It can insert items into the HLL sketch, but is otherwise blind. In particular, an S2 adversary does not have access to the values of the HLL cardinality estimate at any point in its attack. This kind of attack scenario is appropriate in the context of the port scanning attack of~\cite{portscanhll}.
\item[S3:] The adversary has access to the details of the HLL implementation, i.e.\ it knows the number of buckets $m=2^n$ and the hash function $h$ in use. Additionally, it can access and interact with the sketch via an API provided by the sketch owner, allowing it to insert items into the HLL sketch and ask for the HLL's cardinality estimate at any point in time. However, it does not know the individual bucket contents.
\item[S4:] The adversary has direct access to the HLL in use and all its internals, i.e., the number of buckets, the hash function $h$, and also the contents of each bucket in the HLL sketch \emph{at one specific point in time}, i.e.\ it is given a \emph{snapshot} of the sketch. It can access and interact with the sketch via an API provided by the sketch owner, as per scenario S3. Note that, using these capabilities, an S4 adversary can compute a lower bound on the HLL cardinality estimate based on its initial snapshot and whatever it causes to be inserted into the HLL sketch during its attack (this is irrespective of whether its attack proceeds with input from other users being added to the HLL sketch in parallel with its attack or not; if it knows that there is no other input, then it can compute the \emph{exact} value of the HLL cardinality estimate at every point in its attack instead of a lower bound).
\end{itemize}

Scenarios S1 and S2 are presented as M2 and M1, respectively, in~\cite{hllvuln}. S3 is the \textit{external} attack scenario in~\cite{cardestprivacy}. S4 is a slightly weaker version of the \textit{insider} attack scenario in~\cite{cardestprivacy}, the difference being that an insider adversary in~\cite{cardestprivacy} has continuous access to the HLL sketch internals including all bucket values, while our adversary in S4 only has access to a snapshot of the HLL sketch internals at the start of its attack. We consider a weaker version of S4 than in~\cite{cardestprivacy} because it is sufficient for our attacks; when considering security proofs in Section~\ref{sec:formal}, we will switch back to the stronger version of S4. 

The adversarial capabilities in each of scenarios S1 to S4 are summarised in Table~\ref{tab:scenarios}.

\begin{table}[h]
\centering
\caption{Adversarial capabilities in scenarios S1, S2, S3 and S4.}
\medskip
     \makebox[\linewidth]{
\begin{tabular}{| m{11em} | m{2em} | m{2em} | m{2em} | m{2em} |}
    \hline
    {Scenario} 							&  {S1} & {S2} & {S3} & {S4} \\ \hline
   	{API for shadow device} 		& \cmark & \xmark & \xmark & \xmark \\ \hline
    {Insertion of  items} 				& \cmark & \cmark & \cmark & \cmark \\ \hline
    {$h$ and $m$ known}			& \xmark & \cmark & \cmark & \cmark \\ \hline
    {Get cardinality estimate} 		& \xmark & \xmark & \cmark & \cmark \\ \hline
    {Get snapshot} 						& \xmark & \xmark & \xmark & \cmark \\ \hline
\end{tabular}}
\label{tab:scenarios}
\end{table}

\subsection{Discussion of Adversarial Scenarios}\label{sec:scenarios-discussion}

Scenario S1 is rather distinctive in that it is the only one in which the adversary has access via an API to a shadow device (and no other capabilities). We do not consider scenario S1 typical of HLL deployments. In a networking application, the authors of~\cite{hllvuln} argue that the assumptions of S1 could be fulfilled if the adversary learns which machine is used to do the monitoring, buys the same, and uses it as the shadow device. It is possible that the provider uses a different salt or key in the hash per device. This would make any pre-computation on the items using the shadow device irrelevant, and considerably weaken the attack presented in Section 3.2 of~\cite{hllvuln}. Hence we assume for the rest of this work that, in scenario S1, shadow and targeted devices have the same internals and security parameters (if there are any). If the HLL device receives inputs from several users, the attack from Section 3.2 of~\cite{hllvuln} is then the easiest to carry out. It is described in~\cite{hllvuln} as follows: "after each insertion [in the shadow device], elements that do not increase the cardinality estimate are retained". The idea is to iterate through the process of trial insertion several times to refine the list of items, before inserting what remains into the targeted device. We explore in this work scenario S1 in the special case where inputs come only from the adversary, and propose an attack that provides significantly better results than the one presented in~\cite{hllvuln}. 

Scenarios S2, S3 and S4 gradually increase the power of the adversary. 

At first sight it might appear that an S3 adversary can easily infer the knowledge given to our S4 adversary in its snapshot. Indeed, an S3 adversary could try to recover the contents of each bucket in the sketch simply by analysing which inputs lead to increases in the cardinality estimate: to find the value held in a targeted bucket, the adversary would insert a sequence of items $x$ with an incrementing number of leading zeros in the $\ell$ rightmost bits of $h(x)$ whilst holding the $n$ bits determining the bucket index constant, until the cardinality estimate of the sketch increases. This assumes the adversary has sufficient flexibility in its input that the conditions on $h(x)$ can be forced. 

However, this procedure does not work as simply as just described because of numerical issues that arise in the computation of the cardinality estimate. Specifically, increasing the value held in some bucket from $M[j]$ to $M[j]+1$ \emph{decreases} the denominator in the raw cardinality estimate $\sum_{j=1}^m2^{-M[j]}$ by an amount $2^{-M[j]-1}$. This does lead to a change in the raw cardinality estimate $E = \alpha_mm^2\cdot(\sum_{j=1}^m2^{-M[j]})^{-1}$. However, in practice, the raw cardinality estimate is rounded before being presented to the adversary, so this change may not actually be detectable. We have done experiments which shows this to be the case for parameters arising in practice. 

We do not know of attacks which are able to use the extra information available to an S3 adversary over an S2 adversary (namely, the ability to obtain cardinality estimates). Moreover, as we will see below, our attacks in the S2 setting are already quite powerful. For this reason, we will focus in the remainder of the paper on scenarios S1, S2 and S4.

%Furthermore, we show that although~\cite{cardestprivacy} presents S3 (external adversary) and the stronger version of our S4 (insider adversary) as two separate scenarios, an S3 adversary can easily infer the knowledge given to our S4 adversary in its snapshot. Indeed, an adversary can easily recover the contents of each bucket in the sketch simply by analysing which inputs increase the cardinality estimate. To find the estimate held in a targeted bucket, the adversary  inserts a sequence of items $x$ with an incrementing number of leading zeros in the $\ell$ rightmost bits of $h(x)$ whilst holding the $n$ bits determining the bucket index constant, until the cardinality estimate of the sketch increases.\todo{Really? Can the adversary detect a change to an individual bucket through a cardinality estimate change? It's not really clear -- depends on an analysis of the raw HLL estimate using the harmonic mean which we haven't done! We may need to heavily revise all this text.}
% Once an increase occurs, the adversary knows that the maximum number of leading zeros observed in that bucket was just updated to a value that can be recovered by looking at the last inserted item. This assumes the adversary has sufficient flexibility in its choice of inputs $x$ so that the relevant bit conditions on $h(x)$ can be forced. The adversary can determine all bucket contents with, in the worst case, $O(2^{n + \ell})$ pre-computation and $O(\ell \cdot 2^{n})$ insert and cardinality estimate queries, and needs about $n + \ell$ bits of flexibility assuming $h$ behaves like a random function. The pre-computation and query complexity required can be much less if the HLL sketch is only moderately loaded, since then most buckets will store values corresponding to inputs with few leading zero bits. A version of this attack can be carried out to elevate an external adversary to a full-strength insider adversary as per~\cite{cardestprivacy} (instead of our slightly weaker S4 adversary), since it can simply be done each time the external adversary wants to obtain a snapshot of the internals of the HLL sketch. Obvious optimisations can be carried out to reduce the query complexity each time the attack is run (since the bucket contents can only increase over time, some queries are redundant; moreover, the pre-computation is only needed once and is moderate given typical HLL parameters, e.g.\ $n+\ell = 32$).

%Since an adversary can quite efficiently escalate from S3 to S4, we focus on scenarios S1, S2 and S4 in what follows. 

%We first present an attack in scenario S1, providing much stronger results than the corresponding attack of~\cite{hllvuln} (in their equivalent scenario M2). We then use the additional information the adversary has access to in scenarios S2 and S4 to make the attack even more powerful.

\section{Manipulating HLL Cardinality Estimates}\label{sec:manip}

We divide our analysis into two cases: when the HLL sketch is initially empty (and the adversary knows this), and when it is not. For each case, we consider adversaries operating in scenarios S1, S2 and S4.


\subsection{Attacking an Initially Empty HLL Sketch}\label{sec:empty} 

We begin with a preliminary lemma, applying to an adversary operating against an initially empty sketch (in any of S1, S2, S4). As usual, we have $m=2^n$ buckets, while $n+\ell$ is the output length of the employed hash function $h$.

\begin{lemma}\label{lem:bound}
Suppose an adversary inserts elements $x$ from some set ${\cal X}$ into an initially empty HLL sketch. Suppose the adversary can guarantee that in its attack, only a subset of size $B$ out of $2^m$ buckets are hit, where $B \leq B_{\max}$ with 
\[
B_{\max} := \left\lfloor m\cdot\left(1-\frac{2\alpha_m}{5}\right)\cdot\left(1-2^{-(\ell+1)}\right)^{-1} \right\rfloor. \label{eqn:1} 
\]
Then, at the end of the attack, the HLL cardinality estimate is at most $m \cdot \ln (m/V)$ where $V = m-B$.
\end{lemma}

\begin{proof}
Recall that the Hit Counting algorithm is used in the HLL algorithm as described in Figure~\ref{fig:hll} when the raw 
estimate $E = \alpha_mm^2\cdot(\sum_{j=1}^m2^{-M[j]})^{-1}$ does not exceed $\frac{5}{2}m$. Here $M[j]$ is the counter stored in bucket $j$. Note that $E$ increases with increasing $M[j]$ and we have $B$ buckets each contributing, in the worst case, $2^{-(\ell+1)}$ to the sum (and hence $m-B$ buckets which contribute $2^{0}=1$ to the sum). Hence, we see that for the Hit Counting algorithm to be used, we require:
\[
\alpha_mm^2\cdot\left((m-B)+B\cdot2^{-(\ell+1)}\right)^{-1} \le \frac{5}{2}m.
\]
Solving for $B$, we obtain:
\[
B \le B_{\max} := \left\lfloor m\cdot\left(1-\frac{2\alpha_m}{5}\right)\cdot\left(1-2^{-(\ell+1)}\right)^{-1} \right\rfloor.
\]
When the Hit Counting algorithm is used, the HLL cardinality estimate is set as $m \cdot \ln (m/V)$ where $V = m-B$ is the number of non-empty buckets. This completes the proof.
\end{proof}

For example with $n = 8$ (so $m=256$) and $\ell = 24$, we have $\alpha_m = 0.72$ and we find that $B_{\max}=184$. More generally, since for large $m$ we have $\alpha_m \approx 0.72$, and the term $(1-2^{-(\ell+1)})^{-1}$ is always close to but greater than 1, we see that $B_{\max} \approx  0.71\cdot m$. Note also that $m \cdot \ln (m/V)$ can be rewritten as $m \cdot \ln (1+\frac{B}{m-B})$.

A surprising implication of the above analysis is that, for the HLL algorithm as described in Figure~\ref{fig:hll}, provided $B \leq B_{\max} \approx  0.71\cdot m$, no matter which fixed set of $B$ buckets are hit in an attack, and no matter how many leading zero bits the items' hashes have, the final cardinality estimate will not exceed $m \cdot \ln (1+\frac{B}{m-B})$. Setting, say, $B = m/2$, so half of the buckets can be hit in an attack, we would obtain a bound of $m \cdot \ln 2$ on the cardinality estimate.

\subsubsection{Initially Empty HLL Sketch, Scenarios S2 and S4:} 

Armed with the above lemma, we now consider scenarios S2 and S4. We begin by noting that, when dealing with an initially empty sketch, S2 and S4 are actually equivalent, since the only additional information that an S4 adversary has is a snapshot of the HLL sketch, and this information is effectively given to the S2 adversary by virtue of the attack setting. So we consider S2 and S4 together.

Now our proposed attack is very simple. Suppose the adversary has some set ${\cal X}$ of $2^t$ items from which to pick its inputs. The adversary selects a value of $B \leq B_{\max}$, and defines its target buckets to be those numbered $1,\ldots,B$. Since the adversary knows $h$, it can tell whether, for each $x \in {\cal X}$, insertion of $x$ into the HLL sketch would hit one of the target buckets or not; it inserts those that do. In expectation, assuming $h$ behaves like a random function, the adversary will successfully insert a fraction $B/m$ of the items, totalling $(B/m)\cdot 2^t = 2^{t-n+\log_2(B)}$ items. Meanwhile the final cardinality estimate will remain below $m \cdot \ln (1+\frac{B}{m-B})$. 

For example, setting $B = m/2$ (which, for practical parameters, satisfies $B \leq B_{\max}$), we can expect to insert $2^{t-1}$ items while keeping the cardinality estimate below $m \ln 2$.

As a second example, setting $B = 1$, we can expect to insert $2^{t-n}$ items with a final cardinality estimate equal to $m \cdot \ln (1+\frac{1}{m-1})$, which is rounded to $1$ by the HLL algorithm (for $m$ of practical interest, e.g.\ $m=256$). Thus we keep the HLL cardinality estimate to a constant (1!) whilst being able to insert exponentially many items, provided there is sufficient flexibility in the choice of input set ${\cal X}$.

\subsubsection{Initially Empty HLL Sketch, Scenario S1:} 

Here the attack is somewhat more complex. We begin by explaining the core idea, then a simple version of our attack, then a more complex but performant version, and finally, we give a heuristic performance analysis of the attack.

Recall that in scenario S1, the adversary has access to a shadow HLL sketch which behaves identically to the target HLL sketch, and which can be reset at will. We will seek to minimise the number of resets performed in our attack, since we assume these are costly.  The adversary can also read out the HLL estimate of the shadow device at will. 

\paragraph{Core Idea:}
We assume the adversary again has (or is given) some set ${\cal X}$ of $2^t$ items from which to pick its inputs. The core idea of our attack is to use the first $B \leq B_{\max}$ items $x_1,\ldots, x_B$ from ${\cal X}$ to define a set of target buckets, and then to insert into the shadow HLL sketch further items $x \in {\cal X}$, checking for a cardinality estimate increase on each insertion. We initialise a list ${\cal L}$ containing $x_1,\ldots, x_B$. We add those items that do not increase the cardinality estimate on insertion into the shadow HLL sketch to ${\cal L}$ and reject the rest. We reset the shadow device when the cardinality estimate has increased too much, reinserting the first $B$ items from ${\cal X}$ again to re-establish a ``baseline'' of target buckets for the shadow HLL sketch. A key observation is that if in the attack we keep the number of ``active'' buckets to values $B^\prime$ that are always below $B_{\max}$, then the Hit Counting algorithm is used to compute the cardinality estimate; given its output $m \cdot \ln (m/V)$ (or, more exactly, a rounded version of this value) we can infer the exact value of $V$, and hence $B^\prime$. This enables precise detection of cardinality increases, provided we perform a reset before the Hit Counting algorithm stops being used (this is ensured by resetting when the estimate indicates that $B_{\max}$ buckets have been hit). After all of ${\cal X}$ has been processed as described, we take the constructed list ${\cal L}$ and reprocess it in the same way. We must ensure that the same set of initial items $x_1,\ldots, x_B$ is inserted into the shadow HLL sketch before any other items, so as to fix the same set of target buckets. The order of insertion can be otherwise randomised. % (and our analysis will take advantage of the fact that it can be). 
By repeated reprocessing, we will gradually remove items that do not hit the $B$ target buckets (defined by $x_1,\ldots, x_B$). Eventually, we obtain a final list ${\cal L}^*$ that we insert into the target HLL sketch. 

By construction and from Lemma~\ref{lem:bound}, we are guaranteed that the cardinality estimate for the target HLL sketch will be at most $m \cdot \ln (m/V)$ where $V = m-B$. Assuming $h$ behaves like a random function, it is clear that the expected size of the final list ${\cal L}^*$ is $\frac{B}{m}\cdot |{\cal X}|$. Thus the attack's final performance is the same as that in scenarios S2 and S4 above (but more processing and resets are required).

\paragraph{Simple Attack:}
The simplistic version of our attack uses the above idea, inserting items $x$ into the shadow HLL sketch but resetting it as soon as the cardinality estimate increases from $B$ to $B+1$ (indicating that a non-target bucket has been hit by item $x$). Acting in this way precisely determines whether each inserted item hits the target buckets or not, so has no ``false positives'' which need to be eliminated through reprocessing of the list ${\cal L}$. This means that the attack can be carried out in a streaming fashion in a single pass over ${\cal X}$. Assuming $h$ behaves like a random function, we must perform a reset with probability $\frac{m-B}{m} = 1 - \frac{B}{m}$ per item inserted, and hence in total an expected number of $(1 - \frac{B}{m})\cdot 2^t$ resets.

\paragraph{Complex Attack:}
The more complex version of the attack only resets the shadow HLL sketch when its cardinality estimate reaches a value indicating that $B_{\max}$ buckets have been hit (recall this number can be determined precisely because of the use of the Hit Counting algorithm for cardinality estimation). Hence, less resets should be required. However, when the number of active buckets has reached some value $B^\prime \leq B_{\max}$ this means that an item $x$ may not increase the cardinality estimate, yet still not hit one of the $B$ target buckets (instead it hits one of the other $B^\prime - B$ active buckets). Such an $x$ acts as a ``false positive" and needs to be eliminated through reprocessing of the list ${\cal L}$.

\paragraph{Analysis of the Complex Attack:}
%Suppose that after each reset, the order of the items in ${\cal X}$ (or ${\cal L}$) that have not yet been processed on the current pass is randomised. (Here we exclude the special items  $x_1,\ldots, x_B$ from the reordering.) Then 
On each pass, the probability that an item $x$ that should \emph{not} be inserted (i.e.\ $x$ is a true negative) does \emph{not} increase the cardinality estimate (i.e.\ $x$ is a false positive on this pass) is given by $(B^\prime - B)/m$, where $B^\prime$ is the current number of active buckets at the point when we try to insert $x$. (Here we ignore any inconvenient dependence issues across the passes.) This is bounded by $q := (B_{\max} - B)/m$ in the attack, since $B^\prime$ does not exceed $B_{\max}$. The probability that such an $x$ is correctly rejected is at least $1-q$. Hence the expected number of trial insertions done on $x$ (across different passes of the attack) before $x$ is correctly rejected is $1/(1-q) = m/(m - B_{\max} + B)$. 

Now we expect to reject in total $(1 - \frac{B}{m})\cdot 2^t$ items, and each one needs an expected number of trial insertions that is bounded by $m/(m - B_{\max} + B)$. (In fact, we can model the number of trial insertions needed as an independent geometric distribution with parameter $q$ for each such $x$.) Meanwhile, each of the $\frac{B}{m}\cdot 2^t$ items in the final insertion list ${\cal L}^*$ for the target HLL sketch is trial inserted into the shadow HLL sketch on every pass. 

All the passes in which some item $x$ is a false positive do not lead to cardinality estimate increases; only the final pass in which $x$ is identified as a true negative does so. Hence each rejected item only increases the number of active buckets once during the entire attack. Since we allow the number of active buckets to start at $B$ and reach $B_{\max}$ before resetting, the expected total number of resets (and passes) needed is given by:
\[
N_R:= \frac{1 - \frac{B}{m}}{B_{\max} - B}\cdot 2^t.
\]
Hence the expectation of the total number of trial insertions $N_I$ is bounded as:
\[
N_I \leq \frac{m}{ m-B_{\max} + B} \cdot \left(1 - \frac{B}{m} \right)\cdot 2^t +   N_R \cdot \frac{B}{m}\cdot 2^t.
\]
 
As a concrete illustration, we take $B = m/2$ and $B_{\max} = 0.71\cdot m$, to obtain $N_R = (2.38/m) \cdot 2^t$ and $N_I \leq 0.63 \cdot 2^t + 1.19\cdot 2^{2t}/m$. Here the dominant term in $N_I$ is ${\cal O}(2^{2t})$, i.e.\ the total insertion cost grows as the square of the size of the starting set ${\cal X}$. The result of the attack is that, as in S2 and S4, an expected $2^{t-1}$ items can be inserted into the target HLL sketch while keeping its cardinality estimate below $m \ln 2$.

The insertion cost of the attack can be reduced by more smartly ordering the items $x$ to be trial inserted on each pass. Note that, in a given pass, when $B^\prime$ is still close to $B$, an item $x$ that does not increase the number of active buckets is more likely to have hit one of the target buckets than is the case when $B^\prime$ is large; such an $x$ can be tried earlier in the sequence of trials in the next pass and its exact status determined sooner. Any $x$ that can be determined to be a true positive (because its insertion into the shadow HLL sketch keeps the number of active buckets at exactly $B$) can be placed immediately in ${\cal L}^*$ and not reprocessed any further. We leave the analysis of this optimisation to future work.  

%The adversary blindly picks the first $b$ buckets to be hit by simply inserting items into the shadow HLL until the cardinality estimate indicates that $b$ buckets have been touched. It puts these items in a final target list ${\cal L}$. It continues to insert items, discarding all items whose insertion causes the cardinality estimate for the shadow HLL sketch to increase from $b+k$ to $b+k+1$ for $k \in \mathbb{N}$ (here we allow some slack in the total number of buckets that have been hit in order to reduce the number of resets of the shadow device that are required in the attack). In other words, when $b+k$ buckets have already been hit and the insertion of an item causes the cardinality estimate to increase further, the adversary can discard this item as it definitely knows that a new bucket (that is not one of the $b$ targeted buckets) switched to a non-empty state. On the other hand, it adds to the list ${\cal L}$ any item that does \emph{not} cause the cardinality estimate to increase. Such items may have hit one of the initial $b$ buckets, or one of the later buckets; at this stage the adversary cannot be certain. The adversary can keep inserting items and discarding those which cause the cardinality estimate to increase while in the small cardinality range.

%As soon as the cardinality estimate exceeds $\frac{5}{2}m$, the adversary can assume that the shadow HLL sketch's output corresponds to the raw HLL cardinality estimate with high probability. In this case, no more useful information can be inferred about the bucket mappings in the shadow HLL sketch, so the adversary resets the shadow HLL sketch to the empty state and starts again with the list ${\cal L}$ of non-discarded items obtained so far. Reinserting these items will (eventually) determine whether they hit the original $b$ buckets or later buckets, and hence whether they should be retained in ${\cal L}$ or discarded. Once ${\cal L}$ is reprocessed (and assuming the cardinality estimate does not exceed $\frac{5}{2}m$), the adversary can start to insert fresh items, as previously described. The adversary must be careful not to shuffle the non-discarded items in between iterations, as the $b$ buckets to be filled are blindly determined by the first inserted items, and must stay the same throughout the attack. 

%After sufficiently many iterations of this procedure, the adversary will have created a list of items ${\cal L}$ such that, when inserted into the \emph{target} HLL, create a sketch in which $b$ target buckets are filled and the others left empty. With an appropriate choice of $b$ (as discussed immediately below), the final cardinality estimate for the target HLL sketch will be $E^*= m \cdot \ln (m/V)$ where $V = m-b$ is the number of empty buckets. This can be rewritten as $E^*= m \cdot \ln (1+b/(m-b))$.

%The value of $b$ has to be picked carefully in order to keep the shadow and target HLL cardinality estimates low, as the registers in both sketches may hold values that (potentially) become very high as more items are inserted. The trick is select $b$ so that, even in the worst case scenario where all register values $M[j]$ hold the maximal value of $\ell+1$, the Hit Counting algorithm will still be used to produce the cardinality estimates. Noting that the raw estimate $E = \alpha_mm^2\cdot(\sum_{j=1}^m2^{-M[j]})^{-1}$ increases with increasing $M[j]$, and that we have $b$ buckets each contributing, in the worst case, $2^{-(\ell+1)}$ to the sum (and hence $m-b$ buckets which contribute $2^{0}=1$ to the sum) we see that for the Hit Counting algorithm still to be used, we require:
%\[
%\alpha_mm^2\cdot\left((m-b)+b\cdot2^{-(\ell+1)}\right)^{-1} \le \frac{5}{2}m.
%\]
%Solving for $b$, we obtain:
%\[
%b \le b_{\max} := \left\lfloor m\cdot\left(1-\frac{2\alpha_m}{5}\right)\cdot\left(1-2^{-(\ell+1)}\right)^{-1} \right\rfloor. \label{eqn:1} 
%\]
%
%For example with $n = 8$ (so $m=256$) and $\ell = 24$, we find that $b_{\max}=184$.
%
%A surprising implication of this analysis is that, for the HLL algorithm as described in Figure~\ref{fig:hll}, provided $b \leq b_{\max}$, no matter which fixed set of $b$ buckets are hit, and no matter how many leading zero bits the items have, the final cardinality estimate will not exceed $m \cdot \ln (1+b/(m-b))$. We will exploit this in the sequel in scenarios S2 and S4. For now, we note that setting, say, $b=128$ with $n = 8$ and $\ell = 24$ would result in a final cardinality estimate of $256 \ln 2$ in the above attack in Scenario S1, irrespective of how long the list ${\cal L}$ of inserted items is. Moreover, setting $b=128$ would mean that half of all items available to the adversary could be inserted into the target HLL. 
%
%%The initial set of items to be filtered can be built by varying the flexible fields of the items $x$ the adversary is trying to insert.
%%so that the adversary can ``hide'' $2^{t-n}$ elements while keeping the HLL cardinality estimate to 1.
%


%\subsubsection{Scenario S2} The adversary knows $h$ so it can adjust fields in the item $x$ it is trying to insert and keep only those for which the $\ell$ rightmost bits of $h(x)$ start with a one bit. In this case, the number of leading zero bits computed by the estimate of the destination bucket is always zero, so the estimate of this bucket is unlikely to be updated and the final averaged approximate cardinality is likely to be left unchanged.
%For example, in the scenario of~\cite{portscanhll}, the adversary could change the source port of its IP packet until it is satisfied with its hash value. Given sufficient flexibility in the input $x$, the adversary can insert many items into the HLL sketch without increasing the cardinality markedly, by repeatedly sampling from the input domain.

%When attacking an empty sketch, the adversary can keep this strategy and already mount a powerful attack, but can do better by following the approach presented under S1. Filtering items is easier in this scenario as the adversary can infer the bucket mapping of an item $x$ directly from the first $n$ bits of $h(x)$. A trivial approach for the adversary is to retain items mapped to buckets 1 to $b$. We argue why targeting $b$ buckets is a better approach in the impact analysis presented in Section~\ref{sec:impact}. 

%\subsubsection{Scenario S4} The adversary can adopt the same strategy as for S2, but make the attack more efficient and reduce the sampling requirements with the additional information available in this scenario (namely, the count values per bucket in the HLL sketch). Therefore, it can allow some leading zero bits in the $\ell$ rightmost bits of $h(x)$ so long as this does not increase the current bucket estimate. In other words, if $h(x)$ is mapped to the $i$-th bucket with estimated cardinality $2^{M[i]}=2^{c_i + 1}$ (because a value with $c_i$ leading zeroes was previously observed for this bucket), then, instead of the strict restriction of having a one bit in the first position in the $\ell$ rightmost bits of $h(x)$, the adversary is satisfied when there is a one bit in any of the first $c_i+1$ positions in this substring. In case the adversary hits an empty bucket (for which the estimate is 0), it must skip that bucket completely. Under these conditions, the estimates in the buckets are never updated, and there is no effect on the cardinality estimate made by HLL.

%A special case arises when the HLL is completely empty at the beginning of the attack. Here the adversary is forced to increase the estimate in at least one bucket. Once again, the adversary can choose to fill $b$ buckets following the strategy described in S2, leaving all the other buckets untouched.
%
%\subsection{Impact}\label{sec:impact}
%In this section we evaluate the impact of the above attacks on the HLL cardinality estimate. We assume that $h$ is a ``good" hash function, so that each of the bits of $h(x)$ can be regarded as being independent and uniformly random. We assume that the adversary has $t$ bits of flexibility in its choice of inputs.

\subsection{Attacking an Initially Non-Empty HLL Sketch}\label{sec:nonempty}

Now we turn to the case where the HLL sketch under attack is not initially empty, but instead contains some honestly inserted items. As usual, the adversary's goal is to insert as many items as possible whilst increasing the cardinality estimate as little as possible.

We focus here on scenarios S2 and S4, and make brief remarks on S1.

We assume that the target HLL sketch in S2 and S4 has already had sufficiently many items inserted that every one of the $m=2^n$ buckets has counter value $M[j]$ with $M[j] \ge 1$. This is highly likely to be the case (with probability at least $1-1/m^2$) as long as at least $m\ln m$ items have already been inserted into the target HLL sketch. This follows from the standard analysis of the coupon collector problem under our usual assumption that $h$ behaves as a random function.
In S4, the adversary can use its snapshot to detect if this is the case or not, and behave differently if not (details for this case follow below).

With $m \ln m$ honest insertions, it is also highly likely that the small range correction (Hit Counting algorithm) will not be used, and the target HLL's cardinality estimate will be computed using the raw estimate:
\[
E = \alpha_mm^2\cdot(\sum_{j=1}^m2^{-M[j]})^{-1}.
\]

We assume the adversary has (or is given) some set of items ${\cal X}$ that it wishes to insert; as usual, we write $|{\cal X}| = 2^t$.

\subsubsection{Initially Non-empty HLL Sketch, Scenario S2:}\label{sec:nonemptyHLLS2}
The adversary knows $h$ so it selects items $x$ from ${\cal X}$ for which the $\ell$ rightmost bits of $h(x)$ start with a ``1'' bit. In this case, since the relevant bucket $j$ already has $M[j] \ge 1$, the bucket's counter is not updated, and the cardinality estimate of the HLL sketch remains unchanged. 

The adversary then expects to be able to insert half of the items from ${\cal X}$ into the HLL sketch, without increasing the HLL cardinality estimate at all.

If some buckets are actually empty (but the adversary does not know which ones in the blind setting of S2), then we may expect a moderate increase in the HLL cardinality estimate in the above attack. If the adversary suspects this is the case, then it can first insert $m$ carefully selected items into the target HLL sketch to ensure that $M[j] \ge 1$ for every bucket. This should increase its cardinality estimate by about $m$ (since HLL is a good cardinality estimator!). Then it can mount the attack, and be sure that it does not increase the cardinality estimate any further.

\subsubsection{Initially Non-empty HLL Sketch, Scenario S4:}\label{sec:nonemptyHLLS4}
The adversary can adopt the same strategy as for S2, but make relax the sampling requirements with the additional information available in this scenario (namely, the count values $M[j]$ per bucket in the HLL sketch). Specifically, it can allow some leading zero bits in the $\ell$ rightmost bits of $h(x)$ so long as this does not increase the current bucket estimate. In other words, if $h(x)$ is mapped to the $j$-th bucket containing $M[j] \ge 1$, then, instead of the strict requirement of having a ``1'' bit in the first position in the $\ell$ rightmost bits of $h(x)$, the adversary is satisfied when there is a ``1'' bit in any of the first $M[j]$ positions in this substring. Under these conditions, the estimates in the buckets are never updated, and there is no effect on the cardinality estimate made by HLL.

We now compute the expected number of elements $x$ from ${\cal X}$ such that $h(x)$ meets the attack's requirements. We assume that $M[j] \ge 1$ for all buckets $j$. Assume $h(x)$ is mapped to bucket $j$. Inserting $x$ does not increase the cardinality estimate if the rightmost $\ell$ bits of $h(x)$ have $M[j]-1$ or fewer leading zeroes; this probability is equal to $1-\frac{1}{2^{M[j]}}$ assuming $h$ behaves like a random function. Averaging over all $m$ buckets (and using the fact that the bucket choice is uniformly random since the bits of $h$ are independent and uniformly random), the probability that input $x$ meets the attack's requirements is given by:
\begin{eqnarray*}
\sum_{j=1}^{m}\frac{1}{m} \cdot (1-\frac{1}{2^{M[j]}}) & = & 1- \frac{1}{m}\cdot \sum_{j=1}^{m} {(2^{M[j]})}^{-1} \\
& = &1 - H_M^{-1} 
\end{eqnarray*}
where $H_M$ is the harmonic mean of the counts $2^{M[1]}, 2^{M[2]}, \ldots, 2^{M[m]}$.
So, given ${\cal X}$ containing $2^t$ items, the adversary can expect to insert $(1 - H_M^{-1}) \cdot 2^t$ items from ${\cal X}$ without increasing the HLL cardinality estimate at all.

If some buckets are actually empty (and in S4 the adversary knows which ones using its snapshot), then the above analysis can be reused in order to avoid touching those buckets altogether when inserting elements from ${\cal X}$. The above analysis does not change at all because the term $\frac{1}{m} \cdot (1-\frac{1}{2^{M[j]}})$ already signs a probability of 0 to inserting $x$ in bucket $j$ if $M[j] = 0$. Hence we still expect to insert $(1 - H_M^{-1}) \cdot 2^t$ items from ${\cal X}$ without increasing the HLL cardinality estimate at all. Of course, the adversary can relax its attack and allow some of the empty buckets to be hit as well. This would increase the insertion rate at the cost of an increase in the final cardinality estimate. 

In this setting, where the target HLL sketch is initially non-empty, we see that an adversary who has sufficient flexibility in its inputs (as represented by the size $2^t$ of the set ${\cal X}$) can expect to insert exponentially many (in $t$) values into the HLL sketch without increasing the cardinality estimate (or whilst increasing the cardinality estimate slightly in the case where the HLL sketch has some empty buckets in S2). The result is largely independent of the HLL parameters, but does depend on the extent to which the HLL sketch is already filled by other users, via a factor $1-H_M^{-1}$: the more full the HLL sketch is, the easier it is to insert new items without them increasing the cardinality estimate.

\subsubsection{Initially Non-empty HLL Sketch, Scenario S1:} 

Finally, we briefly consider scenario S1 with an initially non-empty target HLL sketch. Since the target sketch has an unknown status for the adversary, our previous strategy for S1 of keeping many buckets empty does not work. One can attempt to use the shadow device to identify items that would result in small entries $M[j]$ (for unknown indices $j$) since such items are unlikely to increase the cardinality estimate of the target device by much.  This requires first filling the shadow device with sufficiently many values to escape from the ``small range correction'' phase. Then one could enter random values $x$ into the shadow device and check for cardinality increases. However this runs into the rounding issue described in Section~\ref{sec:scenarios-discussion}. Still, one could retain those items which do lead to a detectable change in cardinality and reject the others (and also estimate the value of $M[j]$ for the unknown bucket $j$ from the magnitude of the change, retaining only those for which $M[j]$ is small). But here the lack of a cardinality change could result either from rounding issues or because insertion of $x$ into the shadow device really did lead to no increase in cardinality. This indicates that  ``good'' items would be rejected unnecessarily. This could be addressed by using multiple resets, filling with random items, and then multiple insertions per item to try to estimate the true value of $M[j]$ for each item. The details of such an attack require further analysis which we do not pursue here, since we consider S1 to be the least realistic attack scenario. 

%We first make an empty shadow HLL sketch via a reset. Consider an adversary that simply inserts $m \ln m$ random items $x$ from ${\cal X}$ into the shadow and target HLL sketches. This increases the target HLL sketch's cardinality estimate by about $m \ln m$ since the HLL algorithm is a good estimator. It also sets the shadow HLL sketch's estimate to about $m \ln m$, by the same argument.  At the same time, with probability $1 - 1/m^2$, it ensures that every bucket in the shadow HLL sketch becomes active (i.e.\ has counter values $M[j]$ with $M[j] \ge 1$ for all $j$). Now the adversary can carry out a version of our previous attack in scenario S1 (where the target HLL sketch is initially empty). By repeatedly processing lists of inputs, it can find a large subset ${\cal L}^*$ of ${\cal X}$ which does not cause the cardinality estimate of the shadow HLL sketch to increase beyond $m \ln m$. This assumes that single updates to the vector of counter values $(M[j])_{j=1}^m$ in the shadow HLL sketch can be detected through changes to the HLL cardinality estimate. This seems plausible but requires further experimental investigation. Inserting all the items from ${\cal L}^*$ into the target HLL sketch then leads to an increase in its cardinality estimate of at most $m \ln m$. It is not hard to see that, since every bucket in the shadow HLL sketch is active, we can expect ${\cal L}^*$ to contain at least half of the items of ${\cal X}$. 

%An alternative approach is to simply run the scenario S1 attack as if the target HLL sketch were empty (i.e.\ as described in Section~\ref{sec:empty} above), take the resulting list ${\cal L}^*$, and insert it into the target HLL sketch, hoping that the cardinality does not increase too much. %We report on this approach experimentally in Section~\ref{sec:exp}. It turns out to work well in practice. 
%% Nah, that doesn't work -- the S1 attack crucially depends on staying in the Hit Count range, and you go out of that as soon as you try to ``merge'' with a non-empty target.

%We omit further analysis of this scenario.

\section{Analysis of the Redis HLL Implementation}\label{sec:redis}

We present a brief analysis of the HLL implementation used in Redis, since Redis is a very popular distributed, in-memory key-value store implementation and the version of HLL used there differs significantly from that described in Figure~\ref{fig:hll}.

HLL was first added to Redis in release 2.8.9 in April 2014. Redis 4.0 released in April 2017 introduced HLL Beta, which improves on the raw HLL cardinality estimator by using polynomial interpolation to reduce systematic errors. Redis 5.0 released in October 2018 replaces this with a new cardinality estimator based closely on the work of Ertl~\cite{hllnew}. This method is still used in the current release (Redis 6.0.8) and so is the object of our study here.\footnote{For details of Redis releases, see \url{https://github.com/redis/redis/blob/6.0/00-RELEASENOTES} and other files obtained by replacing ``6.0'' in this URL with the desired version number.}

Ertl~\cite{hllnew} proposed using the following cardinality estimator in place of the raw estimator in Figure~\ref{fig:hll}:
\begin{equation}\label{eqn:ertl}
\hat{\lambda}  = \frac{\alpha_\infty m^2}{m\sigma(C_0/m) + \sum_{k=1}^{\ell}C_k 2^{-k} + m \tau(1-C_{\ell+1}/m)2^{-\ell}} \, .
\end{equation}
Here, in the denominator, $C_k$ denotes the number of buckets containing value $k$ in the HLL filter, so $C_0$ is in particular the number of empty buckets; $\sigma(\cdot)$ and $\tau(\cdot)$ are specific functions defined as:
\[
\sigma(x) :=  x + \sum_{k=1}^{\infty} x^{2^k} 2^{k-1}
\]
and
\[
\tau(x) :=  \frac{1}{2} \left( -x + \sum_{k=1}^{\infty} x^{2^{-k}} 2^{-k} \right).
\]
Finally,  $\alpha_\infty  = 0.72134752\ldots$ is a constant. (Note that we have translated from the original notation in~\cite{hllnew} to ours.)

Redis 5.0 onwards uses exactly the formula in equation~(\ref{eqn:ertl}) to form its cardinality estimate. It uses $n=14$, $m=16,384$ and $\ell = 50$ (so $n+\ell = 64$ and a 64-bit hash function is used). 

Note that when evaluating (\ref{eqn:ertl}), functions $\sigma$ and $\tau$ are only called on inputs in the range $[0,1]$. Moreover, in this range, we have $\sigma(x) \ge x$. Also it is highly likely that $C_{\ell+1} = 0$ (since it is very unlikely that a 50-bit all-0 string will result from the application of the Redis HLL hash function, and in any case in scenarios S2 and S4, we can always arrange for this to be avoided). When $C_{\ell+1} = 0$, $\tau$ is evaluated at 1, where its value is zero. Combining this information, we see that, provided $C_{\ell+1} = 0$, we have:
\[
m\sigma(C_0/m) + \sum_{k=1}^{\ell}C_k 2^{-k} + m \tau(1-C_{\ell+1}/m)2^{-\ell} \ge m\sigma(C_0/m) \ge C_0
\]
and so
\[
\hat{\lambda}  \leq \frac{\alpha_\infty m^2}{C_0}
\]
where, recall, $C_0$ is the number of empty buckets.

Attacks are now straightforward. Suppose we can arrange that $C_0$, the number of empty buckets, stays above ${\epsilon}m$, a linear function of the total number of buckets, and that $C_{\ell+1} = 0$. Then we obtain $\hat{\lambda}  \leq \alpha_\infty \epsilon^{-1} m$, a linear function of $m$. For example, we can set $\epsilon = 1/2$ to obtain $\hat{\lambda}  \leq 2\alpha_\infty m$, where recall that $\alpha_\infty$ is about $0.72$, hence $\hat{\lambda}  \leq 1.44m$.

The requirement on $C_0 \geq {\epsilon}m$ is easily met in scenarios S2 and S4 with an initially empty HLL filter, through the trick of sampling inputs $x \in {\cal X}$ so that only a fraction $1-\epsilon$ of the buckets are ever hit. We can expect to insert a fraction $1-\epsilon-\epsilon2^{-\ell}$ of the elements of  ${\cal X}$ whilst keeping the cardinality estimate $\hat{\lambda}$ below  $\alpha_\infty \epsilon^{-1} m$. (The term $\epsilon2^{-\ell}$ arises from the requirement to keep $C_{\ell+1} =0$ in the attack; recall that $\ell=50$ in Redis, so the effect of this term is very small.) 

In scenarios S2 and S4 with an initially non-empty HLL filter, we assume as for our earlier analysis of the standard HLL algorithm that all buckets have already been hit (this will be the case with high probability as soon as $m \log m$ items have been honestly inserted). Hence $C_0 = 0$ and $\sigma$ is evaluated at 0 in (\ref{eqn:ertl}), where its value is 0. This means that the estimate $\hat{\lambda}$ is computed using a denominator of the form:
\[
\sum_{k=1}^{\ell}C_k 2^{-k} + m \tau(1-C_{\ell+1}/m)2^{-\ell}.
\]
Now we can proceed exactly as in Sections~\ref{sec:nonemptyHLLS2} and~~\ref{sec:nonemptyHLLS4}, inserting values $x$ whose hashes have rightmost $\ell$ bits beginning with a ``1'' bit (and with a more relaxed condition in S4). These insertions do not alter the values of the $C_k$, and hence $\hat{\lambda}$ is unaffected. In S2, we expect to insert half the items in ${\cal X}$ without increasing the cardinality estimate $\hat{\lambda}$ at all. In S4, we can insert even more items. The analysis can be extended in S4 to the case where some of the buckets are still empty: in S4 we can avoid hitting these empty buckets and again the cardinality estimate does not increase at all.

This completes our analysis of Redis. The key takeaway is that, in scenarios S2 and S4, an adversary can expect to insert a constant fraction of ${\cal X}$ into a non-empty Redis HLL filter whilst not increasing its cardinality estimate at all (and whilst keeping the estimated cardinality linear in $m$ in the case where the filter is initially empty).

%\todo{What about S1 here? Seems harder because we may not get accurate HLL estimates due to rounding again. We need to show that decreasing $C_0$ by 1 is noticeable in Ertl's formula even with rounding.}

\section{Experimental Results}\label{sec:exp}
Many different implementations of HLL are available online. Among the repositories collecting more than 350 stars on GitHub are~\cite{clahll,datasketch}. HLL is also featured in many frameworks, including Redis~\cite{redis}, Google's BigQuery~\cite{bigquery}, Facebook's Airlift~\cite{airlift} and several products of Apache such as Spark~\cite{spahll} and Druid~\cite{druhll}. It is interesting to point out that all these implementations use a constant number of buckets and a fixed hash function. Thus they are potentially vulnerable to our attacks.

As a proof-of-concept, we implemented our attacks against the HLL implementation from~\cite{clahll}, since it is faithful to the original description of HLL given in~\cite{hll}.\footnote{We will make our attack code available on acceptance.}
%at \href{https://github.com/PizzaWhisperer/HLLVuln}{\url{https://github.com/PizzaWhisperer/HLLVuln}}.} 
%\todo{We need to make sure the repo is publicly accessible but anonymised for submission, if we are going to include it.}
For simplicity, our items are 4-character long ASCII strings, giving us a set ${\cal X}$ of $7,311,616= 2^{22.8}$ distinct items that can be inserted (hence we have $t=22.8$ in our attacks). We choose $h$ to be the 32-bit Murmur3 hash function from~\cite{murmur3code} (note that the library allows the hash function to be specified; we pick Murmur3 for our attack and consider it reasonable that the adversary should know the hash function in use).
We create an HLL sketch using $n=8$ and hence $m=256$ buckets and, for attacks in the setting of an initially non-empty target HLL sketch, initialise it with 1,000 random items representing honest users' data.

We then challenge the adversary to pick the largest possible number of items to add to the sketch from our set of  $2^{22.8}$ possible inputs while trying to keep the cardinality estimate as low as possible, in each of the scenarios S1, S2 or S4. 

As a second experiment, and in order to get results that can be compared to the work of~\cite{hllvuln}, we mount our attack in their more restrictive setting: instead of picking from the universe of possible strings, the adversary has to choose what to insert from a set of 250,000 random items that are given to it. Finally, for completeness, we also run the attacks against HLL sketches filled with adversarial inputs only, thus that are initially empty. We vary the value of $B$ in the attack for scenario S1 to demonstrate some of the available trade-offs. For simplicity and to keep the attack as striking as possible, we set $B$ to 1 in the attacks on an initially empty HLL sketch in scenarios S2 and S4. All attacks were run 30 times; we report average outcomes. Our results are reported in Tables~\ref{table:tab1} and~\ref{table:tab2}, which detail the estimated cardinality at the beginning of the attack, the number of items added by the adversary, and the cardinality estimate after adding the adversary's items.

These empirical results confirm our analysis on several points.

First, when attacking a sketch already containing 1000 random items, we can see that the adversary is on average able to insert about half of the $2^{22.8}$ items in our starting set in scenario S2, and about 83\% in scenario S4. The cardinality estimate is, on average, increased slightly in S2 (from 1,000 to 1,011) and remains at the same starting value (1,005 on average) in S4. In both cases, we inserted more than 3 million items whilst increasing the cardinality estimate by about $0.1\%$!  These results align well with our theoretical analysis. 

%In scenario S1 we run the attack designed for an initially empty target HLL sketch. It performs well in practice, for example, increasing the HLL cardinality estimate from 100 to 126????
%( $205$'$000 \approx 250'000 \times 0.82 \approx 250$'$000\times(1-\frac{1}{2\times H_c})$, with $H_c\approx\frac{1'000}{256}=3.9$)
%Here $0.83 \approx (1-\frac{1}{2\cdot H_c})$ with $H_c\approx\frac{1000}{256}=3.9$, so the results are also consistent with our theoretical analysis.
%% That analysis was now updated -- does it still match?

Secondly, when attacking an empty HLL sketch, we can see that in all scenarios, the final cardinality estimate is always exactly a rounded version of the Hit Counting estimator $m \cdot \ln (m/(m-B))$ after inserting a fraction $\frac{B}{m}$ of the items, as predicted by our analysis. For example, in scenario S1 with $B=100$, the final cardinality estimate is 126 (in agreement with the formula) while with $B=1$ in scenarios S2, S4, the final cardinality estimate is just 1 (again, as predicted by the formula).

We can also compare our results from Table~\ref{table:tab2} with results from the attack presented in~\cite[Section 5.1]{hllvuln}. In that paper, the authors target the Redis implementation of HLL and perform the attack against an empty sketch filled with adversarial inputs only. Given a set of 250,000 items, the adversary could choose ``74,390 distinct items and obtain an HLL estimate of only 15,780''~\cite{hllvuln}, achieving a five-fold reduction from the true cardinality after 4 iterations (resets). As explained in Section~\ref{sec:redis}, the Redis implementation analysed in~\cite{hllvuln} does not conform precisely to the original HLL algorithm as presented in Figure~\ref{fig:hll}. Also as discussed in Section~\ref{sec:redis}, the Redis implementation has changed its HLL algorithm several times in recent releases, but the authors of~\cite{hllvuln} do not specify which version was used in their experiments. This makes it difficult to make a precise comparison with their results. Nevertheless, the S1 adversary following our strategy against the classical HLL algorithm is able to add 977 and 97,433 distinct items while holding the cardinality estimate at 1 and 126, respectively. Compared to~\cite{hllvuln}, we can reduce the cardinality estimate by a factor of almost 1,000 instead of 5. This comes at the cost of performing more resets compared to~\cite{hllvuln}. 

\begin{table}[tb!]
\centering
\caption{Attack results, averaged over 30 iterations.}
\medskip
     \makebox[\linewidth]{
\begin{tabular}{| m{11em} | m{4em} | m{4.2em} | m{4.1em} | m{4.1em} | m{4.1em} | m{4em} |}
    \hline
    \textbf{Scenario} &  \multicolumn{2}{c|}{S1} & \multicolumn{2}{c|}{S2} & \multicolumn{2}{c|}{S4} \\ \hline
    \textbf{\# Targeted buckets $B$} & 1 & 100 & n/a & 1 & n/a & 1 \\ \hline
    \textbf{HLL initially empty?} & yes & yes & no & yes & no & yes \\ \hline
    \textbf{Original Card. Est.} & 0 & 0 & 1,000 & 0 & 1,005 & 0 \\ \hline
    \textbf{\# Items added} & 28,566 & 2,856,100 & 3,655,744 & 28,566 & 6,090,003 & 28,566 \\ \hline
    \textbf{Final Card. Est.} & 1 & 126 & 1,011 & 1 & 1,005 & 1 \\ \hline
\end{tabular}}
\label{table:tab1}
\end{table}

\begin{table}[tb!]
\centering
\caption{Attack results, averaged over 30 iterations, in the setting of~\cite{hllvuln}.}
\medskip
     \makebox[\linewidth]{
\begin{tabular}{| m{11em} | m{4em} | m{4.2em} | m{4.1em} | m{4.1em} | m{4.1em} | m{4em} |}
    \hline
    \textbf{Scenario} & \multicolumn{2}{c|}{S1} & \multicolumn{2}{c|}{S2} & \multicolumn{2}{c|}{S4} \\ \hline
    \textbf{\# Targeted buckets $B$ } & 1 & 100 & n/a & 1 & n/a & 1 \\ \hline
    \textbf{HLL initially empty?} & yes & yes & no & yes & no & yes \\ \hline
    \textbf{Original Card. Est.} & 0 & 0 & 1,004 & 0 & 998 & 0 \\ \hline
    \textbf{\# Items added} & 977 & 97,433 & 124,940 & 979 & 204,669 & 988 \\ \hline
    \textbf{Final Card. Est.} & 1 & 126 & 1,058 & 1 & 998 & 1 \\ \hline
\end{tabular}}
\label{table:tab2}
\end{table}

\section{Formal Security Analysis}\label{sec:formal}

In this section we present a formal security analysis of HLL, focussing on whether we can augment the original HLL design with cryptographic mechanisms and thereby provably prevent attacks.

\subsection{Scenario S1}
We begin by disposing of scenario S1. In that scenario, the attacker has access to insertion and reset capabilities, and to cardinality estimates, for a shadow HLL that is identical in every respect to the target HLL. The attacks we presented previously are oblivious to any internals of the implementation (other than that they follow the HLL description in Figure~\ref{fig:hll}). This means that, say, replacing the hash function $h$ with a keyed function or any other cryptographic primitive would be futile in preventing attacks in scenario S1. Interestingly, our attacks in scenario S1 are more complex and require more effort than in scenarios S2 and S4. The fact that we cannot prove security in S1 with improved cryptography is not in contradiction to this, but rather an illustration of the power conferred by giving the adversary access to an exact and resettable replica of the target device in the keyed setting.

In summary, we cannot hope to gain any security in scenario S1 by adding more cryptography.

\subsection{Formal Security Analysis for S2, S3, S4}

For scenarios S2, S3 and S4, the picture is much more positive. In the remainder of this section, we introduce a formal attack model covering all three scenarios. This is done by allowing an adversary ${\cal A}$ to interact with an HLL via various oracles that capture its capabilities in each of the three scenarios. In the security model, ${\cal A}$ interacts in one of two worlds. In the real world, ${\cal A}$ interacts with a real HLL, operating as in Figure~\ref{fig:hll}. In the ideal world, ${\cal A}$ interacts with an ideal HLL in which the operation $y: = h(x)$ is replaced by random sampling; that is, by setting $y \leftarrow_{\$} \{0,1\}^{n+\ell}$. 

We then show that, if $h$ is replaced by a keyed Pseudo Random Function (PRF) $F$ whose key is not available to the adversary, then any adversary ${\cal A}$ that can distinguish between the real and random worlds with oracles as in S4 can be used to build an adversary ${\cal B}$ which breaks the PRF security of $F$. Under the assumption that $F$ is a good PRF, no such efficient adversary ${\cal B}$ can exist, and we may conclude that we can safely switch from the real world to the ideal world without incurring any significant change to the behaviour of ${\cal A}$. 

However, it is clear that any adversary in the ideal world is reduced to operating in the setting used in the average case analysis of HLL (wherein every insertion query results in a random value $y$ being used to update the HLL). Thus, ${\cal A}$ can have no advantage when it comes to influencing the HLL cardinality estimate. It follows that HLL, when instantiated with a keyed PRF, is secure against attacks in S4 (and therefore in the weaker attacks models S2 and S3 as well).

We will conclude by describing how the necessary keyed PRF can be instantiated.

We note that our formal security analysis could be conducted using the framework recently laid out in~\cite{CCS:ClaPatShrPS19}. However, we prefer to give a compact and self-contained treatment here.

\subsection{Formal Security Model for HLL}

We consider adversary ${\cal A}$ to be an algorithm running against a challenger ${\cal C}$ in a security game. The game begins with ${\cal C}$ selecting a bit $b$ uniformly at random. Adversary ${\cal A}$ interacts with up to four oracles which are handled by ${\cal C}$; different combinations of oracles define scenarios S2, S3, S4. After making some queries, ${\cal A}$ returns a bit $b'$ estimating whether it is interacting with a real HLL instantiation ($b'=0$) or an ideal one ($b'=1$). For $i=1, 2, 3$, we define ${\cal A}$'s S$i$-Real-or-Ideal advantage against HLL to be:
\[
\mbox{Adv}^{\mbox{\tiny S$i$-RoI}}_{\mbox{\tiny HLL}}({\cal A}) := \lvert 2\Pr[b' = b] - 1 \rvert \, .
\]
%Using a standard transformation, this probability can be rewritten as $\lvert \Pr[b' = 1| b = 1] - \Pr[b'=0|b=1] \rvert$. 
The four oracles that  ${\cal A}$ can interact with are as follows:
\begin{itemize}
\item \texttt{init}: this oracle performs initialisation of the HLL (and selection of the PRF key when $h$ is replaced by a PRF $F$); this oracle is available in all of S2, S3, S4, but can only be called once as ${\cal A}$'s first oracle query.
\item \texttt{insert($\cdot$)}: on input $x \in {\cal X}$, the oracle inserts value $x$ into the HLL. This oracle is available in all of S2, S3, S4. Without loss of generality (since HLL and variants of it we consider here have a deterministic insertion procedure), we assume all queries $x$ made by ${\cal A}$ to \texttt{insert($\cdot$)} are distinct. We let $q$ denote the number of insertion queries made by ${\cal A}$ during the course of its attack.
\item \texttt{read}: this oracle returns the HLL cardinality estimate. We allow multiple queries to this oracle and make it available to ${\cal A}$ in S3 and S4, but not S2.
\item \texttt{corrupt}: this oracle returns the vector $(M[j])_{j=1}^{m}$ of HLL bucket contents (but not any secret key $K$). This oracle is available only in S4. We allow the adversary to query \texttt{corrupt} as often as it wishes during its attack. Our description of S4 in Section~\ref{sec:attacks} limited the adversary to at most one such query; we allow multiple queries here because doing so does not increase the difficulty of achieving security.  
\end{itemize}

We note that in S4 (and only in S4) the adversary has access to all four oracles. This makes it the strongest model, and hereafter we only target security in this model. We count queries $q$ to \texttt{insert($\cdot$)} because this will enable us to quantify the quality of our security reduction to PRF security below.

\subsection{Security Proof for HLL Using a PRF}

We now replace the hash function $h$ acting on ${\cal X}$ from the HLL algorithm with a keyed function $F: \{0,1\}^k \times {\cal X} \rightarrow  \{0,1\}^{n+\ell}$. Here the first input to $F$ is a $k$-bit string, the key, denoted $K$. We assume that \texttt{init} in the HLL security model now initialises key $K$ by selecting it uniformly at random from $\{0,1\}^k$.

We define the PRF advantage of an adversary ${\cal B}$ against $F$ outputting a bit $d'$ as being its advantage in distinguishing whether it is interacting with $F$ for a uniformly random key $K$ ($d=0$) or interacting with a truly random function ($d=1$). Thus: 
\[
\mbox{Adv}^{\mbox{\tiny PRF}}_{F}({\cal B}) := \lvert 2\Pr[d' = d] - 1 \rvert\,.
\]
Here, we assume without loss of generality that all of ${\cal B}$'s queries are distinct. See~\cite[Section 4.4.1]{BonehShoup} for details.

Now we are ready to state:

\begin{theorem}\label{thm:security-of-HLL-with-PRF}
Suppose ${\cal A}$ is an S4 adversary against HLL instantiated with a keyed function $F: \{0,1\}^k \times {\cal X} \rightarrow  \{0,1\}^{n+\ell}$. Then we can construct from ${\cal A}$ a PRF adversary ${\cal B}$ against $F$ such that:
\[
\mbox{Adv}^{\mbox{\tiny S4-RoI}}_{\mbox{\tiny HLL}}({\cal A}) = \mbox{Adv}^{\mbox{\tiny PRF}}_{F}({\cal B}) \,.
\]
Moreover, if ${\cal A}$ makes $q$ queries to its \texttt{insert($\cdot$)} oracle, then ${\cal B}$ makes $q$ queries to its real or random oracle. Finally, the running time of ${\cal B}$ is (roughly) the same as that of ${\cal A}$.
\end{theorem}

The interpretation of the above theorem is that if $F$ is a good PRF, then $\mbox{Adv}^{\mbox{\tiny PRF}}_{F}({\cal B})$ should be small for any adversary ${\cal B}$ running in a reasonable amount of time. Hence we can conclude that the advantage of any S4 adversary  ${\cal A}$ against HLL using PRF $F$ is also small. 

\begin{proof}
PRF adversary ${\cal B}$ against $F$ is built as follows. Let $d$ denote the hidden bit determining whether ${\cal B}$ receives real responses of the form $F(K,x)$ where $K$ is a uniformly random key ($d=0$) or responses from a truly random function ($d=1$); ${\cal B}$ runs S4 adversary ${\cal A}$ and responds to each of its queries as follows:
\begin{itemize}
\item \texttt{init}: here ${\cal B}$ initialises an HLL filter in accordance with Figure~\ref{fig:hll}. Thus it initialises an array $M[j]$ in which each entry is set to 0. 
\item \texttt{insert($\cdot$)}: on input $x \in {\cal X}$,  ${\cal B}$ makes a query on $x$ to its real-or-random oracle and receives value $y$; ${\cal B}$ uses $y$ to update the HLL bucket $j$ generated via $j=1 + <y_1y_2...y_{n}>_2$ as in the regular HLL algorithm (shown in Figure~\ref{fig:hll}).
\item \texttt{read}: ${\cal B}$ uses its knowledge of the buckets' contents to compute and return the HLL cardinality estimate in accordance with Figure~\ref{fig:hll}. 
\item \texttt{corrupt}: ${\cal B}$ returns the vector $(M[j])_{j=1}^{m}$ of HLL bucket contents. 
\end{itemize}
When ${\cal A}$ outputs a bit $b'$, ${\cal B}$ outputs its own bit $d'=b'$. Note that when $d=0$, ${\cal B}$ plays against the real $F$ with a random key $K$ and provides a simulation for ${\cal A}$ that perfectly instantiates the real world. On the other hand, when $d=1$, ${\cal B}$ plays against a truly random function and provides a simulation for ${\cal A}$ that perfectly instantiates the ideal world. The equality of advantages in the statement of the theorem now follows immediately. The remainder of the proof follows on counting queries and noting that the HLL algorithm for computing cardinality is efficient, so ${\cal B}$'s running time due to handling \texttt{read} queries is not increased significantly compared to that of ${\cal A}$.
\end{proof}
 
Note that the above proof still goes through if the classical HLL algorithm is replaced with the improved algorithm of~\cite{hllnew} that is used in Redis. The only changes to the proof are to use the Redis approach for computing responses to \texttt{read} queries in place of the classical HLL approach.

Finally, we conclude our analysis by reiterating the key point: we have shown that no efficient S4 adversary ${\cal A}$ can tell that it is playing against HLL implemented using truly random values $y$ instead of with values obtained from a PRF, except with a small advantage. This means that ${\cal A}$ is effectively reduced to attacking HLL in the exact same setting used in the average case analysis of HLL from~\cite{hll}, wherein it is proved that the HLL cardinality estimate endures relative errors of the form $1.04/\sqrt{m}$. It does not matter that ${\cal A}$ can adaptively choose its inputs $x$ in \texttt{insert($\cdot$)} queries, nor that it can access \texttt{read} and \texttt{corrupt} queries. For each \texttt{insert($\cdot$)} query, a fresh value $x$ leads to a new random $y$ being generated and used to update a bucket, as in the average case analysis of HLL. The \texttt{read} and \texttt{corrupt} queries do leak information about $F$'s outputs to the adversary, but such outputs are indistinguishable from random under the assumption that $F$ is a PRF. 

\subsection{Instantiating the PRF}

The above construction requires a PRF $F: \{0,1\}^k \times {\cal X} \rightarrow  \{0,1\}^{n+\ell}$. Here, the set ${\cal X}$ represents the space of possible inputs to the HLL which can be modelled by a set of bit-strings of some maximum length. This mean that $F$ needs to be Variable Input Length PRF (VIL-PRF). 

Making $F$ competitive in speed with existing implementations of HLL, which use non-cryptographic hash functions such as Murmur3, is challenging, and in practice we must sacrifice some speed for security.

A particularly simple construction for $F$ involves composing a collision-resistant hash function $H: {\cal X} \rightarrow  {\cal D}$ onto some domain ${\cal D}$ with a PRF $G: \{0,1\}^k \times {\cal D} \rightarrow \{0,1\}^{n+\ell}$, viz: $F(K,x) = G(K,H(x))$. It is easy to show that the result is a PRF of the required input-output signature (see for example~\cite[Section 8.2]{BonehShoup}). As a concrete instantiation, one could take $H$ to be SHA-256 with outputs truncated to 128 bits and $G$ to be AES-128 truncated to ${n+\ell}$ bits (since ${n+\ell} \leq 64 < 128$ in practice, this truncation is always possible). 

An alternative is to simply use HMAC-SHA-256 with an appropriately truncated output. HMAC is a PRF under reasonable assumptions on the hash function used in its construction~\cite{Bellare15}. 

These constructions would be relatively fast on commodity platforms due to widespread hardware support for SHA-256 and AES operations. It would be a simple exercise to add the above PRF proposals to an existing HLL implementation and benchmark them against, say, Murmur3.  The exact slowdown depends on platform specifics, but based on microbenchmarks can be expected to be in the range 2-fold to 5-fold compared to Murmur3. 

More complex but faster constructions using randomised or nonce-based MACs constructed from universal hashing could be envisaged. However, these do not seem to be directly applicable here since we need the function $F$ to be both stateless and deterministic so as to cope with the repeated inputs one would encounter during the normal operation of HLL.

We note that using keyed functions sacrifices the mergeability property that HLL enjoys, if different keys $K$ are used in different HLL filters. The reason is that one can no longer emulate running a single filter by taking the maximum over all buckets with each index $j$ in the different filters, since now the bucket contents are created using different keys in a PRF and hence bear no relation to one another. 

%\todo{Discussion: note again that no-one currently does this in practice; also point out that you want different keys in different sketches for security, but this sacrifices the merge capability of HLL.}


\section{Conclusion}\label{sec:conclusions}
The HyperLogLog algorithm is an elegant and efficient solution to the problem of estimating the cardinality of large sets. Its simple structure makes it easy to code and use, as shown by the growing number of available open-source implementations. Nonetheless, we have shown that malicious users can manipulate the HLL cardinality estimate and thence break the security properties of systems relying on HLL. Our attacks are simple but powerful and should raise awareness of the limitations of HLL. Our analysis may assist software developers in understanding the risks they run when using HLL in adversarial settings. We have also provided a simple method for securing HLL in adversarial settings: simply replace the internal hash function by a keyed PRF, and one obtains full protection even in scenario S4, our strongest attack scenario.

%Future work could consist in finding mitigations that do not rely on salt. For example, would the imposed ID setting (like Facebook) solve the issue but still be convenient for the various HLL applications?
Left to future work is the task of extending our attacks and defences to HLL variants such as sliding HLL~\cite{slidinghll} and HLL++~\cite{hllpractice}. 

It would be interesting to find and analyse a strategy for more cleverly ordering the candidates in our attack in scenario S1 to reduce the total insertion cost of the attack, whilst also keeping the number of resets low. Finding an attack in scenario S1 when the target HLL filter is initially non-empty is also open.

We have given a formal analysis of using a secretly-keyed PRF as a countermeasure to our attacks. The reliance on secret keys means sacrificing the mergeability property of HLL sketches in distributed environments (where the keys in the different HLLs are independently generated). It is an interesting cryptographic challenge to find methods for securing HLL sketches against adversarial inputs whilst retaining some form of mergeability. This may also require the introduction of entirely new data structures that trade increased storage for mergeability and improved security. It is also an important open problem to close the performance gap that exists between fast but non-cryptographic hashing used in HLL implementations today and the slower but cryptographically strong VIL-PRF that is needed to obtain security in the face of adversarial inputs.


\bibliographystyle{ieeetr}
\bibliography{ref.bib}

\end{document}
