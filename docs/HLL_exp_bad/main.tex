\documentclass[11pt]{article}
\usepackage{graphicx, amsmath, amsthm, amssymb, setspace, array, url, hyperref, xspace}
\usepackage{a4wide}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\title{HyperLogLog: Exponentially Bad in Adversarial Settings}
\author{Kenneth G. Paterson and Mathilde Raynal\\
\vspace{3mm}
Department of Computer Science, ETH Zurich\\
{\tt kenny.paterson@inf.ethz.ch, mraynal@student.ethz.ch}}

\begin{document}

\maketitle

%\IEEEtitleabstractindextext{%
\begin{abstract}
Computing the count of distinct elements in massive data sets is a common task but naive approaches are memory-expensive. The HyperLogLog (HLL) algorithm (Flajolet \emph{et al.}, 2007) estimates a data stream's cardinality while using significantly less memory than a naive approach at the cost of some accuracy. This trade-off makes the HLL algorithm very attractive for a wide range of applications such as database management and network monitoring, where an exact count may not be needed. Recently, the HLL algorithm has started to be proposed for use in scenarios where the inputs may be adversarially generated, for example detection of network scanning attacks. This prompts an examination of the performance of the HLL algorithm in the face of adversarial inputs. In this cautionary note, we show that in such a setting, the HLL algorithm's estimate of cardinality can be exponentially bad: when an adversary has access to the internals of the HLL algorithm and has some flexibility in choosing what inputs will be recorded, it can manipulate the cardinality estimate to be exponentially smaller than the true cardinality.
\end{abstract}
%}

%\IEEEdisplaynontitleabstractindextext

\section{Introduction}
Naive approaches to computing the cardinality of large data sets, such as sorting the elements or simply maintaining the set of unique elements seen, are impractical at scale and in streaming settings. The HyperLogLog algorithm (HLL henceforth)~\cite{hll} is today the most widely used cardinality estimator. It is increasingly used in settings where adversaries may have  incentives to manipulate the estimate made by the algorithm. For example,~\cite{portscanhll} proposes its use for detecting network scanning attacks, while Facebook engineers have reported~\cite{fbhll} that they use HLL for estimating the number of active users. These applications demand a careful evaluation of the performance of HLL under adversarial input.

Very recently, Reviriego and Ting~\cite{hllvuln} initiated work in this direction, showing that in specific attack settings, the cardinality estimate made by HLL could be modified through input selection. In this paper, we present a complete analysis of HLL in the adversarial setting, considering more realistic attack scenarios and giving more powerful attacks than~\cite{hllvuln}. In particular, we show that with only modest knowledge of the HLL internals and a moderate amount of computation, an adversary with sufficient flexibility in the choice of inputs can make the HLL cardinality estimate exponentially smaller than the true cardinality (e.g.\ constant instead of $O(2^t)$ where $t$ is the number of bits of flexibility in the adversary's input).

The rest of the paper is organised as follows. Section~\ref{sec:overview} gives an overview of HLL. A brief summary of  related work can be found in Section \ref{sec:related}. Section~\ref{sec:attacks} presents our different adversarial models. There, we also give attacks and evaluate their impacts under the different adversarial models. The experimental results are discussed in Section~\ref{sec:exp} and Section~\ref{sec:conclusions} provides our conclusions and some ideas for future work.


\section{Overview of HLL}\label{sec:overview}
\subsection{The Algorithm}
HLL~\cite{hll} is a streaming algorithm based on the key observation that, for a stream of randomly distributed values represented as bit-strings of some fixed length, if we observe a value with the maximum of $k$ leading zero-bits, then the cardinality of the stream (i.e.\ the number of distinct values it contains) is likely to be on the order of $2^{k+1}$. In practice, to ensure a uniform distribution, the stream values are first processed by a hash function $h$ before the leading bits are inspected. So to estimate the cardinality of a stream, we need only to store a representation of $k$, the length of the largest observed string of leading zero-bits for numbers in the stream. This provides a very compact mechanism for estimating the stream cardinality -- if the true cardinality is $N$, then just $\log\log(N)$ bits are needed to store the estimate.

This simple approach suffers from large variance in the estimation. In order to improve the estimate, we can use many estimators in parallel instead of one, and average the results. Each estimate is stored in its own register, that we call a bucket. We refer to the collection of buckets and their contents as the HLL \emph{sketch}. To map a value $x$ to one of $m=2^n$ buckets, \cite{loglog} suggests using the first $n$ bits of $h(x)$ as a bucket index, and then computing the longest sequence of leading zero bits on the remaining $\ell$ bits of $h(x)$. The output length of $h(\cdot)$ is thus $n+\ell$, fixed to 32 in~\cite{hll}. estimates from all buckets are averaged using the harmonic mean and scaled by a constant $\alpha$, where $\alpha$ is empirically computed to correct a systematic multiplicative bias.

Relatively small and big cardinalities are handled separately to correct for systematic distortions, as shown in Fig.~\ref{fig:hll}. When the raw cardinality estimate is below $\frac{5}{2}m$, the Hit Counting algorithm of Whang et al.~\cite{hitcounting} is used to produce the final cardinality estimate, looking at the number of empty buckets (whose register is 0) instead of the number of leading zeros in the stream. A similar correction is applied for large cardinalities in order to take into account the likelihood of hashing collisions. As a result, HLL is able to estimate cardinalities greater than $10^9$ with a typical standard error of 2\%, using only 1.5 kB of memory~\cite{hll}.


Some works building on~\cite{hll} replace the Hit Counting algorithm by other schemes and use a hash function with a larger output domain to reduce the chances of collision and thus get rid of the need for large cardinality range correction. For example, \cite{hllpratice} uses a 64-bit hash function and Linear Counting along with bias correction in the small cardinality range. In this report we target the HLL formulation in the original paper~\cite{hll} as represented in Fig.~\ref{fig:hll}, and leave for future work the analysis of its variants. 


An interesting property of HLL is that it supports merging in a lossless way. To combine two buckets at index $i$ in two different HLL instances, one can take the maximum of the two bucket entries and assign that to the matching bucket $i$ in the merged HLL sketch. Such a simple set union operation allows easy parallelisation of operations among multiple machines independently, provided they use the same hash function and the same number of buckets.

\begin{figure}[ht]
\centering
     \makebox[\linewidth]{
\fbox{
\begin{minipage}{40em}
\begin{flushleft}
\textit{Let $h : \mathcal{D} \rightarrow \{0, 1\}^{32}$ hash data from some domain $\mathcal{D}$ to binary $32$-bit strings.\\
Let $\rho(s)$ be the position of the leftmost $1$-bit of $s$: e.g., $\rho(1...) = 1, \rho(0001...) = 4, \rho(0^K) = K + 1.$\\}
\textbf{define} $m = 2^n$ with $n \in [4..16].$\\
\textbf{define} $\alpha_{16} = 0.673$; $\alpha_{32} = 0.697$; $\alpha_{64} = 0.709$; $\alpha_{m} = 0.7213/(1+ 1.079/m)$ for $m \ge 128$;\\
\vspace{0.5em}
\textbf{Algorithm} HyperLogLog (\textbf{input} $\mathcal{M}$ : multiset of items from domain $\mathcal{D}$).\\
\textbf{initialize} a collection of $m$ registers, $M[1], ..., M[m]$, to $0$;\\
\vspace{0.5em}
\textbf{for} $v \in \mathcal{M}$ \textbf{do}\\
    \hspace{2em} \textbf{set} $x := h(v);$\\
    \hspace{2em} \textbf{set} $j = 1 + <x_1x_2...x_{n}>_2$; \hfill \textit{\{the binary address determined by the first $n$ bits of $x$\}}\\
    \hspace{2em} \textbf{set} $w := x_{n+1}x_{n+2}...$;\\
   \hspace{2em} \textbf{set} $M[j] :=$ max$(M[j], \rho(w))$;\\
\vspace{0.2em}
\textbf{compute} $E := \alpha_mm^2\cdot(\sum_{j=1}^m2^{-M[j]})^{-1}$; \hfill \textit{\{the raw HyperLogLog estimate\}}\\
\vspace{0.2em}
\textbf{if} $E \le \frac{5}{2}m$ \textbf{then}\\
    \hspace{2em}  \textbf{let} $V$ be the number of registers equal to $0$;\\
    \hspace{2em}  \textbf{if} $V \neq 0$ \textbf{then set} $E^*:=  m\cdot \ln(m/V)$ \textbf{else set} $E^*:= E$; \hfill \textit{\{small range correction\}}\\
\textbf{if} $E \le \frac{1}{30}2^{32}$ \textbf{then}\\
    \hspace{2em} \textbf{set} $E^*:= E$; \hfill \textit{\{intermediate rangeâ€”no correction\}}\\
\textbf{if} $E > \frac{1}{30}2^{32}$ \textbf{then}\\
    \hspace{2em}  \textbf{set} $E^*:= -2^{32}$ln$(1 - E/2^{32})$; \hfill \textit{\{large range correction\}}\\
\textbf{return} \textit{cardinality estimate $E^*$ with typical relative error} $\pm1.04/\sqrt{m}$.
\end{flushleft}
\end{minipage}
}}
\caption{The HyperLogLog Algorithm from~\cite{hll}}
\label{fig:hll}
\end{figure}

\subsection{Applications}
For completeness and motivation, we provide examples of concrete uses of HLL:
\begin{itemize}
    \item Cardinality estimation provides a significantly faster way for Facebook to compute statistics on their users. They use HLL to find out how many distinct people visited their website in the past week~\cite{fbhll}.
    \item HLL has been proposed as a solution to detecting Denial-of-Service attacks~\cite{portscanhll} (this paper actually uses a variation on HLL called \emph{sliding HLL}~\cite{slidinghll} which estimates the cardinality over a moving time window). Specifically, HLL is used to count how many different ports are used as destination port over all packets received, as an attempt to identify when a port scan attack is underway.
    \item HLL has been implemented in network soft-switches to approximate the number of distinct packets in traffic flows and overcome the switch resource limits~\cite{flexswitch}. The number of unique flows traversing a switch can then be used to provide a better congestion control protocol.
\end{itemize}

Regardless of the sensitivity of the above examples, none of the above-cited papers are clear about their threat model, especially for the HLL component.  They are all potentially vulnerable to the attacks we present below.

\section{Related Work}\label{sec:related}

The work of Desfontaines \emph{et al.}~\cite{cardestprivacy} studies the privacy properties of HLL in two different attack scenarios, referred to as \emph{insider} and \emph{external}. The attack target of~\cite{cardestprivacy} is different from ours, being concerned with loss of privacy, whereas we care about manipulating cardinality estimation. It is argued in~\cite{cardestprivacy} that HLL can be used by organisations to store and process location data, which is inherently sensitive, so losing privacy would leak users' locations and potentially cause great harm. In Section~\ref{sec:attacks} we will discuss the two attack scenarios from~\cite{cardestprivacy} in more detail. In particular, we will show that an adversary who can insert items into the HLL and who can access the HLL cardinality estimate (via typical APIs in HLL implementations) as permitted in the external scenario of~\cite{cardestprivacy} can actually realise the same attacks as the insider adversary can, at least insofar as cardinality estimate manipulation is concerned.

In~\cite{hllvuln}, Reviriego and Ting exploit a vulnerability of HLL to manipulate the cardinality estimate, leading to a five-fold reduction in the estimate compared to the true cardinality. We consider the attack model used in~\cite{hllvuln} to be quite artificial, since it gives the adversary the ability to test whether inserting an item \emph{would} increase the cardinality estimate, without actually having to insert the item. After some clarifications obtained through personal correspondence with the authors of~\cite{hllvuln}, we understand that such a setup might be possible under certain circumstances, for example where the adversary has access to an identical ``shadow'' device; see further discussion in Section~\ref{sec:attacks}.

Both~\cite{cardestprivacy,hllvuln} propose mitigations using a salted sketch, either as a replacement for (but at the cost of losing mergeability) or in addition to (inducing a memory overhead) an unsalted sketch.
%This might not be a convenient solution for some applications, they thus encourage further research.

Clayton \emph{et al.}~\cite{CCS:ClaPatShrPS19} provide a provable-security treatment of probabilistic data structures in adversarial environments more generally. They use their formalism to analyze Bloom filters, counting (Bloom) filters and count-min sketch data structures, but not HLL. It is an interesting open problem to use their framework to develop a formal understanding of how HLL can be made secure in the adversarial setting. 

\section{HLL in Adversarial Settings}\label{sec:attacks}

This section presents different adversarial settings and corresponding attacks that manipulate the HLL cardinality estimate. The adversary's goal is to reduce the estimate as much as possible, since this would cause more damage when  considering the above-mentioned applications of HLL. It is easy to modify our attack strategies for an adversary who instead wishes to artificially inflate the HLL estimate given a limited input capability. We also analyze the impact on the HLL cardinality estimate and the computational effort required.

Throughout, we assume the adversary has the ability to vary the contents of free fields in the input strings which will be inserted into the stream of values. For example it can manipulate IPID and other header fields in the context of IP packets. This enables the adversary to control the output of hash function $h$ and flexibly choose which items will be inserted into the HLL sketch.

\subsection{Adversarial Scenarios}\label{sec:set}
We model four distinct adversarial scenarios based on the adversary's knowledge and capabilities. To simulate real-life settings, we consider both cases when the sketch is shared and receives inputs from other honest users but also when inputs are under the sole control of the adversary.
It is reasonable to assume that the adversary knows which case it is in. 
These two settings differ mainly in the fact that the adversary is attacking a sketch that may already contain other users' data in the former option, but is empty in the latter.

Among the scenarios, the \textit{insider} setup of~\cite{cardestprivacy} that we present as S4 makes the strongest assumptions about the adversarial capabilities since it assumes that the adversary has a perfect view of the sketch (at some point in time).

\begin{itemize}
\item[S1:] The adversary does not know the details of the target HLL implementation but can access a copy of the HLL instance as a user. It can use the API to insert elements into this shadow sketch, get its cardinality estimate, and reset it to its native empty state. The adversary does not know the state of the targeted HLL sketch, meaning that it does not have the list of elements previously inserted into it by other users.
\item[S2:] The adversary has access to the details of the HLL implementation, i.e.\ it knows the number of buckets $m=2^n$ and the hash function $h$ in use, but is otherwise ``blind''. In particular, an S2 adversary does not have access to the values of the HLL cardinality estimate at any point in its attack. This kind of attack scenario is appropriate in the context of the port scanning attack of~\cite{portscanhll}.
\item[S3:] The adversary has access to the details of the HLL implementation, i.e.\ it knows the number of buckets $m=2^n$ and the hash function $h$ in use. Additionally, it can access and interact with the sketch via an API provided by the sketch owner, allowing it to insert items into the HLL sketch and ask for the HLL's cardinality estimate at any point in time. However, it does not know the individual bucket contents.
\item[S4:] The adversary has direct access to the HLL in use and all its internals, i.e., the number of buckets, the hash function $h$, and also the contents of each bucket in the HLL sketch \emph{at one specific point in time}, i.e.\ it is given a \emph{snapshot} of the sketch. It then causes chosen inputs to be inserted into the HLL sketch. Note that, using these capabilities, an S4 adversary can compute a lower bound on the HLL cardinality estimate based on its initial snapshot and whatever it causes to be inserted into the HLL sketch during its attack (this is irrespective of whether its attack proceeds with input from other users being added to the HLL sketch in parallel with its attack or not; if it knows that there is no other input, then it can compute the \emph{exact} value of the HLL cardinality estimate at every point in its attack instead of a lower bound).
\end{itemize}

Scenarios S1 and S2 are presented as M2 and M1, respectively, in~\cite{hllvuln}. S3 is the \textit{external} attack scenario in~\cite{cardestprivacy}. S4 is a slightly weaker version of the \textit{insider} attack scenario in~\cite{cardestprivacy}, the difference being that an insider adversary in~\cite{cardestprivacy} has continuous access to the HLL sketch internals including all bucket values, while our adversary in S4 only has access to a snapshot of the HLL sketch internals at the start of its attack.

\subsection{Discussion of Adversarial Scenarios}\label{sec:scenarios-discussion}

We do not consider scenario S1 typical of HLL deployments. In a networking application, the authors of~\cite{hllvuln} argue that the assumptions of S1 could be fulfilled if the adversary learns which machine is used to do the monitoring, buys the same, and uses it as the shadow device. It is possible that the provider uses a different salt or key in the hash per device. This would make any pre-computation on the items using the shadow device irrelevant, and considerably weaken the attack presented in Section 3.2 of~\cite{hllvuln}. Hence we assume for the rest of this work that, in scenario S1, shadow and targeted devices have the same internals and security parameters. If the HLL device receives inputs from several users, the attack from Section 3.2 of~\cite{hllvuln} is then the easiest to carry out. It is described in~\cite{hllvuln} as follows: "after each insertion [in the shadow device], elements that do not increase the cardinality estimate are retained". The idea is to iterate through the process of trial insertion several times to refine the list of items, before inserting what remains into the targeted device. We explore in this work scenario S1 in the special case where inputs come only from the adversary, and propose an attack that provides better results than the one presented in~\cite{hllvuln}. 

Furthermore, we show that although~\cite{cardestprivacy} presents S3 (external adversary) and the stronger version of our S4 (insider adversary) as two separate scenarios, an S3 adversary can easily infer the knowledge given to our S4 adversary in its snapshot. Indeed, an adversary can easily recover the contents of each bucket in the sketch simply by analysing which inputs increase the cardinality estimate. To find the estimate held in a targeted bucket, the adversary  inserts a sequence of items $x$ with an incrementing number of leading zeros in the $\ell$ rightmost bits of $h(x)$ whilst holding the $n$ bits determining the bucket index constant, until the cardinality estimate of the sketch increases. Once an increase occurs, the adversary knows that the maximum number of leading zeros observed in that bucket was just updated to a value that can be recovered by looking at the last inserted item. This assumes the adversary has sufficient flexibility in its choice of inputs $x$ so that the relevant bit conditions on $h(x)$ can be forced. The adversary can determine all bucket contents with, in the worst case, $O(2^{n + \ell})$ pre-computation and $O(\ell \cdot 2^{n})$ insert and cardinality estimate queries, and needs about $n + \ell$ bits of flexibility assuming $h$ behaves like a random function. The pre-computation and query complexity required can be much less if the HLL sketch is only moderately loaded, since then most buckets will store values corresponding to inputs with few leading zero bits. A version of this attack can be carried out to elevate an external adversary to a full-strength insider adversary as per~\cite{cardestprivacy} (instead of our slightly weaker S4 adversary), since it can simply be done each time the external adversary wants to obtain a snapshot of the internals of the HLL sketch. Obvious optimisations can be carried out to reduce the query complexity each time the attack is run (since the bucket contents can only increase over time, some queries are redundant; moreover, the pre-computation is only needed once and is moderate given typical HLL parameters, e.g.\ $n+\ell = 32$).

Since an adversary can quite efficiently escalate from S3 to S4, we focus on scenarios S1, S2 and S4 in what follows. We first present an attack in scenario S1, providing much stronger results than the corresponding attack of~\cite{hllvuln} (in their equivalent scenario M2). We then use the additional information the adversary has access to in scenarios S2 and S4 to make the attack even more powerful.

\subsection{Manipulating the HLL Cardinality Estimate}

Although we consider scenario S1 to be quite artificial, we will commence our analysis there, since the techniques we present are more broadly applicable in S2 and S4.

\noindent\paragraph{Scenario S1} The goal of our adversary is to infer some information about targeted items' hash values, precisely their bucket mappings, knowing neither $h$ nor the salt in use (if any). Using this information, the adversary is able to retain items that are mapped to a subset of $b$ buckets out of the $2^n$, and minimize the resulting estimated cardinality. We assume the adversary either has flexibility in choosing the items to insert, or is presented with a static list of items at the start of its attack. We also assume that the shadow HLL sketch and the target HLL sketch are both initially empty.

Our adversary takes advantage of the use of HLL's small cardinality Hit Counting correction to recover the number of non-empty buckets (whose register does not contain value 0) after inserting a few items into the initially empty shadow HLL sketch. Indeed, in the small cardinality range, the cardinality estimate $E^*$ is computed as $E^*= m \cdot \ln (m/V)$ with $V$ the number of empty buckets. For typical HLL parameters, e.g.\ $n = 8, m =256$, the exact value of $V$, hence $b$, can be recovered from $E^*$. Moreover, when inserting an item in the small cardinality range, the adversary will see an increase in the cardinality estimate for the shadow HLL sketch if and only if $V$ is modified. This in turn means that the number of empty buckets in the shadow HLL sketch decreased by 1, and therefore that the freshly inserted item was mapped to a bucket that was not hit before. 

The adversary blindly picks the first $b$ buckets to be hit by simply inserting items into the shadow HLL until the cardinality estimate indicates that $b$ buckets have been touched. It puts these items in a final target list ${\cal L}$. It continues to insert items, discarding all items whose insertion causes the cardinality estimate for the shadow HLL sketch to increase from $b+k$ to $b+k+1$ for $k \in \mathbb{N}$ (here we allow some slack in the total number of buckets that have been hit in order to reduce the number of resets of the shadow device that are required in the attack). In other words, when $b+k$ buckets have already been hit and the insertion of an item causes the cardinality estimate to increase further, the adversary can discard this item as it definitely knows that a new bucket (that is not one of the $b$ targeted buckets) switched to a non-empty state. On the other hand, it adds to the list ${\cal L}$ any item that does \emph{not} cause the cardinality estimate to increase. Such items may have hit one of the initial $b$ buckets, or one of the later buckets; at this stage the adversary cannot be certain. The adversary can keep inserting items and discarding those which cause the cardinality estimate to increase while in the small cardinality range.

As soon as the cardinality estimate exceeds $\frac{5}{2}m$, the adversary can assume that the shadow HLL sketch's output corresponds to the raw HLL cardinality estimate with high probability. In this case, no more useful information can be inferred about the bucket mappings in the shadow HLL sketch, so the adversary resets the shadow HLL sketch to the empty state and starts again with the list ${\cal L}$ of non-discarded items obtained so far. Reinserting these items will (eventually) determine whether they hit the original $b$ buckets or later buckets, and hence whether they should be retained in ${\cal L}$ or discarded. Once ${\cal L}$ is reprocessed (and assuming the cardinality estimate does not exceed $\frac{5}{2}m$), the adversary can start to insert fresh items, as previously described. The adversary must be careful not to shuffle the non-discarded items in between iterations, as the $b$ buckets to be filled are blindly determined by the first inserted items, and must stay the same throughout the attack. 

After sufficiently many iterations of this procedure, the adversary will have created a list of items ${\cal L}$ such that, when inserted into the \emph{target} HLL, create a sketch in which $b$ target buckets are filled and the others left empty. With an appropriate choice of $b$ (as discussed immediately below), the final cardinality estimate for the target HLL sketch will be $E^*= m \cdot \ln (m/V)$ where $V = m-b$ is the number of empty buckets. This can be rewritten as $E^*= m \cdot \ln (1+b/(m-b))$.

The value of $b$ has to be picked carefully in order to keep the shadow and target HLL cardinality estimates low, as the registers in both sketches may hold values that (potentially) become very high as more items are inserted. The trick is select $b$ so that, even in the worst case scenario where all register values $M[j]$ hold the maximal value of $\ell+1$, the Hit Counting algorithm will still be used to produce the cardinality estimates. Noting that the raw estimate $E = \alpha_mm^2\cdot(\sum_{j=1}^m2^{-M[j]})^{-1}$ increases with increasing $M[j]$, and that we have $b$ buckets each contributing, in the worst case, $2^{-(\ell+1)}$ to the sum (and hence $m-b$ buckets which contribute $2^{0}=1$ to the sum) we see that for the Hit Counting algorithm still to be used, we require:
\[
\alpha_mm^2\cdot\left((m-b)+b\cdot2^{-(\ell+1)}\right)^{-1} \le \frac{5}{2}m.
\]
Solving for $b$, we obtain:
\[
b \le b_{\max} := \left\lfloor m\cdot\left(1-\frac{2\alpha_m}{5}\right)\cdot\left(1-2^{-(\ell+1)}\right)^{-1} \right\rfloor. \label{eqn:1} 
\]

For example with $n = 8$ (so $m=256$) and $\ell = 24$, we find that $b_{\max}=184$.

A surprising implication of this analysis is that, for the HLL algorithm as described in Figure~\ref{fig:hll}, provided $b \leq b_{\max}$, no matter which fixed set of $b$ buckets are hit, and no matter how many leading zero bits the items have, the final cardinality estimate will not exceed $m \cdot \ln (1+b/(m-b))$. We will exploit this in the sequel in scenarios S2 and S4. For now, we note that setting, say, $b=128$ with $n = 8$ and $\ell = 24$ would result in a final cardinality estimate of $256 \ln 2$ in the above attack in Scenario S1, irrespective of how long the list ${\cal L}$ of inserted items is. Moreover, setting $b=128$ would mean that half of all items available to the adversary could be inserted into the target HLL. 

%The initial set of items to be filtered can be built by varying the flexible fields of the items $x$ the adversary is trying to insert.
%so that the adversary can ``hide'' $2^{t-n}$ elements while keeping the HLL cardinality estimate to 1.

\noindent\paragraph{Scenario S2} The adversary knows $h$ so it can adjust fields in the item $x$ it is trying to insert and keep only those for which the $\ell$ rightmost bits of $h(x)$ start with a one bit. In this case, the number of leading zero bits computed by the estimate of the destination bucket is always zero, so the estimate of this bucket is unlikely to be updated and the final averaged approximate cardinality is likely to be left unchanged.
For example, in the scenario of~\cite{portscanhll}, the adversary could change the source port of its IP packet until it is satisfied with its hash value. Given sufficient flexibility in the input $x$, the adversary can insert many items into the HLL sketch without increasing the cardinality markedly, by repeatedly sampling from the input domain.

When attacking an empty sketch, the adversary can keep this strategy and already mount a powerful attack, but can do better by following the approach presented under S1. Filtering items is easier in this scenario as the adversary can infer the bucket mapping of an item $x$ directly from the first $n$ bits of $h(x)$. A trivial approach for the adversary is to retain items mapped to buckets 1 to $b$. We argue why targeting $b$ buckets is a better approach in the impact analysis presented in Section~\ref{sec:impact}. 

\noindent\paragraph{Scenario S4} The adversary can adopt the same strategy as for S2, but make the attack more efficient and reduce the sampling requirements with the additional information available in this scenario (namely, the count values per bucket in the HLL sketch). Therefore, it can allow some leading zero bits in the $\ell$ rightmost bits of $h(x)$ so long as this does not increase the current bucket estimate. In other words, if $h(x)$ is mapped to the $i$-th bucket with estimated cardinality $2^{M[i]}=2^{c_i + 1}$ (because a value with $c_i$ leading zeroes was previously observed for this bucket), then, instead of the strict restriction of having a one bit in the first position in the $\ell$ rightmost bits of $h(x)$, the adversary is satisfied when there is a one bit in any of the first $c_i+1$ positions in this substring. In case the adversary hits an empty bucket (for which the estimate is 0), it must skip that bucket completely. Under these conditions, the estimates in the buckets are never updated, and there is no effect on the cardinality estimate made by HLL.

A special case arises when the HLL is completely empty at the beginning of the attack. Here the adversary is forced to increase the estimate in at least one bucket. Once again, the adversary can choose to fill $b$ buckets following the strategy described in S2, leaving all the other buckets untouched.

\subsection{Impact}\label{sec:impact}
In this section we evaluate the impact of the above attacks on the HLL cardinality estimate. We assume that $h$ is a ``good" hash function, so that each of the bits of $h(x)$ can be regarded as being independent and uniformly random. We assume that the adversary has $t$ bits of flexibility in its choice of inputs.

\subsubsection{Non-Empty Sketch}\label{sec:nonempty}

We start by analysing our results when the attack is ran against a sketch that already received inputs from other users, first under scenario S2 then S4. 

In scenario S2, the adversary needs to craft two distinct candidates for $x$ on average to obtain a one bit at the first position in the $\ell$ rightmost bits of $h(x)$, thus successfully performing the attack for a single insertion. The adversary can expect to ``hide''  $2^{t-1}$ distinct items without increasing the HLL cardinality estimate at all, under the assumption that all buckets in the HLL sketch already hold a value $c_i \ge 1$. On the other hand, if some buckets are empty (but the adversary does not know which ones in the blind setting of S2), then we may expect a moderate increase in the HLL cardinality estimate. Notice that in all cases, provided $t$ is large enough, the adversary can keep the cardinality estimate linear in the total number of buckets ($2^n$) while the expected value of the true cardinality, $2^{t-1}$, can be made as large as the adversary pleases.

In scenario S4, we compute the average work required of the adversary to find an input $x$ such that $h(x)$ meets the attack's requirements. Let us assume for the moment that $c_i \ge 1$ for all buckets $i$. Assume $h(x)$ is mapped to bucket $i$. The probability that $h(x)$ does not increase the estimate is the probability that the  rightmost $\ell$ bits of $h(x)$ have $c_i$ or less leading zeroes; this probability is equal to $1-\frac{1}{2^{c_i+1}}$ assuming $h$ behaves like a random function. Averaging over all $2^n$ buckets (and using the fact that the bucket choice is uniformly random since the bits of $h$ are independent and uniformly random), the probability that input $x$ does not increase the estimate is given by:
\begin{eqnarray*}
\sum_{i=1}^{2^n}\frac{1}{2^n} \cdot (1-\frac{1}{2^{c_i+1}}) & = & 1-\frac{1}{2}\cdot (\frac{1}{2^n}\cdot \sum_{i=1}^{2^n} {(2^{c_i})}^{-1}) \\
& = &1-(2H_c)^{-1} 
\end{eqnarray*}
where $H_c$ is the harmonic mean of the counts $2^{c_1}, 2^{c_2}, \ldots, 2^{c_{2^n}}$.
So the adversary needs to craft on average $(1-(2H_c)^{-1})^{-1}$ distinct candidates for $x$ in order to insert a single candidate, and, given $t$ bits of flexibility in its choice of inputs, can expect to ``hide" up to $(1-(2H_c)^{-1}) \cdot 2^t$ distinct items without increasing the HLL cardinality estimate at all.

Note that the above analysis for scenario S4 made the assumption that every bucket count $c_i$ was already at least 1. When some bucket counts are 0, the above analysis needs to be modified in order to avoid touching those buckets altogether. The modified analysis is quite straightforward. Suppose $V \le 2^n-1$ out of $2^n$ buckets have a count of zero. Then the cost of crafting an input $x$ becomes $(1-(2H_c)^{-1})^{-1} \cdot (1- \frac{V}{2^n})^{-1}$ trials, while the total number of items the adversary can expect to insert without increasing the HLL cardinality estimate becomes $(1-(2H_c)^{-1}) \cdot (1- \frac{V}{2^n})\cdot 2^t$. Of course, the adversary can relax its attack and allow some of the empty buckets to be hit. This would increase the insertion rate at the cost of a small increase in the final cardinality estimate. 

\textbf{Conclusion:} In this setting, we see that an adversary who has $t$ bits of flexibility in its inputs can expect to insert exponentially (in $t$) many input values into the HLL sketch without increasing the cardinality estimate (or whilst increasing the cardinality estimate slightly in the cases where the HLL sketch has empty buckets in S2). The result is largely independent of the HLL parameters, but does depend on the extent to which the HLL sketch is already filled by other users, via a factor $1-(2H_c)^{-1}$ and the number of empty buckets $V$, via a factor $1- \frac{V}{2^n}$. Other trade-offs between work and the cardinality estimate are possible.

\subsubsection{Empty Sketch}

We now consider the case where the HLL sketch is initially empty.
In that case, the output of the attacks is the same for all scenarios.
Targeting $b$ buckets allows the adversary to insert a fraction $\frac{b}{2^n}$ of the items, and thus to retain $\frac{b}{2^n}2^t=2^{t-n+\text{log}_2(b)}$ elements.
After inserting the retained items, the cardinality of the sketch is estimated to $2^n$ln$(2^n/(2^n-b))$, approximating to $b$ for relatively small values of $b$.

When following the initial strategy of S2, we saw in paragraph~\ref{sec:nonempty} that the adversary retains $2^{t-1}$ items. The insertion of those items will hit every bucket of the sketch with high probability, and update every register once to 1. The cardinality of the sketch is then estimated to $\alpha2^{2n}(\sum_{j=1}^{2^n}2^{-1})^{-1}=\alpha2^{n+1}$ and, as $V=0$, no small range cardinality correction is applied. 
On the other hand, when targeting and filling half the buckets ($b=2^{n-1}$), the adversary can insert as many items but lower the estimated cardinality to $2^n$ln$(2^n/(2^n-2^{n-1}))=2^n\text{ln}(2)$ instead of $\alpha{2^{n+1}}$ that we previously had, at no extra computational cost.

\textbf{Conclusion:} When confronted to an empty sketch, the adversary is forced to concede a slight cardinally estimation increase in order to be able to insert some elements.
For a fixed $b$, an adversary who has $t$ bits of flexibility in its inputs is then able to insert exponentially (in $t$) many input values into the HLL sketch while keeping the cardinality estimate constant.
Any value below the upper-bound specified in eq. (4.1) can be chosen for $b$ based on the setup requirements. For example if the reset operation is costly, the adversary can adjust $b$ accordingly. 


\section{Experimental Results}\label{sec:exp}
Many different implementations of HLL are available online. Among the repositories collecting more than 350 stars on GitHub, we can cite~\cite{clahll} and~\cite{datasketch}. HLL is also featured in many frameworks, including Redis~\cite{redis}, Google's BigQuery~\cite{bigquery}, Facebook's Airlift~\cite{airlift} and several products of Apache such as Spark~\cite{spahll} and Druid~\cite{druhll}. It is interesting to point out that all these implementations use a constant number of buckets and a fixed hash function. Thus they are vulnerable to our attacks in scenarios S2 and S4.

As a proof-of-concept, we implemented our attacks against the HLL implementation from~\cite{clahll}. The attack code is available at \href{https://github.com/PizzaWhisperer/HLLVuln}{\textit{\url{https://github.com/PizzaWhisperer/HLLVuln}}}. For simplicity, our items are 4-character long ASCII strings, giving us up to $7,311,616= 2^{22.8}$ distinct items that can be inserted (hence we have $t=22.8$ in our attacks). We choose $h$ to be the 32-bit Murmur3 hash function from~\cite{murmur3code} (note that the library allows the hash function to be specified; we pick Murmur3 for our attack and consider it reasonable that the adversary should know the hash function in use).
We create an HLL sketch using $2^n = 256$ buckets and initialise it with 1,000 random items representing honest users' data.

We then challenge the adversary to pick the largest possible number of items to add to the sketch from our set of  $7,311,616$ possible inputs while trying to keep the cardinality estimate as low as possible, either in the S1, S2 or S4 scenario. As a second experiment, and in order to get results that can be compared to the work of~\cite{hllvuln}, we mount our attack in their more restrictive setting: instead of picking from the universe of possible strings, the adversary has to choose what to insert from a set of 250,000 random items that are given to it. Finally, for completeness, we also run the attacks against HLL sketches filled with adversarial inputs only, thus that are initially empty. We vary the value of $b$ in the attack under the S1 scenario to demonstrate possible trade-offs. For simplicity and to keep the attack as striking as possible, we set $b$ to 1 in the attacks on empty sketch under S2 and S4. All attacks were run 30 times and the outcomes averaged. Our results are reported in Tables~\ref{table:tab1} and~\ref{table:tab2}, which detail the estimated cardinality at the beginning of the attack, the number of items added by the adversary, and the cardinality estimate after adding the adversary's items.

These empirical results confirm our impact analysis on several points.

First, when attacking a sketch already containing some items, we can see that the adversary is on average able to hide about half of the items in our starting set under S2 (as predicted by our theoretical analysis), and about 83\% under S4. %( $205$'$000 \approx 250'000 \times 0.82 \approx 250$'$000\times(1-\frac{1}{2\times H_c})$, with $H_c\approx\frac{1'000}{256}=3.9$)
Here $0.83 \approx (1-\frac{1}{2\cdot H_c})$ with $H_c\approx\frac{1000}{256}=3.9$, so the results are also consistent with our theoretical analysis.

Secondly, when attacking an empty HLL sketch, we can see that in all scenarios, the final cardinality estimate is always exactly $2^n$ln$(2^n/(2^n-b))$ after inserting a fraction $\frac{b}{2^n}$ of the items (this comes from the Hit Counting algorithm result that arises from our attack targeting $b$ buckets).

We can also compare our results from Table~\ref{table:tab2} with results from the attack presented in Section 5.1 of~\cite{hllvuln}. In that paper, the authors target the Redis~\cite{redis} open-source implementation of HLL and perform the attack against an empty sketch filled with adversarial inputs only. Given a set of 250,000 items, the adversary could choose ``74,390 distinct items and obtain an HLL estimate of only 15,780''~\cite{hllvuln}, achieving a five-fold reduction from the true cardinality after 4 iterations (resets). The S1 adversary following our strategy is able to add 977 and 97,433 distinct items while keeping the cardinality estimate respectively at 1 and 126. Compared to~\cite{hllvuln}, we can reduce the cardinality estimate by a factor of almost 1,000 instead of 5 at the cost of more resets.

\begin{table}[h]
\centering
\caption{Attack results, averaged over 30 iterations.}
     \makebox[\linewidth]{
\begin{tabular}{| m{9.8em} | m{4em} | m{4.2em} | m{4.1em} | m{4.1em} | m{4.1em} | m{4em} |}
    \hline
    \textbf{Scenario} &  \multicolumn{2}{c|}{S1} & \multicolumn{2}{c|}{S2} & \multicolumn{2}{c|}{S4} \\ \hline
    \textbf{\# Targeted buckets} & 1 & 100 & - & 1 & - & 1 \\ \hline
    \textbf{Initial data?} & no & no & yes & no & yes & no \\ \hline
    \textbf{Original Card. Est.} & 0 & 0 & 1,000 & 0 & 1,005 & 0 \\ \hline
    \textbf{\# Items added} & 28,566 & 2,856,100 & 3,655,744 & 28,566 & 6,090,003 & 28,566 \\ \hline
    \textbf{Final Card. Est.} & 1 & 126 & 1'011 & 1 & 1,005 & 1 \\ \hline
\end{tabular}}
\label{table:tab1}
\end{table}

\begin{table}[h]
\centering
\caption{Attack results, averaged over 30 iterations, in the setting of \cite{hllvuln}.}
     \makebox[\linewidth]{
\begin{tabular}{| m{9.8em} | m{4em} | m{4.2em} | m{4.1em} | m{4.1em} | m{4.1em} | m{4em} |}
    \hline
    \textbf{Scenario} & \multicolumn{2}{c|}{S1} & \multicolumn{2}{c|}{S2} & \multicolumn{2}{c|}{S4} \\ \hline
    \textbf{\# Targeted buckets} & 1 & 100 & - & 1 & - & 1 \\ \hline
    \textbf{Initial data?} & no & no & yes & no & yes & no \\ \hline
    \textbf{Original Card. Est.} & 0 & 0 & 1,004 & 0 & 998 & 0 \\ \hline
    \textbf{\# Items added} & 977 & 97,433 & 124,940 & 979 & 204,669 & 988 \\ \hline
    \textbf{Final Card. Est.} & 1 & 126 & 1,058 & 1 & 998 & 1 \\ \hline
\end{tabular}}
\label{table:tab2}
\end{table}

\section{Conclusion}\label{sec:conclusions}
The HyperLogLog algorithm is an elegant and efficient solution to the problem of estimating the cardinality of large sets. Its simple structure makes it easy to code and use, as shown by the growing number of available open-source implementations. Nonetheless, malicious users can manipulate the HLL cardinality estimate and thence break the security properties of systems relying on HLL. Our attacks are simple but powerful and should raise awareness of the limitations of HLL. Our analysis may assist software developers in understanding the risks they run when using HLL in adversarial settings.

A formal analysis of using salting as a countermeasure to our attacks remains open.
%Future work could consist in finding mitigations that do not rely on salt. For example, would the imposed ID setting (like Facebook) solve the issue but still be convenient for the various HLL applications?
Also left for future work is the task of extending our attacks to HLL variants such as sliding HLL~\cite{slidinghll}, HLL++~\cite{hllpratice} and Redis alternative using the maximum likelihood approach from~\cite{newhll}.


\bibliographystyle{ieeetr}
\bibliography{ref.bib}

\end{document}
